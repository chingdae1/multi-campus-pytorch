{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network: Neural Weather Forecaster\n",
    "\n",
    "<img src=\"./img/weather.png\" width=\"45%\" height=\"45%\">\n",
    "* 이미지 출처: 네이버\n",
    "<br>\n",
    "많은 사람들이 아침에 집을 나서기 전에 오늘 기온이 어떤지 혹은 비가 오는지 알기 위해 일기예보를 확인합니다. 그런데 혹시 일기예보가 어떻게 이루어지는지 생각해 보신적이 있으신가요? 아직 오지도 않은 미래의 날씨를 어떻게 예측할 수 있을까요? 아마 여러분 대부분은 기상 예측과 관련된 전문적인 지식에 대해 잘 알지 못할 것입니다. 엄청난 계산능력을 갖춘 슈퍼컴퓨터가 복잡한 계산을 통해 예측을 한다는 정도는 들어보신 분들도 계실 수 있겠네요. 하지만 딥러닝을 활용할 수 있고, 지난 과거의 기후 데이터만 가지고 있으면 여러분의 PC에서도 훌륭한 일기예보 모델을 학습시킬 수 있습니다. 이번 프로젝트에서는 RNN을 직접 설계하여 24시간 후의 기온을 예측하는 문제를 해결할 것입니다.\n",
    "\n",
    "이번 실습의 목표는 다음과 같습니다.\n",
    "- RNN을 설계하여 지난 며칠 동안의 날씨 정보를 기반으로 24시간 이후의 기온을 예측한다.\n",
    "- 다양한 속성의 시계열 정보를 활용하기 위해 적절한 전처리 과정을 적용한다. \n",
    "- 설계한 모델의 성능을 검증하기 위해 베이스라인 모델을 도입한다.\n",
    "\n",
    "실습코드는 Python 3.6, Pytorch 1.0 버전을 기준으로 작성되었습니다.\n",
    "\n",
    "이번 과정을 통해 얻는 최종 결과물은 아래 그림과 같습니다.\n",
    "<img src=\"./img/result_rnn.png\" width=\"65%\" height=\"65%\">\n",
    "\n",
    "**\"[TODO] 코드 구현\"** 부분의 **\"#코드 시작\"** 부터 **\"#코드 종료\"** 구간에 필요한 코드를 작성해주세요. **나머지 작성구간이 명시 되지 않은 구간은 임의로 수정하지 마세요!**\n",
    "\n",
    "**본문 중간중간에 Pytorch 함수들에 대해 [Pytorch API 문서](https://pytorch.org/docs/stable/) 링크를 걸어두었습니다. API 문서를 직접 확인하는 일에 익숙해지면 나중에 여러분이 처음부터 모델을 직접 구현해야 할 때 정말 큰 도움이 됩니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Package load\n",
    "필요한 패키지들을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 하이퍼파라미터 세팅\n",
    "학습에 필요한 하이퍼파리미터의 값을 초기화해줍니다.\n",
    "\n",
    "미니배치의 크기, 학습할 Epoch(세대) 수, Learning rate(학습률) 값들을 다음과 같이 정합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 30\n",
    "learning_rate = 0.00003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 전처리 함수 정의\n",
    "\n",
    "우리는 이번 실습에서 지난 10년간의(2009년~2018년) 서울시 기후 데이터를 활용해 기온을 예측하는 모델을 학습시킬 것입니다. 데이터셋은 [기상자료개방포털](https://data.kma.go.kr/)에서 받은 자료입니다. 이번 실습에서 사용하는 데이터 이외에도 기상자료개방포털에서 기상과 관련된 다양한 자료들을 내려받으실 수 있습니다. \n",
    "\n",
    "'./data/climate_seoul' 경로의 디렉토리를 보시면, test / train / val 디렉토리에 csv파일이 각각 1개 / 8개 / 1개 담겨있음을 확인하실 수 있습니다. 각 csv파일은 1년간의 서울시 기후 데이터를 담고 있으며, 1시간 간격으로 기록된 정보입니다. (사실 아주 가끔씩 30분 간격으로 기록한 구간도 있기도 하지만, 이후 본문에서는 편의상 모두 1시간 간격으로 기록된 것으로 간주하겠습니다.) 매 시간마다 기록되는 정보는 기온, 강수량, 풍속 등을 포함한 총 25가지 속성으로 이루어져 있습니다. 이 중에서 우리는 기온, 강수량, 풍속, 습도, 증기압을 포함한 총 9가지의 속성만을 사용하여 기온 예측 모델을 학습시켜 보겠습니다.\n",
    "\n",
    "그렇다면 왜 하필 이 9가지의 속성을 선택한 것일까요? 이렇게 9가지의 속성을 선택한 배경에는 어떠한 전문적인 지식도 고려되지 않은 것입니다. 25가지의 속성을 모두 사용해볼 수도 있겠죠. 어떤 속성들을 활용할지는 설계자의 몫입니다. 그런데 여러분이 기상과 관련된 전문적인 지식을 갖고 있지 않는 이상 이중에서 어떤 속성이 기온 예측에 가장 중요한지, 또는 어떤 속성이 가장 불필요한 속성인지 알지 못할 것입니다. 하지만 고맙게도 딥러닝은 이러한 속성 선택 문제에 덜 예민한 학습 방식입니다. 더 정확하게 말하면, 다소 불필요한 정보가 입력으로 주어진다고 해서 극단적으로 학습이 이루어지지 않는 일은 일어나지 않을 가능성이 큽니다. 학습과정에서 인공신경망이 필요한 특징(feature)을 알아서 추출하기 때문입니다. 그러니까 우리는 어떤 속성을 활용할지를 너무 심각하게 고민하지 않아도 되는 것입니다. 다만 이번 실습에서는 매번 빠짐없이 잘 기록된 속성들을 위주로 9가지를 선택한 것 뿐입니다.\n",
    "\n",
    "아래에 정의한 **preprocess** 메소드는 csv파일들을 읽어 9가지 속성 정보만을 numpy 배열에 저장해 반환하는 역할을 합니다. 이 메소드는 이후에 구현할 Dataset class에서 활용할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(all_files):\n",
    "    data_0 = [] # 기온\n",
    "    data_1 = [] # 강수량\n",
    "    data_2 = [] # 풍속\n",
    "    data_3 = [] # 습도\n",
    "    data_4 = [] # 증기압\n",
    "    data_5 = [] # 이슬점 온도\n",
    "    data_6 = [] # 현지 기압\n",
    "    data_7 = [] # 해면 기압\n",
    "    data_8 = [] # 지면 온도\n",
    "    for f in all_files:\n",
    "        with open(f, encoding='euc-kr') as c:\n",
    "            csv_reader = csv.reader(c, delimiter=',')\n",
    "            header = True\n",
    "            for col in csv_reader:\n",
    "                if header:\n",
    "                    header = False\n",
    "                    continue\n",
    "                data_0.append(float(col[2])) if col[2] != '' else data_0.append(0.0)\n",
    "                data_1.append(float(col[3])) if col[3] != '' else data_1.append(0.0)\n",
    "                data_2.append(float(col[4])) if col[4] != '' else data_2.append(0.0)\n",
    "                data_3.append(float(col[6])) if col[6] != '' else data_3.append(0.0)\n",
    "                data_4.append(float(col[7])) if col[7] != '' else data_4.append(0.0)\n",
    "                data_5.append(float(col[8])) if col[8] != '' else data_5.append(0.0)\n",
    "                data_6.append(float(col[9])) if col[9] != '' else data_6.append(0.0)\n",
    "                data_7.append(float(col[10])) if col[10] != '' else data_7.append(0.0)\n",
    "                data_8.append(float(col[22])) if col[22] != '' else data_8.append(0.0)\n",
    "\n",
    "    data = np.zeros((len(data_0), 9))\n",
    "    for i, d in enumerate(data):\n",
    "        data[i, 0] = data_0[i]\n",
    "        data[i, 1] = data_1[i]\n",
    "        data[i, 2] = data_2[i]\n",
    "        data[i, 3] = data_3[i]\n",
    "        data[i, 4] = data_4[i]\n",
    "        data[i, 5] = data_5[i]\n",
    "        data[i, 6] = data_6[i]\n",
    "        data[i, 7] = data_7[i]\n",
    "        data[i, 8] = data_8[i]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 전체에 대해 각 속성별 평균과 표준편차를 계산합니다. 이후 Dataset class에서 구현할 데이터 normalization에 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.28621188e+01 1.57517712e-01 2.48297948e+00 5.97072011e+01\n",
      " 1.11914287e+01 4.42484570e+00 1.00576801e+03 1.01607704e+03\n",
      " 1.44160941e+01]\n",
      "[11.28150794  1.35869296  1.39080164 19.96987271  8.18364372 12.46515073\n",
      " 15.77583125 14.97739634 13.14013178]\n"
     ]
    }
   ],
   "source": [
    "all_csv_files = glob.glob(os.path.join('../data/climate_seoul', '*', '*'))\n",
    "all_data = preprocess(all_csv_files)\n",
    "mean = all_data.mean(axis=0)\n",
    "std = all_data.std(axis=0)\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset 정의 및 DataLoader 할당\n",
    "\n",
    "이제 우리가 사용할 데이터셋에 대해 정의할 차례입니다. Pytorch 의 Dataset과 DataLoader에 대해 잘 기억나지 않는다면 ['Lab-04-2'](https://www.youtube.com/watch?v=B3VG-TeO9Lk&list=PLQ28Nx3M4JrhkqBVIXg-i5_CVVoS1UzAv&index=8&t=0s) 강의를 참고하시기 바랍니다.\n",
    "\n",
    "생성자(**\\__init__**)는 미리 구현을 해두었습니다. 여기서 사용되는 각 변수의 의미는 다음과 같습니다.\n",
    "- **seq_len**: 우리가 이후에 설계할 RNN의 입력으로 줄 데이터 시퀀스의 길이, 즉 총 타임스텝의 길이를 뜻합니다. 다시 말하면, 현재로부터 24시간 뒤의 기온을 예측하기 위해 얼마만큼의 과거 정보를 참고할지 결정하는 것입니다. 기본값을 480로 해두었는데, 이는 480개의 과거 데이터를 살펴보겠다는 것이고, 데이터 1개는 1시간마다 기록되기 때문에 결과적으로 지난 20일간의 데이터를 기반으로 24시간 후의 기온을 예측하겠다는 의미입니다. \n",
    "- **target_delay**: 우리가 예측할 시점이 입력 시퀀스의 마지막 타임스텝으로 얼만큼 이후인지 결정하는 것입니다. 24로 기본값을 해두었고, 이는 24시간 이후의 기온이 우리가 예측할 대상임을 의미하는 것입니다. \n",
    "- **stride**: 데이터를 모델에게 입력으로 주기 위해 우리는 **seq_len** 길이 만큼의 정보를 우리가 가진 전체 데이터에서 임의로 선택해야 합니다. 임의로 선택된 그 시작 지점을 시작 인덱스라고 부른다면, **stride**는 그 시작 인덱스 후보들 간의 간격을 의미합니다. **stride=1**이라면 모든 시점의 데이터가 시작 인덱스가 될 수 있는 것이고, **stride=2**이면 아래의 그림처럼 시작 인덱스 후보가 하나씩 건너띄어 존재하므로, 전체 데이터 포인트에서 절반만이 시작 인덱스가 될 수 있습니다. **stride**가 작을수록 모델의 서로 다른 입력간에 정보가 중복되는 정도가 크겠죠. (**stride=1**을 '01234', '12345', '23456' 이라고 생각하고 **stride=2**를 '01234', '23456', '45678' 이라고 생각하면 이해가 되실 겁니다.) 어떤 값으로 정할지는 역시 설계자인 우리의 몫입니다. 이번 예제에서는 기본값을 5로 정하겠습니다. \n",
    "<img src=\"./img/stride.png\" width=\"80%\" height=\"60%\">\n",
    "- **all_files**: 정의할 Dataset에 사용할 모든 csv파일의 경로를 담고 있습니다. **data_dir**은 데이터셋의 디렉토리 경로를 의미하고 **mode**는 정의하고자 하는 Dataset에 따라 'train' 또는 'val' 또는 'test'으로 구분될 것입니다. \n",
    "- **self.data**: 위에서 정의한 데이터 전처리 메소드인 **preprocess**에서 데이터를 전처리한 결과가 저장됩니다. **self.data**의 shape은 (데이터의 총 길이, 9)가 됩니다.\n",
    "- **normalize**: 입력으로 사용하는 데이터의 각 속성은 저마다 값의 범위가 다릅니다. 예를들어 기온은 보통 -15에서 35사이의 값을 갖지만, 강수량과 풍속은 음수 값이 존재하지 않고, 기압의 경우에는 1000 내외의 값이 일반적입니다. 이러한 경우 각각의 속성들을 저마다의 평균과 표준편차를 통해 값을 정규화해주는 것이 바람직합니다. \n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "이제 다음을 읽고 코드를 완성해보세요.\n",
    "- **\\__len__**은 미리 구현을 해두었고, 이제 여러분이 직접 **\\__getitem__**을 구현해야 합니다. 모델의 입력으로 줄 데이터 시퀀스를 **sequence** 변수에 저장하고, 그 **sequence**의 가장 마지막 타임스텝으로부터 24시간 후의 기온을 **target** 변수에 저장하여 코드를 완성해보세요.\n",
    "- **sequence**의 shape은 (**seq_len**, 9)가 되어야 합니다. **index**는 **\\__len__**에서 정의한 데이터의 총 길이를 값의 범위로 합니다. 예를 들어 현재 가진 데이터셋의 총 길이와 주어진 **seq_len**, **stride**를 고려했을 때 존재할 수 있는 시작 인덱스 후보가 1000개라면 **\\__len__**는 1000을 반환할 것이고 **index**는 0~999 사이의 값을 가지게 됩니다. \n",
    "- **target**의 shape은 (1)이 되어야 합니다. \n",
    "- **힌트**: **sequence**를 정의하기 위해서는 아마 우리가 가진 전체 데이터, 즉 **self.data**에서 시작 인덱스부터 **seq_len** 길이만큼을 인덱싱을 해서 가져와야 할 것입니다. 이때 시작 인덱스로 **index**를 곧장 사용해서는 안됩니다. 앞서 언급한 것처럼 **index**는 0과 시작 인덱스 후보의 총 갯수 사이의 값이지, 그 자체로 시작 인덱스를 의미하는 것이 아닙니다. 시작 인덱스를 구하기 위해서는 **index**에 **self.stride**를 곱해주어야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, data_dir, mode, mean, std, seq_len=480, target_delay=24, stride=5, normalize=True):\n",
    "        self.seq_len = seq_len\n",
    "        self.target_delay = target_delay\n",
    "        self.stride = stride\n",
    "        all_files = glob.glob(os.path.join(data_dir, mode, '*'))\n",
    "        self.data = preprocess(all_files)\n",
    "        if normalize:\n",
    "            self.data = (self.data - mean) / std\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 코드 시작\n",
    "        index *= self.stride\n",
    "        sequence = self.data[index:index + self.seq_len, :]\n",
    "        target = self.data[index + self.seq_len + self.target_delay - 1]\n",
    "        target = target[0]\n",
    "        target = np.expand_dims(target, 0)\n",
    "        # 코드 종료\n",
    "        return sequence, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        max_idx = len(self.data) - self.seq_len - self.target_delay\n",
    "        num_of_idx = max_idx // self.stride\n",
    "        return num_of_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습용, 검증용, 테스트용 Dataset과 DataLoader를 각각 할당합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/climate_seoul'\n",
    "train_data = Dataset(data_dir, 'train', mean, std)\n",
    "val_data = Dataset(data_dir, 'val', mean, std)\n",
    "test_data = Dataset(data_dir, 'test', mean, std)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드를 실행해 코드를 성공적으로 완성했는지 확인해보세요. \n",
    "\n",
    "별다른 문제가 없다면 이어서 진행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__getitem__ 함수가 반환하는 target이 24시간 후의 기온이 아닙니다. 인덱싱을 올바르게 했는지 다시 확인하시기 바랍니다.\n",
      "__getitem__ 함수가 반환하는 sequence가 self.data로부터 올바르게 인덱싱되고 있지 않습니다. stride를 고려하여 시작 인덱스를 올바르게 구했는지 다시 확인하시기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "checker.customized_dataset_check(train_data.seq_len, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 데이터 샘플 시각화\n",
    "\n",
    "**train_data**의 첫번째 **sequence**에서 기온 정보만을 그래프 형태로 시각화합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp = train_data[0][0]\n",
    "temp = temp[:, 0]\n",
    "plt.plot(range(len(temp)), temp)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('temperature\\n(normalized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 베이스라인 성능 측정\n",
    "\n",
    "이후에 우리가 학습시킨 모델의 성능의 좋고 나쁨을 판단할 기준(베이스라인)이 필요합니다. 적당한 수준의 기준 성능을 정해놓고 테스트 결과에서 우리의 모델이 그것보다 더 좋은 성능을 보이면 만족할만한 모델을 학습시킨 것으로 간주하면 되고, 반대로 그보다 성능이 좋지 않다면 모델을 더 개선시키는 방식으로 네트워크 구조를 변경해 나아가면 됩니다.\n",
    "\n",
    "과거의 기후 정보를 활용하는 우리의 딥러닝 모델이 과연 의미있는 성능이라는 걸 보이기 위해서는 어떠한 예측 방식이 기준이 될 수 있을까요? 아마 기온 예측 전문가가 아닌 대부분의 여러분이 지금으로부터 24시간 후의 기온을 예측하라는 질문을 받았다고 생각해 봅시다. 아마 가장 무난하면서도 안정적으로 예측하는 방식은 지금의 기온과 같다고 답하는 것일 겁니다. 이러한 예측 방식은 과거의 기후 정보를 복잡하게 고려할 필요도 없이 간단하지만, 많은 경우에 실제로 꽤나 정확한 예측을 할 수 있는 방식입니다. 내일 낮 12시의 기온은 특이한 경우가 아니라면, 오늘 낮 12시의 기온과 거의 비슷할 것입니다. \n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "아래의 **eval_baseline**은 이런 예측 방식의 성능, 즉 평균적인 loss를 측정하는 메소드입니다. 다음을 읽고 코드를 완성해보세요. 단, \"#코드 시작\"과 \"#코드 종료\" 사이에 주어진 변수 명으로 코드를 작성하세요!\n",
    "- 우리의 베이스라인은 현재 기온을 24시간 후의 기온으로 예측하는 방식입니다. 현재 기후 정보는 **data_loader**에서 받은 **sequence**의 가장 마지막 타임스텝에 담겨있습니다. 입력으로 주는 **sequence**는 기온 뿐만아니라 총 9가지 속성이 포함된 것임에 유의하세요. 기온은 9가지 속성중 가장 첫번째에 위치하고 있음을 염두에 두고 **sequence**로 부터 현재 기온을 가져와 **pred** 변수에 저장하세요.\n",
    "- **pred**의 shape은 (batch_size, 1)이 되어야 합니다. 만약 여러분이 구한 **pred**의 shape이 (batch_size)라면, [torch.unsqueeze](https://pytorch.org/docs/stable/torch.html#torch.unsqueeze)를 활용하여 크기 1인 차원을 텐서에 삽입해보세요. \n",
    "- **criterion**이 우리가 선언한 loss function일 때, loss를 계산하여 **loss** 변수에 저장하세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_baseline(data_loader, criterion):\n",
    "    total_loss = 0\n",
    "    cnt = 0\n",
    "    for step, (sequence, target) in enumerate(data_loader):\n",
    "        # 코드 시작\n",
    "        pred = sequence[:, -1, 0].unsqueeze(1)\n",
    "        loss = criterion(pred, target)\n",
    "        # 코드 종료\n",
    "        total_loss += loss\n",
    "        cnt += 1\n",
    "    avrg_loss = total_loss / cnt\n",
    "    print('Baseline Average Loss: {:.4f}'.format(avrg_loss))\n",
    "    return avrg_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "베이스라인의 성능을 측정합니다. 다음과 같은 결과가 출력된다면 성공적으로 구현한 것입니다.\n",
    "\n",
    "Baseline Average Loss: 0.0963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Average Loss: 0.0963\n"
     ]
    }
   ],
   "source": [
    "baseline_loss = eval_baseline(test_loader, nn.MSELoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평균 loss만 봐서는 베이스라인이 어느 정도로 예측을 잘하는지 감이 잘 오지 않습니다. 베이스라인 모델의 예측 기온과 실제 기온을 몇가지 살펴보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 기온: 27.0 / 실제 기온: 26.3\n",
      "예측 기온: -2.0 / 실제 기온: 3.1\n",
      "예측 기온: 13.8 / 실제 기온: 13.4\n",
      "예측 기온: 25.4 / 실제 기온: 26.4\n",
      "예측 기온: 27.9 / 실제 기온: 29.4\n",
      "예측 기온: -1.5 / 실제 기온: 5.8\n",
      "예측 기온: 7.3 / 실제 기온: 3.4\n",
      "예측 기온: 14.1 / 실제 기온: 15.6\n",
      "예측 기온: 22.3 / 실제 기온: 25.4\n",
      "예측 기온: 22.9 / 실제 기온: 18.6\n",
      "예측 기온: 22.8 / 실제 기온: 24.3\n",
      "예측 기온: 25.1 / 실제 기온: 25.8\n",
      "예측 기온: 15.6 / 실제 기온: 10.7\n",
      "예측 기온: 4.4 / 실제 기온: 2.3\n",
      "예측 기온: 4.8 / 실제 기온: 8.6\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    data_idx = np.random.randint(len(test_data))\n",
    "    pred = test_data[data_idx][0][-1, 0]\n",
    "    pred = pred * std[0] + mean[0]  # 예측 기온을 normalization 이전 상태(섭씨 단위)로 되돌리는 작업\n",
    "    target = test_data[data_idx][1][0] * std[0] + mean[0]  # 실제 기온을 normalization 이전 상태(섭씨 단위)로 되돌리는 작업\n",
    "    print('예측 기온: {:.1f} / 실제 기온: {:.1f}'.format(pred, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 네트워크 설계\n",
    "\n",
    "우리는 LSTM 구조를 통해 기온 예측을 모델을 학습시킬 것입니다. 설계할 네트워크의 대략적인 개요는 아래 그림과 같습니다.\n",
    "\n",
    "<img src=\"./img/lstm.png\" width=\"80%\" height=\"60%\">\n",
    "\n",
    "LSTM의 매 타임스텝의 입력은 매 시간마다 기록된 9가지 속성값이 들어가게 됩니다. 그리고 마지막 타입스텝의 출력을 마무리로 Fully Connected 레이어에 넣어 최종 예측 기온값을 출력하는 구조입니다.  \n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "다음을 읽고 코드를 완성해보세요. 단, \"#코드 시작\"과 \"#코드 종료\" 사이에 주어진 변수 명으로 코드를 작성하세요!\n",
    "- 생성자의 파라미터 **hidden_size**는 우리가 설계할 LSTM 레이어의 hidden state의 크기입니다. 이 크기를 얼마를 할지는 역시 설계자의 몫입니다. 이번 예제에서 기본값은 100으로 하겠습니다. \n",
    "- **num_layers**는 LSTM 레이어의 총 레이어 수입니다. 기본값은 1로 하겠습니다.\n",
    "- [nn.LSTM](https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM)을 활용하여 생성자에 LSTM 레이어를 선언하고 이를 **self.lstm** 변수에 저장하세요. LSTM 레이어 선언에 필요한 파라미터인 **input_size**에 적절한 값을 넣어보세요. 또 다른 파라미터인 **hidden_size**, **num_layers**에는 각각 **self.hidden_size**, **self.num_layers**를 넣어주세요. 그리고 **batch_first=True**로 하시기 바랍니다. **batch_first=True**이면 LSTM 레이어의 입력의 shape이 (**batch_size**, 전체 시퀀스 길이, **input_size**)가 됩니다. 기본값은 **batch_first=False**인데, 이 경우 (전체 시퀀스 길이, **batch_size**, **input_size**)가 됩니다. \n",
    "- LSTM의 마지막 타임스텝의 출력을 입력으로 받아 최종 기온을 예측하는 FC 레이어를 선언하고 이를 **self.fc** 변수에 저장하세요. \n",
    "- **forward** 함수를 구현할 차례입니다. **forward**의 파라미터 **x**, **h**, **c**는 각각 LSTM의 입력, LSTM의 초기 hidden state, 초기 cell state를 의미합니다. Pytorch의 LSTM 레이어는 **x, (h, c)**를 입력으로 주면 **LSTM의 모든 타입스텝의 출력, (마지막 타임스텝의 hidden state, 마지막 타임스텝의 cell state)**를 반환합니다. (hidden state와 cell state을 입출력으로 주고 받을 때에는 둘을 괄호로 묶어 tuple 형태여야 함에 유의하시기 바랍니다.)\n",
    "- LSTM의 출력의 마지막 타임스텝의 출력을 FC 레이어에 입력으로 주어 얻은 결과를 **final_output** 변수에 저장하세요.\n",
    "- 마지막으로 **init_hidden** 함수를 구현할 차례입니다. **init_hidden**은 초기 hidden state, cell state를 만들어 반환하는 함수입니다. 초기 hidden state, cell state는 일반적으로 0으로 채운 값을 사용합니다. 0으로 채운 텐서를 생성하기 위해서는 [torch.zeros](https://pytorch.org/docs/stable/torch.html?highlight=zeros#torch.zeros)를 활용하시기 바랍니다. hidden state와 cell state의 shape은 (**num_layers**, **batch_size**, **hidden_size**)가 되어야 합니다. 0으로 채운 hidden state와 cell state를 선언하여 각각 **hidden**, **cell** 변수에 저장하세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=100, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # 코드 시작\n",
    "        self.lstm = nn.LSTM(input_size=9, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "        # 코드 종료\n",
    "    \n",
    "    def forward(self, x, h, c):\n",
    "        # 코드 시작\n",
    "        out, (h, c) = self.lstm(x, (h, c))\n",
    "        last_out = out[:, -1, :]\n",
    "        final_output = self.fc(last_out)\n",
    "        # 코드 종료\n",
    "        return final_output\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # 코드 시작\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        # 코드 종료\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드를 실행해 코드를 성공적으로 완성했는지 확인해보세요. \n",
    "\n",
    "별다른 문제가 없다면 이어서 진행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네트워를 잘 구현하셨습니다! 이어서 진행하셔도 좋습니다.\n"
     ]
    }
   ],
   "source": [
    "checker.model_check(SimpleLSTM(), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. train, validation, test 함수 정의\n",
    "\n",
    "이번에는 훈련, 검증, 테스트를 진행하는 함수를 정의하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "먼저 훈련 함수입니다. 다음을 읽고 코드를 완성해 보세요. 단, \"#코드 시작\"과 \"#코드 종료\" 사이에 주어진 변수 명으로 코드를 작성하세요!\n",
    "- 먼저 **sequence**, **target**의 데이터 형식을 **torch.float32**로 변환해주었습니다. 이렇게 해주는 이유는 현재 구현한 DataLoader에서는 **torch.float64** 형식으로 데이터를 반환하는데, 모델의 파라미터는 기본적으로 **torch.float32**로 이루어져 있기 때문입니다. 모델의 파라미터와 입력 텐서가 같은 데이터 형식을 갖도록 해주어야 합니다. Torch.Tensor의 다양한 데이터 형식에 대해 알아보려면 [여기](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype)를 참고하시기 바랍니다. \n",
    "- 위의 모델에서 구현했던 **init_hidden** 함수를 이용해 초기 hidden state와 cell state를 받아 각각 **h0**, **c0** 변수에 저장합니다.\n",
    "- **train** 함수에 여러 인자들이 보입니다. **model**이 우리가 선언한 모델일 때, 모델에 입력 시퀀스와, 초기 hidden state, cell state를 주고 얻은 결과를 **outputs**에 저장합니다.\n",
    "- **criterion**이 우리가 선언한 loss function일 때, **outputs**와 **target**을 통해 loss를 계산하고 그 결과를 **loss**에 저장합니다.\n",
    "- **optim**이 우리가 선언한 optimizer일 때, 이전에 계산한 기울기를 모두 clear하고, backpropagation을 통해 기울기를 계산하고, optimizer를 통해 파라미터를 업데이트합니다. \n",
    "\n",
    "**tarin**에서는 일정한 에폭마다 다음에 구현할 **validation**함수를 통해 검증을 수행합니다. 모델 검증을 수행했을 때, 만약 검증 과정의 평균 loss가 현재까지 가장 낮다면 가장 잘 훈련된 모델로 가정하고 그때까지 학습한 모델을 저장합니다. 저장은 추후에 구현할 **save_model** 함수가 수행합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, model, data_loader, criterion, optim, saved_dir, val_every):\n",
    "    print('Start training..')\n",
    "    best_loss = 9999999\n",
    "    for epoch in range(num_epochs):\n",
    "        for step, (sequence, target) in enumerate(data_loader):\n",
    "            sequence = sequence.type(torch.float32)\n",
    "            target = target.type(torch.float32)\n",
    "            # 코드 시작\n",
    "            h0, c0 = model.init_hidden(batch_size)\n",
    "            outputs = model(sequence, h0, c0)\n",
    "            loss = criterion(outputs, target)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 코드 종료\n",
    "            \n",
    "            if (step + 1) % 1 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
    "                    epoch+1, num_epochs, step+1, len(train_loader), loss.item()))\n",
    "                \n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            avrg_loss = validation(epoch + 1, model, val_loader, criterion)\n",
    "            if avrg_loss < best_loss:\n",
    "                print('Best performance at epoch: {}'.format(epoch + 1))\n",
    "                print('Save model in', saved_dir)\n",
    "                best_loss = avrg_loss\n",
    "                save_model(model, saved_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "검증 함수입니다. 다음을 읽고 코드를 완성해 보세요. 단, \"#코드 시작\"과 \"#코드 종료\" 사이에 주어진 변수 명으로 코드를 작성하세요!\n",
    "- 검증 과정에서는 파라미터 업데이트를 하지 않기 때문에 기울기를 계산할 필요는 없습니다. 하지만 검증 과정에서의 평균 loss를 계산하기 위해 loss는 계산해야 합니다. \n",
    "- 먼저 **train** 함수와 마찬가지로 초기 hidden state, cell state를 선언해 **h0**, **c0** 변수에 저장합니다. 그리고 **model**에 입력 시퀀스와 초기 hidden state, cell state를 주어 얻은 결과를 **outputs**에 저장하고, **criterion**을 통해 loss를 계산한 뒤, 그 결과를 **loss**에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch, model, data_loader, criterion):\n",
    "    print('Start validation #{}'.format(epoch))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        for step, (sequence, target) in enumerate(data_loader):\n",
    "            sequence = sequence.type(torch.float32)\n",
    "            target = target.type(torch.float32)\n",
    "            # 코드 시작\n",
    "            h0, c0 = model.init_hidden(batch_size)\n",
    "            outputs = model(sequence, h0, c0)\n",
    "            loss = criterion(outputs, target)\n",
    "            # 코드 종료\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "        avrg_loss = total_loss / cnt\n",
    "        print('Validation #{}  Average Loss: {:.4f}'.format(epoch, avrg_loss))\n",
    "    model.train()\n",
    "    return avrg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "테스트 함수입니다. 다음을 읽고 코드를 완성해 보세요. 단, \"#코드 시작\"과 \"#코드 종료\" 사이에 주어진 변수 명으로 코드를 작성하세요!\n",
    "\n",
    "- **validation** 함수와 마찬가지로 초기 hidden state, cell state를 선언해 **h0**, **c0** 변수에 저장합니다. 그리고 **model**에 입력 시퀀스와 초기 hidden state, cell state를 주어 얻은 결과를 **outputs**에 저장하고, **criterion**을 통해 loss를 계산한 뒤, 그 결과를 **loss**에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, criterion, baseline_loss):\n",
    "    print('Start test..')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        for step, (sequence, target) in enumerate(data_loader):\n",
    "            sequence = sequence.type(torch.float32)\n",
    "            target = target.type(torch.float32)\n",
    "            # 코드 시작\n",
    "            h0, c0 = model.init_hidden(batch_size)\n",
    "            outputs = model(sequence, h0, c0)\n",
    "            loss = criterion(outputs, target)\n",
    "            # 코드 종료\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "        avrg_loss = total_loss / cnt\n",
    "        print('Test  Average Loss: {:.4f}  Baseline Loss: {:.4f}'.format(avrg_loss, baseline_loss))\n",
    "        \n",
    "    if avrg_loss < baseline_loss:\n",
    "        print('베이스라인 성능을 뛰어 넘었습니다!')\n",
    "    else:\n",
    "        print('아쉽지만 베이스라인 성능을 넘지 못했습니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 모델 저장 함수 정의\n",
    "\n",
    "모델을 저장하는 함수입니다. 모델 저장은 [torch.save](https://pytorch.org/docs/stable/torch.html?highlight=save#torch.save) 함수를 통해 할 수 있습니다. \n",
    "[nn.Module.state_dict](https://pytorch.org/docs/stable/nn.html?highlight=state_dict#torch.nn.Module.state_dict)를 통해 Module, 즉 우리 모델의 파라미터를 가져올 수 있습니다. 이렇게 불러온 파라미터를 **check_point** 딕셔너리에 저장합니다. 그리고 이 **check_point**를 정해준 경로에 저장하면 됩니다. \n",
    "\n",
    "torch.save는 단순히 모델의 파라미터만 저장하는 함수가 아닙니다. 어떤 파이썬 객체든 저장할 수 있습니다. 그래서 경우에 따라 **check_point** 딕셔너리에 모델의 파라미터 뿐만 아니라 다른 여러 가지 필요한 정보를 저장할 수도 있습니다. 예를 들어 총 몇 에폭동안 학습한 모델인지 그 정보도 저장할 수 있겠죠? \n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "다음을 읽고 코드를 완성해보세요. \n",
    "- torch.save를 통해 **output_path** 경로에 **check_point**를 저장하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, saved_dir, file_name='best_model.pt'):\n",
    "    os.makedirs(saved_dir, exist_ok=True)\n",
    "    check_point = {\n",
    "        'net': model.state_dict()\n",
    "    }\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    # 코드 시작\n",
    "    torch.save(check_point, output_path)\n",
    "    # 코드 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 모델 생성 및 Loss function, Optimizer 정의\n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "이제 학습할 모델을 생성하고 loss function과 optimizer를 정의할 차례입니다. 다음을 읽고 코드를 완성해보세요. 단, \"#코드 시작\"과 \"#코드 종료\" 사이에 주어진 변수 명으로 코드를 작성하세요!\n",
    "- 위에서 정의한 SimpleLSTM class를 통해 모델을 생성하고 이를 **model** 변수에 저장합니다.\n",
    "- 연속된 값을 예측하는 회기(regression) 문제에서는 보통 Mean Squared Error(MSE) loss function을 사용합니다. [nn.MSELoss](https://pytorch.org/docs/stable/nn.html?highlight=mseloss#torch.nn.MSELoss)를 통해 MSE loss function을 선언하고 이를 **criterion** 변수에 저장합니다.\n",
    "- Adam optimizer를 통해 파라미터를 업데이트 하겠습니다. Adam optimizer([torch.optim.Adam](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam))를 **optimizer** 변수에 저장합니다.\n",
    "\n",
    "**val_every**는 검증을 몇 에폭마다 진행할지 정하는 변수입니다. **saved_dir**은 모델이 저장될 디렉토리의 경로입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7777) # 일관된 weight initialization을 위한 random seed 설정\n",
    "# 코드 시작\n",
    "model = SimpleLSTM()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# 코드 종료\n",
    "val_every = 1\n",
    "saved_dir = './saved/LSTM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드를 실행해 코드를 성공적으로 완성했는지 확인해보세요. \n",
    "\n",
    "별다른 문제가 없다면 이어서 진행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE loss function을 잘 정의하셨습니다! 이어서 진행하셔도 좋습니다.\n",
      "Adam optimizer를 잘 정의하셨습니다! 이어서 진행하셔도 좋습니다.\n"
     ]
    }
   ],
   "source": [
    "checker.loss_func_check(criterion)\n",
    "checker.optim_check(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training\n",
    "\n",
    "**train** 함수를 통해 모델 학습을 진행합니다. LSTM과 같은 recurrent layer는 타임스텝을 따라 계산이 순차적으로 진행을 해야 되기 때문에 상대적으로 학습 시간이 많이 소요됩니다. 따라서 시간이 여유가 없는 분들은 모델 학습이 적당히 진행된다는 정도만 확인하고 다음 단계로 넘어가셔도 됩니다.\n",
    "\n",
    "만약 한 에폭 이상이 지나도록 loss가 전혀 감소하는 기미가 보이지 않는다면 이전의 구현에 문제가 있을 가능성이 높으니 코드를 다시 검토해주시기 바랍니다.\n",
    "\n",
    "또한, 모델 저장 코드를 제대로 구현했다면 첫 에폭 학습후에 ./saved/LSTM 경로에 best_model.pt 파일이 저장되어 있어야 합니다. 만약에 파일이 존재하지 않는다면 모델 저장 코드를 다시 확인하시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training..\n",
      "Epoch [1/30], Step [1/139], Loss: 1.1552\n",
      "Epoch [1/30], Step [2/139], Loss: 0.9361\n",
      "Epoch [1/30], Step [3/139], Loss: 0.8526\n",
      "Epoch [1/30], Step [4/139], Loss: 0.9923\n",
      "Epoch [1/30], Step [5/139], Loss: 0.6953\n",
      "Epoch [1/30], Step [6/139], Loss: 0.7517\n",
      "Epoch [1/30], Step [7/139], Loss: 0.7115\n",
      "Epoch [1/30], Step [8/139], Loss: 0.7474\n",
      "Epoch [1/30], Step [9/139], Loss: 0.9836\n",
      "Epoch [1/30], Step [10/139], Loss: 0.8719\n",
      "Epoch [1/30], Step [11/139], Loss: 1.0787\n",
      "Epoch [1/30], Step [12/139], Loss: 0.7603\n",
      "Epoch [1/30], Step [13/139], Loss: 0.9822\n",
      "Epoch [1/30], Step [14/139], Loss: 0.9289\n",
      "Epoch [1/30], Step [15/139], Loss: 0.8981\n",
      "Epoch [1/30], Step [16/139], Loss: 0.9097\n",
      "Epoch [1/30], Step [17/139], Loss: 0.7176\n",
      "Epoch [1/30], Step [18/139], Loss: 0.9529\n",
      "Epoch [1/30], Step [19/139], Loss: 1.0760\n",
      "Epoch [1/30], Step [20/139], Loss: 0.9675\n",
      "Epoch [1/30], Step [21/139], Loss: 0.9815\n",
      "Epoch [1/30], Step [22/139], Loss: 1.0226\n",
      "Epoch [1/30], Step [23/139], Loss: 0.8701\n",
      "Epoch [1/30], Step [24/139], Loss: 0.9977\n",
      "Epoch [1/30], Step [25/139], Loss: 0.9700\n",
      "Epoch [1/30], Step [26/139], Loss: 0.9801\n",
      "Epoch [1/30], Step [27/139], Loss: 0.9595\n",
      "Epoch [1/30], Step [28/139], Loss: 0.9231\n",
      "Epoch [1/30], Step [29/139], Loss: 1.0389\n",
      "Epoch [1/30], Step [30/139], Loss: 0.8208\n",
      "Epoch [1/30], Step [31/139], Loss: 0.8613\n",
      "Epoch [1/30], Step [32/139], Loss: 0.8853\n",
      "Epoch [1/30], Step [33/139], Loss: 0.7881\n",
      "Epoch [1/30], Step [34/139], Loss: 0.8028\n",
      "Epoch [1/30], Step [35/139], Loss: 1.0496\n",
      "Epoch [1/30], Step [36/139], Loss: 0.8695\n",
      "Epoch [1/30], Step [37/139], Loss: 0.8946\n",
      "Epoch [1/30], Step [38/139], Loss: 0.8191\n",
      "Epoch [1/30], Step [39/139], Loss: 0.8414\n",
      "Epoch [1/30], Step [40/139], Loss: 1.0221\n",
      "Epoch [1/30], Step [41/139], Loss: 0.8637\n",
      "Epoch [1/30], Step [42/139], Loss: 0.8942\n",
      "Epoch [1/30], Step [43/139], Loss: 0.8910\n",
      "Epoch [1/30], Step [44/139], Loss: 0.9602\n",
      "Epoch [1/30], Step [45/139], Loss: 0.8580\n",
      "Epoch [1/30], Step [46/139], Loss: 0.7732\n",
      "Epoch [1/30], Step [47/139], Loss: 0.9536\n",
      "Epoch [1/30], Step [48/139], Loss: 0.9400\n",
      "Epoch [1/30], Step [49/139], Loss: 0.8224\n",
      "Epoch [1/30], Step [50/139], Loss: 0.9418\n",
      "Epoch [1/30], Step [51/139], Loss: 0.8889\n",
      "Epoch [1/30], Step [52/139], Loss: 1.0625\n",
      "Epoch [1/30], Step [53/139], Loss: 0.7664\n",
      "Epoch [1/30], Step [54/139], Loss: 0.8907\n",
      "Epoch [1/30], Step [55/139], Loss: 0.7882\n",
      "Epoch [1/30], Step [56/139], Loss: 0.8335\n",
      "Epoch [1/30], Step [57/139], Loss: 0.9332\n",
      "Epoch [1/30], Step [58/139], Loss: 0.9265\n",
      "Epoch [1/30], Step [59/139], Loss: 0.7828\n",
      "Epoch [1/30], Step [60/139], Loss: 0.7992\n",
      "Epoch [1/30], Step [61/139], Loss: 0.9047\n",
      "Epoch [1/30], Step [62/139], Loss: 0.9347\n",
      "Epoch [1/30], Step [63/139], Loss: 0.7573\n",
      "Epoch [1/30], Step [64/139], Loss: 0.7937\n",
      "Epoch [1/30], Step [65/139], Loss: 0.8332\n",
      "Epoch [1/30], Step [66/139], Loss: 0.9594\n",
      "Epoch [1/30], Step [67/139], Loss: 0.8239\n",
      "Epoch [1/30], Step [68/139], Loss: 0.8179\n",
      "Epoch [1/30], Step [69/139], Loss: 0.8150\n",
      "Epoch [1/30], Step [70/139], Loss: 0.8620\n",
      "Epoch [1/30], Step [71/139], Loss: 0.7605\n",
      "Epoch [1/30], Step [72/139], Loss: 0.7950\n",
      "Epoch [1/30], Step [73/139], Loss: 0.9410\n",
      "Epoch [1/30], Step [74/139], Loss: 0.8183\n",
      "Epoch [1/30], Step [75/139], Loss: 0.7330\n",
      "Epoch [1/30], Step [76/139], Loss: 0.8735\n",
      "Epoch [1/30], Step [77/139], Loss: 0.8934\n",
      "Epoch [1/30], Step [78/139], Loss: 0.7799\n",
      "Epoch [1/30], Step [79/139], Loss: 0.6659\n",
      "Epoch [1/30], Step [80/139], Loss: 0.8270\n",
      "Epoch [1/30], Step [81/139], Loss: 0.7240\n",
      "Epoch [1/30], Step [82/139], Loss: 0.7822\n",
      "Epoch [1/30], Step [83/139], Loss: 0.7871\n",
      "Epoch [1/30], Step [84/139], Loss: 0.7660\n",
      "Epoch [1/30], Step [85/139], Loss: 0.7451\n",
      "Epoch [1/30], Step [86/139], Loss: 0.8874\n",
      "Epoch [1/30], Step [87/139], Loss: 0.8687\n",
      "Epoch [1/30], Step [88/139], Loss: 0.7935\n",
      "Epoch [1/30], Step [89/139], Loss: 0.7737\n",
      "Epoch [1/30], Step [90/139], Loss: 0.6573\n",
      "Epoch [1/30], Step [91/139], Loss: 0.9744\n",
      "Epoch [1/30], Step [92/139], Loss: 0.8142\n",
      "Epoch [1/30], Step [93/139], Loss: 0.6883\n",
      "Epoch [1/30], Step [94/139], Loss: 0.9433\n",
      "Epoch [1/30], Step [95/139], Loss: 0.7956\n",
      "Epoch [1/30], Step [96/139], Loss: 0.8598\n",
      "Epoch [1/30], Step [97/139], Loss: 0.8087\n",
      "Epoch [1/30], Step [98/139], Loss: 0.7645\n",
      "Epoch [1/30], Step [99/139], Loss: 0.7351\n",
      "Epoch [1/30], Step [100/139], Loss: 0.7817\n",
      "Epoch [1/30], Step [101/139], Loss: 0.8025\n",
      "Epoch [1/30], Step [102/139], Loss: 0.7908\n",
      "Epoch [1/30], Step [103/139], Loss: 0.7122\n",
      "Epoch [1/30], Step [104/139], Loss: 0.8021\n",
      "Epoch [1/30], Step [105/139], Loss: 0.9098\n",
      "Epoch [1/30], Step [106/139], Loss: 0.7297\n",
      "Epoch [1/30], Step [107/139], Loss: 0.8660\n",
      "Epoch [1/30], Step [108/139], Loss: 0.7473\n",
      "Epoch [1/30], Step [109/139], Loss: 0.7817\n",
      "Epoch [1/30], Step [110/139], Loss: 0.7774\n",
      "Epoch [1/30], Step [111/139], Loss: 0.7112\n",
      "Epoch [1/30], Step [112/139], Loss: 1.0100\n",
      "Epoch [1/30], Step [113/139], Loss: 0.6537\n",
      "Epoch [1/30], Step [114/139], Loss: 0.7905\n",
      "Epoch [1/30], Step [115/139], Loss: 0.7681\n",
      "Epoch [1/30], Step [116/139], Loss: 0.8303\n",
      "Epoch [1/30], Step [117/139], Loss: 0.7578\n",
      "Epoch [1/30], Step [118/139], Loss: 0.6978\n",
      "Epoch [1/30], Step [119/139], Loss: 0.9127\n",
      "Epoch [1/30], Step [120/139], Loss: 0.6979\n",
      "Epoch [1/30], Step [121/139], Loss: 0.6510\n",
      "Epoch [1/30], Step [122/139], Loss: 0.6969\n",
      "Epoch [1/30], Step [123/139], Loss: 0.8733\n",
      "Epoch [1/30], Step [124/139], Loss: 0.6058\n",
      "Epoch [1/30], Step [125/139], Loss: 0.7335\n",
      "Epoch [1/30], Step [126/139], Loss: 0.8059\n",
      "Epoch [1/30], Step [127/139], Loss: 0.7327\n",
      "Epoch [1/30], Step [128/139], Loss: 0.7496\n",
      "Epoch [1/30], Step [129/139], Loss: 0.8498\n",
      "Epoch [1/30], Step [130/139], Loss: 0.7520\n",
      "Epoch [1/30], Step [131/139], Loss: 0.7636\n",
      "Epoch [1/30], Step [132/139], Loss: 0.9015\n",
      "Epoch [1/30], Step [133/139], Loss: 0.9383\n",
      "Epoch [1/30], Step [134/139], Loss: 0.6987\n",
      "Epoch [1/30], Step [135/139], Loss: 0.8355\n",
      "Epoch [1/30], Step [136/139], Loss: 0.6508\n",
      "Epoch [1/30], Step [137/139], Loss: 0.8442\n",
      "Epoch [1/30], Step [138/139], Loss: 0.7994\n",
      "Epoch [1/30], Step [139/139], Loss: 0.6321\n",
      "Start validation #1\n",
      "Validation #1  Average Loss: 0.5660\n",
      "Best performance at epoch: 1\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [2/30], Step [1/139], Loss: 0.7341\n",
      "Epoch [2/30], Step [2/139], Loss: 0.8797\n",
      "Epoch [2/30], Step [3/139], Loss: 0.6530\n",
      "Epoch [2/30], Step [4/139], Loss: 0.6902\n",
      "Epoch [2/30], Step [5/139], Loss: 0.7182\n",
      "Epoch [2/30], Step [6/139], Loss: 0.6824\n",
      "Epoch [2/30], Step [7/139], Loss: 0.7992\n",
      "Epoch [2/30], Step [8/139], Loss: 0.6507\n",
      "Epoch [2/30], Step [9/139], Loss: 0.7563\n",
      "Epoch [2/30], Step [10/139], Loss: 0.6988\n",
      "Epoch [2/30], Step [11/139], Loss: 0.7523\n",
      "Epoch [2/30], Step [12/139], Loss: 0.6905\n",
      "Epoch [2/30], Step [13/139], Loss: 0.8387\n",
      "Epoch [2/30], Step [14/139], Loss: 0.7862\n",
      "Epoch [2/30], Step [15/139], Loss: 0.6891\n",
      "Epoch [2/30], Step [16/139], Loss: 0.6552\n",
      "Epoch [2/30], Step [17/139], Loss: 0.8671\n",
      "Epoch [2/30], Step [18/139], Loss: 0.6931\n",
      "Epoch [2/30], Step [19/139], Loss: 0.5775\n",
      "Epoch [2/30], Step [20/139], Loss: 0.6888\n",
      "Epoch [2/30], Step [21/139], Loss: 0.6946\n",
      "Epoch [2/30], Step [22/139], Loss: 0.7229\n",
      "Epoch [2/30], Step [23/139], Loss: 0.6870\n",
      "Epoch [2/30], Step [24/139], Loss: 0.7453\n",
      "Epoch [2/30], Step [25/139], Loss: 0.6072\n",
      "Epoch [2/30], Step [26/139], Loss: 0.6825\n",
      "Epoch [2/30], Step [27/139], Loss: 0.6724\n",
      "Epoch [2/30], Step [28/139], Loss: 0.7216\n",
      "Epoch [2/30], Step [29/139], Loss: 0.6390\n",
      "Epoch [2/30], Step [30/139], Loss: 0.6384\n",
      "Epoch [2/30], Step [31/139], Loss: 0.7206\n",
      "Epoch [2/30], Step [32/139], Loss: 0.6749\n",
      "Epoch [2/30], Step [33/139], Loss: 0.7411\n",
      "Epoch [2/30], Step [34/139], Loss: 0.6678\n",
      "Epoch [2/30], Step [35/139], Loss: 0.8619\n",
      "Epoch [2/30], Step [36/139], Loss: 0.5694\n",
      "Epoch [2/30], Step [37/139], Loss: 0.5963\n",
      "Epoch [2/30], Step [38/139], Loss: 0.6495\n",
      "Epoch [2/30], Step [39/139], Loss: 0.6202\n",
      "Epoch [2/30], Step [40/139], Loss: 0.5685\n",
      "Epoch [2/30], Step [41/139], Loss: 0.6347\n",
      "Epoch [2/30], Step [42/139], Loss: 0.5904\n",
      "Epoch [2/30], Step [43/139], Loss: 0.5855\n",
      "Epoch [2/30], Step [44/139], Loss: 0.6371\n",
      "Epoch [2/30], Step [45/139], Loss: 0.8861\n",
      "Epoch [2/30], Step [46/139], Loss: 0.7680\n",
      "Epoch [2/30], Step [47/139], Loss: 0.6352\n",
      "Epoch [2/30], Step [48/139], Loss: 0.6331\n",
      "Epoch [2/30], Step [49/139], Loss: 0.5199\n",
      "Epoch [2/30], Step [50/139], Loss: 0.6220\n",
      "Epoch [2/30], Step [51/139], Loss: 0.6749\n",
      "Epoch [2/30], Step [52/139], Loss: 0.5800\n",
      "Epoch [2/30], Step [53/139], Loss: 0.6324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30], Step [54/139], Loss: 0.5127\n",
      "Epoch [2/30], Step [55/139], Loss: 0.6116\n",
      "Epoch [2/30], Step [56/139], Loss: 0.4479\n",
      "Epoch [2/30], Step [57/139], Loss: 0.5673\n",
      "Epoch [2/30], Step [58/139], Loss: 0.6859\n",
      "Epoch [2/30], Step [59/139], Loss: 0.6165\n",
      "Epoch [2/30], Step [60/139], Loss: 0.6162\n",
      "Epoch [2/30], Step [61/139], Loss: 0.6749\n",
      "Epoch [2/30], Step [62/139], Loss: 0.6457\n",
      "Epoch [2/30], Step [63/139], Loss: 0.6356\n",
      "Epoch [2/30], Step [64/139], Loss: 0.6215\n",
      "Epoch [2/30], Step [65/139], Loss: 0.6873\n",
      "Epoch [2/30], Step [66/139], Loss: 0.6626\n",
      "Epoch [2/30], Step [67/139], Loss: 0.6103\n",
      "Epoch [2/30], Step [68/139], Loss: 0.6690\n",
      "Epoch [2/30], Step [69/139], Loss: 0.6093\n",
      "Epoch [2/30], Step [70/139], Loss: 0.6022\n",
      "Epoch [2/30], Step [71/139], Loss: 0.4736\n",
      "Epoch [2/30], Step [72/139], Loss: 0.4476\n",
      "Epoch [2/30], Step [73/139], Loss: 0.5451\n",
      "Epoch [2/30], Step [74/139], Loss: 0.4326\n",
      "Epoch [2/30], Step [75/139], Loss: 0.5647\n",
      "Epoch [2/30], Step [76/139], Loss: 0.6835\n",
      "Epoch [2/30], Step [77/139], Loss: 0.6200\n",
      "Epoch [2/30], Step [78/139], Loss: 0.6392\n",
      "Epoch [2/30], Step [79/139], Loss: 0.4640\n",
      "Epoch [2/30], Step [80/139], Loss: 0.6112\n",
      "Epoch [2/30], Step [81/139], Loss: 0.5234\n",
      "Epoch [2/30], Step [82/139], Loss: 0.6350\n",
      "Epoch [2/30], Step [83/139], Loss: 0.6001\n",
      "Epoch [2/30], Step [84/139], Loss: 0.5684\n",
      "Epoch [2/30], Step [85/139], Loss: 0.5250\n",
      "Epoch [2/30], Step [86/139], Loss: 0.4747\n",
      "Epoch [2/30], Step [87/139], Loss: 0.5475\n",
      "Epoch [2/30], Step [88/139], Loss: 0.6036\n",
      "Epoch [2/30], Step [89/139], Loss: 0.4584\n",
      "Epoch [2/30], Step [90/139], Loss: 0.5670\n",
      "Epoch [2/30], Step [91/139], Loss: 0.5655\n",
      "Epoch [2/30], Step [92/139], Loss: 0.4810\n",
      "Epoch [2/30], Step [93/139], Loss: 0.4971\n",
      "Epoch [2/30], Step [94/139], Loss: 0.5424\n",
      "Epoch [2/30], Step [95/139], Loss: 0.5742\n",
      "Epoch [2/30], Step [96/139], Loss: 0.5078\n",
      "Epoch [2/30], Step [97/139], Loss: 0.4295\n",
      "Epoch [2/30], Step [98/139], Loss: 0.4533\n",
      "Epoch [2/30], Step [99/139], Loss: 0.4418\n",
      "Epoch [2/30], Step [100/139], Loss: 0.6071\n",
      "Epoch [2/30], Step [101/139], Loss: 0.5139\n",
      "Epoch [2/30], Step [102/139], Loss: 0.5384\n",
      "Epoch [2/30], Step [103/139], Loss: 0.6084\n",
      "Epoch [2/30], Step [104/139], Loss: 0.4425\n",
      "Epoch [2/30], Step [105/139], Loss: 0.4829\n",
      "Epoch [2/30], Step [106/139], Loss: 0.4146\n",
      "Epoch [2/30], Step [107/139], Loss: 0.4317\n",
      "Epoch [2/30], Step [108/139], Loss: 0.4624\n",
      "Epoch [2/30], Step [109/139], Loss: 0.4544\n",
      "Epoch [2/30], Step [110/139], Loss: 0.4693\n",
      "Epoch [2/30], Step [111/139], Loss: 0.4150\n",
      "Epoch [2/30], Step [112/139], Loss: 0.4192\n",
      "Epoch [2/30], Step [113/139], Loss: 0.4587\n",
      "Epoch [2/30], Step [114/139], Loss: 0.4304\n",
      "Epoch [2/30], Step [115/139], Loss: 0.4830\n",
      "Epoch [2/30], Step [116/139], Loss: 0.4994\n",
      "Epoch [2/30], Step [117/139], Loss: 0.4226\n",
      "Epoch [2/30], Step [118/139], Loss: 0.4801\n",
      "Epoch [2/30], Step [119/139], Loss: 0.5059\n",
      "Epoch [2/30], Step [120/139], Loss: 0.3955\n",
      "Epoch [2/30], Step [121/139], Loss: 0.4217\n",
      "Epoch [2/30], Step [122/139], Loss: 0.4468\n",
      "Epoch [2/30], Step [123/139], Loss: 0.4793\n",
      "Epoch [2/30], Step [124/139], Loss: 0.4335\n",
      "Epoch [2/30], Step [125/139], Loss: 0.4465\n",
      "Epoch [2/30], Step [126/139], Loss: 0.4735\n",
      "Epoch [2/30], Step [127/139], Loss: 0.3452\n",
      "Epoch [2/30], Step [128/139], Loss: 0.5462\n",
      "Epoch [2/30], Step [129/139], Loss: 0.2653\n",
      "Epoch [2/30], Step [130/139], Loss: 0.4925\n",
      "Epoch [2/30], Step [131/139], Loss: 0.3836\n",
      "Epoch [2/30], Step [132/139], Loss: 0.3661\n",
      "Epoch [2/30], Step [133/139], Loss: 0.4062\n",
      "Epoch [2/30], Step [134/139], Loss: 0.5049\n",
      "Epoch [2/30], Step [135/139], Loss: 0.4105\n",
      "Epoch [2/30], Step [136/139], Loss: 0.3740\n",
      "Epoch [2/30], Step [137/139], Loss: 0.3865\n",
      "Epoch [2/30], Step [138/139], Loss: 0.3639\n",
      "Epoch [2/30], Step [139/139], Loss: 0.4125\n",
      "Start validation #2\n",
      "Validation #2  Average Loss: 0.3217\n",
      "Best performance at epoch: 2\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [3/30], Step [1/139], Loss: 0.4902\n",
      "Epoch [3/30], Step [2/139], Loss: 0.4377\n",
      "Epoch [3/30], Step [3/139], Loss: 0.4345\n",
      "Epoch [3/30], Step [4/139], Loss: 0.4117\n",
      "Epoch [3/30], Step [5/139], Loss: 0.4274\n",
      "Epoch [3/30], Step [6/139], Loss: 0.3870\n",
      "Epoch [3/30], Step [7/139], Loss: 0.4068\n",
      "Epoch [3/30], Step [8/139], Loss: 0.3665\n",
      "Epoch [3/30], Step [9/139], Loss: 0.3500\n",
      "Epoch [3/30], Step [10/139], Loss: 0.4104\n",
      "Epoch [3/30], Step [11/139], Loss: 0.3596\n",
      "Epoch [3/30], Step [12/139], Loss: 0.3102\n",
      "Epoch [3/30], Step [13/139], Loss: 0.3937\n",
      "Epoch [3/30], Step [14/139], Loss: 0.3677\n",
      "Epoch [3/30], Step [15/139], Loss: 0.2796\n",
      "Epoch [3/30], Step [16/139], Loss: 0.2902\n",
      "Epoch [3/30], Step [17/139], Loss: 0.3754\n",
      "Epoch [3/30], Step [18/139], Loss: 0.2908\n",
      "Epoch [3/30], Step [19/139], Loss: 0.4240\n",
      "Epoch [3/30], Step [20/139], Loss: 0.3238\n",
      "Epoch [3/30], Step [21/139], Loss: 0.2960\n",
      "Epoch [3/30], Step [22/139], Loss: 0.3094\n",
      "Epoch [3/30], Step [23/139], Loss: 0.3533\n",
      "Epoch [3/30], Step [24/139], Loss: 0.3253\n",
      "Epoch [3/30], Step [25/139], Loss: 0.2907\n",
      "Epoch [3/30], Step [26/139], Loss: 0.3077\n",
      "Epoch [3/30], Step [27/139], Loss: 0.2744\n",
      "Epoch [3/30], Step [28/139], Loss: 0.3534\n",
      "Epoch [3/30], Step [29/139], Loss: 0.2757\n",
      "Epoch [3/30], Step [30/139], Loss: 0.3433\n",
      "Epoch [3/30], Step [31/139], Loss: 0.2778\n",
      "Epoch [3/30], Step [32/139], Loss: 0.2625\n",
      "Epoch [3/30], Step [33/139], Loss: 0.2844\n",
      "Epoch [3/30], Step [34/139], Loss: 0.2991\n",
      "Epoch [3/30], Step [35/139], Loss: 0.2996\n",
      "Epoch [3/30], Step [36/139], Loss: 0.2243\n",
      "Epoch [3/30], Step [37/139], Loss: 0.2468\n",
      "Epoch [3/30], Step [38/139], Loss: 0.2274\n",
      "Epoch [3/30], Step [39/139], Loss: 0.2019\n",
      "Epoch [3/30], Step [40/139], Loss: 0.3286\n",
      "Epoch [3/30], Step [41/139], Loss: 0.3276\n",
      "Epoch [3/30], Step [42/139], Loss: 0.1890\n",
      "Epoch [3/30], Step [43/139], Loss: 0.2602\n",
      "Epoch [3/30], Step [44/139], Loss: 0.2774\n",
      "Epoch [3/30], Step [45/139], Loss: 0.2325\n",
      "Epoch [3/30], Step [46/139], Loss: 0.1951\n",
      "Epoch [3/30], Step [47/139], Loss: 0.1883\n",
      "Epoch [3/30], Step [48/139], Loss: 0.2579\n",
      "Epoch [3/30], Step [49/139], Loss: 0.2205\n",
      "Epoch [3/30], Step [50/139], Loss: 0.1834\n",
      "Epoch [3/30], Step [51/139], Loss: 0.2063\n",
      "Epoch [3/30], Step [52/139], Loss: 0.2028\n",
      "Epoch [3/30], Step [53/139], Loss: 0.2853\n",
      "Epoch [3/30], Step [54/139], Loss: 0.2072\n",
      "Epoch [3/30], Step [55/139], Loss: 0.2041\n",
      "Epoch [3/30], Step [56/139], Loss: 0.1791\n",
      "Epoch [3/30], Step [57/139], Loss: 0.2047\n",
      "Epoch [3/30], Step [58/139], Loss: 0.1875\n",
      "Epoch [3/30], Step [59/139], Loss: 0.1945\n",
      "Epoch [3/30], Step [60/139], Loss: 0.1988\n",
      "Epoch [3/30], Step [61/139], Loss: 0.1584\n",
      "Epoch [3/30], Step [62/139], Loss: 0.2314\n",
      "Epoch [3/30], Step [63/139], Loss: 0.2045\n",
      "Epoch [3/30], Step [64/139], Loss: 0.2493\n",
      "Epoch [3/30], Step [65/139], Loss: 0.2470\n",
      "Epoch [3/30], Step [66/139], Loss: 0.1448\n",
      "Epoch [3/30], Step [67/139], Loss: 0.2267\n",
      "Epoch [3/30], Step [68/139], Loss: 0.1578\n",
      "Epoch [3/30], Step [69/139], Loss: 0.1920\n",
      "Epoch [3/30], Step [70/139], Loss: 0.1494\n",
      "Epoch [3/30], Step [71/139], Loss: 0.2232\n",
      "Epoch [3/30], Step [72/139], Loss: 0.1981\n",
      "Epoch [3/30], Step [73/139], Loss: 0.1895\n",
      "Epoch [3/30], Step [74/139], Loss: 0.1999\n",
      "Epoch [3/30], Step [75/139], Loss: 0.1517\n",
      "Epoch [3/30], Step [76/139], Loss: 0.2016\n",
      "Epoch [3/30], Step [77/139], Loss: 0.1788\n",
      "Epoch [3/30], Step [78/139], Loss: 0.1886\n",
      "Epoch [3/30], Step [79/139], Loss: 0.1957\n",
      "Epoch [3/30], Step [80/139], Loss: 0.2128\n",
      "Epoch [3/30], Step [81/139], Loss: 0.1526\n",
      "Epoch [3/30], Step [82/139], Loss: 0.1465\n",
      "Epoch [3/30], Step [83/139], Loss: 0.1900\n",
      "Epoch [3/30], Step [84/139], Loss: 0.1697\n",
      "Epoch [3/30], Step [85/139], Loss: 0.1985\n",
      "Epoch [3/30], Step [86/139], Loss: 0.1997\n",
      "Epoch [3/30], Step [87/139], Loss: 0.1366\n",
      "Epoch [3/30], Step [88/139], Loss: 0.1422\n",
      "Epoch [3/30], Step [89/139], Loss: 0.1971\n",
      "Epoch [3/30], Step [90/139], Loss: 0.1629\n",
      "Epoch [3/30], Step [91/139], Loss: 0.1166\n",
      "Epoch [3/30], Step [92/139], Loss: 0.1540\n",
      "Epoch [3/30], Step [93/139], Loss: 0.1740\n",
      "Epoch [3/30], Step [94/139], Loss: 0.1573\n",
      "Epoch [3/30], Step [95/139], Loss: 0.1707\n",
      "Epoch [3/30], Step [96/139], Loss: 0.1641\n",
      "Epoch [3/30], Step [97/139], Loss: 0.1504\n",
      "Epoch [3/30], Step [98/139], Loss: 0.1763\n",
      "Epoch [3/30], Step [99/139], Loss: 0.2047\n",
      "Epoch [3/30], Step [100/139], Loss: 0.1666\n",
      "Epoch [3/30], Step [101/139], Loss: 0.1645\n",
      "Epoch [3/30], Step [102/139], Loss: 0.1345\n",
      "Epoch [3/30], Step [103/139], Loss: 0.1806\n",
      "Epoch [3/30], Step [104/139], Loss: 0.1908\n",
      "Epoch [3/30], Step [105/139], Loss: 0.1440\n",
      "Epoch [3/30], Step [106/139], Loss: 0.1923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30], Step [107/139], Loss: 0.1482\n",
      "Epoch [3/30], Step [108/139], Loss: 0.1664\n",
      "Epoch [3/30], Step [109/139], Loss: 0.1802\n",
      "Epoch [3/30], Step [110/139], Loss: 0.1402\n",
      "Epoch [3/30], Step [111/139], Loss: 0.1777\n",
      "Epoch [3/30], Step [112/139], Loss: 0.1354\n",
      "Epoch [3/30], Step [113/139], Loss: 0.1452\n",
      "Epoch [3/30], Step [114/139], Loss: 0.1523\n",
      "Epoch [3/30], Step [115/139], Loss: 0.1513\n",
      "Epoch [3/30], Step [116/139], Loss: 0.1913\n",
      "Epoch [3/30], Step [117/139], Loss: 0.1250\n",
      "Epoch [3/30], Step [118/139], Loss: 0.1211\n",
      "Epoch [3/30], Step [119/139], Loss: 0.1736\n",
      "Epoch [3/30], Step [120/139], Loss: 0.1359\n",
      "Epoch [3/30], Step [121/139], Loss: 0.1880\n",
      "Epoch [3/30], Step [122/139], Loss: 0.2198\n",
      "Epoch [3/30], Step [123/139], Loss: 0.1060\n",
      "Epoch [3/30], Step [124/139], Loss: 0.1430\n",
      "Epoch [3/30], Step [125/139], Loss: 0.1292\n",
      "Epoch [3/30], Step [126/139], Loss: 0.1109\n",
      "Epoch [3/30], Step [127/139], Loss: 0.1086\n",
      "Epoch [3/30], Step [128/139], Loss: 0.1255\n",
      "Epoch [3/30], Step [129/139], Loss: 0.1355\n",
      "Epoch [3/30], Step [130/139], Loss: 0.1665\n",
      "Epoch [3/30], Step [131/139], Loss: 0.1153\n",
      "Epoch [3/30], Step [132/139], Loss: 0.1535\n",
      "Epoch [3/30], Step [133/139], Loss: 0.1329\n",
      "Epoch [3/30], Step [134/139], Loss: 0.0983\n",
      "Epoch [3/30], Step [135/139], Loss: 0.1356\n",
      "Epoch [3/30], Step [136/139], Loss: 0.1213\n",
      "Epoch [3/30], Step [137/139], Loss: 0.1612\n",
      "Epoch [3/30], Step [138/139], Loss: 0.1271\n",
      "Epoch [3/30], Step [139/139], Loss: 0.1365\n",
      "Start validation #3\n",
      "Validation #3  Average Loss: 0.1290\n",
      "Best performance at epoch: 3\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [4/30], Step [1/139], Loss: 0.1159\n",
      "Epoch [4/30], Step [2/139], Loss: 0.1220\n",
      "Epoch [4/30], Step [3/139], Loss: 0.1451\n",
      "Epoch [4/30], Step [4/139], Loss: 0.1050\n",
      "Epoch [4/30], Step [5/139], Loss: 0.1148\n",
      "Epoch [4/30], Step [6/139], Loss: 0.1724\n",
      "Epoch [4/30], Step [7/139], Loss: 0.1491\n",
      "Epoch [4/30], Step [8/139], Loss: 0.1160\n",
      "Epoch [4/30], Step [9/139], Loss: 0.1101\n",
      "Epoch [4/30], Step [10/139], Loss: 0.1390\n",
      "Epoch [4/30], Step [11/139], Loss: 0.1517\n",
      "Epoch [4/30], Step [12/139], Loss: 0.1179\n",
      "Epoch [4/30], Step [13/139], Loss: 0.1500\n",
      "Epoch [4/30], Step [14/139], Loss: 0.1287\n",
      "Epoch [4/30], Step [15/139], Loss: 0.1454\n",
      "Epoch [4/30], Step [16/139], Loss: 0.1326\n",
      "Epoch [4/30], Step [17/139], Loss: 0.1483\n",
      "Epoch [4/30], Step [18/139], Loss: 0.1931\n",
      "Epoch [4/30], Step [19/139], Loss: 0.0927\n",
      "Epoch [4/30], Step [20/139], Loss: 0.1497\n",
      "Epoch [4/30], Step [21/139], Loss: 0.1781\n",
      "Epoch [4/30], Step [22/139], Loss: 0.1540\n",
      "Epoch [4/30], Step [23/139], Loss: 0.1457\n",
      "Epoch [4/30], Step [24/139], Loss: 0.0799\n",
      "Epoch [4/30], Step [25/139], Loss: 0.1305\n",
      "Epoch [4/30], Step [26/139], Loss: 0.1198\n",
      "Epoch [4/30], Step [27/139], Loss: 0.0959\n",
      "Epoch [4/30], Step [28/139], Loss: 0.1681\n",
      "Epoch [4/30], Step [29/139], Loss: 0.1570\n",
      "Epoch [4/30], Step [30/139], Loss: 0.1223\n",
      "Epoch [4/30], Step [31/139], Loss: 0.1308\n",
      "Epoch [4/30], Step [32/139], Loss: 0.1263\n",
      "Epoch [4/30], Step [33/139], Loss: 0.1544\n",
      "Epoch [4/30], Step [34/139], Loss: 0.1583\n",
      "Epoch [4/30], Step [35/139], Loss: 0.1271\n",
      "Epoch [4/30], Step [36/139], Loss: 0.1500\n",
      "Epoch [4/30], Step [37/139], Loss: 0.1262\n",
      "Epoch [4/30], Step [38/139], Loss: 0.1427\n",
      "Epoch [4/30], Step [39/139], Loss: 0.1289\n",
      "Epoch [4/30], Step [40/139], Loss: 0.1773\n",
      "Epoch [4/30], Step [41/139], Loss: 0.1255\n",
      "Epoch [4/30], Step [42/139], Loss: 0.1536\n",
      "Epoch [4/30], Step [43/139], Loss: 0.1158\n",
      "Epoch [4/30], Step [44/139], Loss: 0.1419\n",
      "Epoch [4/30], Step [45/139], Loss: 0.1178\n",
      "Epoch [4/30], Step [46/139], Loss: 0.1323\n",
      "Epoch [4/30], Step [47/139], Loss: 0.1629\n",
      "Epoch [4/30], Step [48/139], Loss: 0.1446\n",
      "Epoch [4/30], Step [49/139], Loss: 0.1332\n",
      "Epoch [4/30], Step [50/139], Loss: 0.1524\n",
      "Epoch [4/30], Step [51/139], Loss: 0.1317\n",
      "Epoch [4/30], Step [52/139], Loss: 0.1236\n",
      "Epoch [4/30], Step [53/139], Loss: 0.1215\n",
      "Epoch [4/30], Step [54/139], Loss: 0.1215\n",
      "Epoch [4/30], Step [55/139], Loss: 0.1396\n",
      "Epoch [4/30], Step [56/139], Loss: 0.1444\n",
      "Epoch [4/30], Step [57/139], Loss: 0.1228\n",
      "Epoch [4/30], Step [58/139], Loss: 0.0852\n",
      "Epoch [4/30], Step [59/139], Loss: 0.1015\n",
      "Epoch [4/30], Step [60/139], Loss: 0.1246\n",
      "Epoch [4/30], Step [61/139], Loss: 0.1200\n",
      "Epoch [4/30], Step [62/139], Loss: 0.1343\n",
      "Epoch [4/30], Step [63/139], Loss: 0.1236\n",
      "Epoch [4/30], Step [64/139], Loss: 0.1607\n",
      "Epoch [4/30], Step [65/139], Loss: 0.1036\n",
      "Epoch [4/30], Step [66/139], Loss: 0.1186\n",
      "Epoch [4/30], Step [67/139], Loss: 0.1185\n",
      "Epoch [4/30], Step [68/139], Loss: 0.1042\n",
      "Epoch [4/30], Step [69/139], Loss: 0.1113\n",
      "Epoch [4/30], Step [70/139], Loss: 0.1437\n",
      "Epoch [4/30], Step [71/139], Loss: 0.0990\n",
      "Epoch [4/30], Step [72/139], Loss: 0.1279\n",
      "Epoch [4/30], Step [73/139], Loss: 0.1340\n",
      "Epoch [4/30], Step [74/139], Loss: 0.1455\n",
      "Epoch [4/30], Step [75/139], Loss: 0.1373\n",
      "Epoch [4/30], Step [76/139], Loss: 0.1222\n",
      "Epoch [4/30], Step [77/139], Loss: 0.1399\n",
      "Epoch [4/30], Step [78/139], Loss: 0.1255\n",
      "Epoch [4/30], Step [79/139], Loss: 0.1176\n",
      "Epoch [4/30], Step [80/139], Loss: 0.1157\n",
      "Epoch [4/30], Step [81/139], Loss: 0.1034\n",
      "Epoch [4/30], Step [82/139], Loss: 0.1277\n",
      "Epoch [4/30], Step [83/139], Loss: 0.1459\n",
      "Epoch [4/30], Step [84/139], Loss: 0.0995\n",
      "Epoch [4/30], Step [85/139], Loss: 0.1016\n",
      "Epoch [4/30], Step [86/139], Loss: 0.1131\n",
      "Epoch [4/30], Step [87/139], Loss: 0.1451\n",
      "Epoch [4/30], Step [88/139], Loss: 0.1092\n",
      "Epoch [4/30], Step [89/139], Loss: 0.1333\n",
      "Epoch [4/30], Step [90/139], Loss: 0.1549\n",
      "Epoch [4/30], Step [91/139], Loss: 0.1402\n",
      "Epoch [4/30], Step [92/139], Loss: 0.1264\n",
      "Epoch [4/30], Step [93/139], Loss: 0.1356\n",
      "Epoch [4/30], Step [94/139], Loss: 0.1227\n",
      "Epoch [4/30], Step [95/139], Loss: 0.1273\n",
      "Epoch [4/30], Step [96/139], Loss: 0.1173\n",
      "Epoch [4/30], Step [97/139], Loss: 0.1387\n",
      "Epoch [4/30], Step [98/139], Loss: 0.0902\n",
      "Epoch [4/30], Step [99/139], Loss: 0.1032\n",
      "Epoch [4/30], Step [100/139], Loss: 0.1546\n",
      "Epoch [4/30], Step [101/139], Loss: 0.1195\n",
      "Epoch [4/30], Step [102/139], Loss: 0.1103\n",
      "Epoch [4/30], Step [103/139], Loss: 0.1301\n",
      "Epoch [4/30], Step [104/139], Loss: 0.1570\n",
      "Epoch [4/30], Step [105/139], Loss: 0.1586\n",
      "Epoch [4/30], Step [106/139], Loss: 0.1710\n",
      "Epoch [4/30], Step [107/139], Loss: 0.1316\n",
      "Epoch [4/30], Step [108/139], Loss: 0.1342\n",
      "Epoch [4/30], Step [109/139], Loss: 0.1419\n",
      "Epoch [4/30], Step [110/139], Loss: 0.1049\n",
      "Epoch [4/30], Step [111/139], Loss: 0.1031\n",
      "Epoch [4/30], Step [112/139], Loss: 0.1602\n",
      "Epoch [4/30], Step [113/139], Loss: 0.1073\n",
      "Epoch [4/30], Step [114/139], Loss: 0.1153\n",
      "Epoch [4/30], Step [115/139], Loss: 0.1279\n",
      "Epoch [4/30], Step [116/139], Loss: 0.1110\n",
      "Epoch [4/30], Step [117/139], Loss: 0.1344\n",
      "Epoch [4/30], Step [118/139], Loss: 0.0883\n",
      "Epoch [4/30], Step [119/139], Loss: 0.1562\n",
      "Epoch [4/30], Step [120/139], Loss: 0.0994\n",
      "Epoch [4/30], Step [121/139], Loss: 0.1114\n",
      "Epoch [4/30], Step [122/139], Loss: 0.1290\n",
      "Epoch [4/30], Step [123/139], Loss: 0.1380\n",
      "Epoch [4/30], Step [124/139], Loss: 0.1064\n",
      "Epoch [4/30], Step [125/139], Loss: 0.1023\n",
      "Epoch [4/30], Step [126/139], Loss: 0.0826\n",
      "Epoch [4/30], Step [127/139], Loss: 0.1464\n",
      "Epoch [4/30], Step [128/139], Loss: 0.1147\n",
      "Epoch [4/30], Step [129/139], Loss: 0.0937\n",
      "Epoch [4/30], Step [130/139], Loss: 0.1213\n",
      "Epoch [4/30], Step [131/139], Loss: 0.1158\n",
      "Epoch [4/30], Step [132/139], Loss: 0.1236\n",
      "Epoch [4/30], Step [133/139], Loss: 0.1128\n",
      "Epoch [4/30], Step [134/139], Loss: 0.1142\n",
      "Epoch [4/30], Step [135/139], Loss: 0.1043\n",
      "Epoch [4/30], Step [136/139], Loss: 0.0889\n",
      "Epoch [4/30], Step [137/139], Loss: 0.1164\n",
      "Epoch [4/30], Step [138/139], Loss: 0.1118\n",
      "Epoch [4/30], Step [139/139], Loss: 0.1255\n",
      "Start validation #4\n",
      "Validation #4  Average Loss: 0.1121\n",
      "Best performance at epoch: 4\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [5/30], Step [1/139], Loss: 0.1229\n",
      "Epoch [5/30], Step [2/139], Loss: 0.1056\n",
      "Epoch [5/30], Step [3/139], Loss: 0.1342\n",
      "Epoch [5/30], Step [4/139], Loss: 0.1229\n",
      "Epoch [5/30], Step [5/139], Loss: 0.1020\n",
      "Epoch [5/30], Step [6/139], Loss: 0.1321\n",
      "Epoch [5/30], Step [7/139], Loss: 0.1232\n",
      "Epoch [5/30], Step [8/139], Loss: 0.1242\n",
      "Epoch [5/30], Step [9/139], Loss: 0.1078\n",
      "Epoch [5/30], Step [10/139], Loss: 0.1043\n",
      "Epoch [5/30], Step [11/139], Loss: 0.0819\n",
      "Epoch [5/30], Step [12/139], Loss: 0.1133\n",
      "Epoch [5/30], Step [13/139], Loss: 0.1195\n",
      "Epoch [5/30], Step [14/139], Loss: 0.1005\n",
      "Epoch [5/30], Step [15/139], Loss: 0.1136\n",
      "Epoch [5/30], Step [16/139], Loss: 0.0995\n",
      "Epoch [5/30], Step [17/139], Loss: 0.1070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Step [18/139], Loss: 0.1338\n",
      "Epoch [5/30], Step [19/139], Loss: 0.0998\n",
      "Epoch [5/30], Step [20/139], Loss: 0.1090\n",
      "Epoch [5/30], Step [21/139], Loss: 0.1053\n",
      "Epoch [5/30], Step [22/139], Loss: 0.1215\n",
      "Epoch [5/30], Step [23/139], Loss: 0.1048\n",
      "Epoch [5/30], Step [24/139], Loss: 0.0987\n",
      "Epoch [5/30], Step [25/139], Loss: 0.1329\n",
      "Epoch [5/30], Step [26/139], Loss: 0.1087\n",
      "Epoch [5/30], Step [27/139], Loss: 0.1098\n",
      "Epoch [5/30], Step [28/139], Loss: 0.1097\n",
      "Epoch [5/30], Step [29/139], Loss: 0.1124\n",
      "Epoch [5/30], Step [30/139], Loss: 0.1804\n",
      "Epoch [5/30], Step [31/139], Loss: 0.1219\n",
      "Epoch [5/30], Step [32/139], Loss: 0.1160\n",
      "Epoch [5/30], Step [33/139], Loss: 0.1253\n",
      "Epoch [5/30], Step [34/139], Loss: 0.1242\n",
      "Epoch [5/30], Step [35/139], Loss: 0.1206\n",
      "Epoch [5/30], Step [36/139], Loss: 0.1404\n",
      "Epoch [5/30], Step [37/139], Loss: 0.1205\n",
      "Epoch [5/30], Step [38/139], Loss: 0.1255\n",
      "Epoch [5/30], Step [39/139], Loss: 0.1116\n",
      "Epoch [5/30], Step [40/139], Loss: 0.1454\n",
      "Epoch [5/30], Step [41/139], Loss: 0.1642\n",
      "Epoch [5/30], Step [42/139], Loss: 0.1298\n",
      "Epoch [5/30], Step [43/139], Loss: 0.1219\n",
      "Epoch [5/30], Step [44/139], Loss: 0.1071\n",
      "Epoch [5/30], Step [45/139], Loss: 0.1529\n",
      "Epoch [5/30], Step [46/139], Loss: 0.1105\n",
      "Epoch [5/30], Step [47/139], Loss: 0.0887\n",
      "Epoch [5/30], Step [48/139], Loss: 0.1271\n",
      "Epoch [5/30], Step [49/139], Loss: 0.1104\n",
      "Epoch [5/30], Step [50/139], Loss: 0.1293\n",
      "Epoch [5/30], Step [51/139], Loss: 0.1136\n",
      "Epoch [5/30], Step [52/139], Loss: 0.1227\n",
      "Epoch [5/30], Step [53/139], Loss: 0.1130\n",
      "Epoch [5/30], Step [54/139], Loss: 0.0806\n",
      "Epoch [5/30], Step [55/139], Loss: 0.1582\n",
      "Epoch [5/30], Step [56/139], Loss: 0.1226\n",
      "Epoch [5/30], Step [57/139], Loss: 0.1032\n",
      "Epoch [5/30], Step [58/139], Loss: 0.1091\n",
      "Epoch [5/30], Step [59/139], Loss: 0.1183\n",
      "Epoch [5/30], Step [60/139], Loss: 0.0981\n",
      "Epoch [5/30], Step [61/139], Loss: 0.1201\n",
      "Epoch [5/30], Step [62/139], Loss: 0.0887\n",
      "Epoch [5/30], Step [63/139], Loss: 0.1206\n",
      "Epoch [5/30], Step [64/139], Loss: 0.1322\n",
      "Epoch [5/30], Step [65/139], Loss: 0.1403\n",
      "Epoch [5/30], Step [66/139], Loss: 0.0850\n",
      "Epoch [5/30], Step [67/139], Loss: 0.1072\n",
      "Epoch [5/30], Step [68/139], Loss: 0.1273\n",
      "Epoch [5/30], Step [69/139], Loss: 0.1154\n",
      "Epoch [5/30], Step [70/139], Loss: 0.1267\n",
      "Epoch [5/30], Step [71/139], Loss: 0.1219\n",
      "Epoch [5/30], Step [72/139], Loss: 0.1577\n",
      "Epoch [5/30], Step [73/139], Loss: 0.1126\n",
      "Epoch [5/30], Step [74/139], Loss: 0.0817\n",
      "Epoch [5/30], Step [75/139], Loss: 0.1059\n",
      "Epoch [5/30], Step [76/139], Loss: 0.1230\n",
      "Epoch [5/30], Step [77/139], Loss: 0.1058\n",
      "Epoch [5/30], Step [78/139], Loss: 0.1119\n",
      "Epoch [5/30], Step [79/139], Loss: 0.0678\n",
      "Epoch [5/30], Step [80/139], Loss: 0.1117\n",
      "Epoch [5/30], Step [81/139], Loss: 0.1282\n",
      "Epoch [5/30], Step [82/139], Loss: 0.1019\n",
      "Epoch [5/30], Step [83/139], Loss: 0.1291\n",
      "Epoch [5/30], Step [84/139], Loss: 0.1214\n",
      "Epoch [5/30], Step [85/139], Loss: 0.1159\n",
      "Epoch [5/30], Step [86/139], Loss: 0.1238\n",
      "Epoch [5/30], Step [87/139], Loss: 0.1032\n",
      "Epoch [5/30], Step [88/139], Loss: 0.1028\n",
      "Epoch [5/30], Step [89/139], Loss: 0.1385\n",
      "Epoch [5/30], Step [90/139], Loss: 0.1350\n",
      "Epoch [5/30], Step [91/139], Loss: 0.1021\n",
      "Epoch [5/30], Step [92/139], Loss: 0.0843\n",
      "Epoch [5/30], Step [93/139], Loss: 0.1075\n",
      "Epoch [5/30], Step [94/139], Loss: 0.1150\n",
      "Epoch [5/30], Step [95/139], Loss: 0.1036\n",
      "Epoch [5/30], Step [96/139], Loss: 0.0992\n",
      "Epoch [5/30], Step [97/139], Loss: 0.1155\n",
      "Epoch [5/30], Step [98/139], Loss: 0.1122\n",
      "Epoch [5/30], Step [99/139], Loss: 0.1087\n",
      "Epoch [5/30], Step [100/139], Loss: 0.0913\n",
      "Epoch [5/30], Step [101/139], Loss: 0.1163\n",
      "Epoch [5/30], Step [102/139], Loss: 0.1267\n",
      "Epoch [5/30], Step [103/139], Loss: 0.1025\n",
      "Epoch [5/30], Step [104/139], Loss: 0.1121\n",
      "Epoch [5/30], Step [105/139], Loss: 0.0969\n",
      "Epoch [5/30], Step [106/139], Loss: 0.1130\n",
      "Epoch [5/30], Step [107/139], Loss: 0.0903\n",
      "Epoch [5/30], Step [108/139], Loss: 0.0867\n",
      "Epoch [5/30], Step [109/139], Loss: 0.1221\n",
      "Epoch [5/30], Step [110/139], Loss: 0.0860\n",
      "Epoch [5/30], Step [111/139], Loss: 0.1294\n",
      "Epoch [5/30], Step [112/139], Loss: 0.1074\n",
      "Epoch [5/30], Step [113/139], Loss: 0.1026\n",
      "Epoch [5/30], Step [114/139], Loss: 0.0894\n",
      "Epoch [5/30], Step [115/139], Loss: 0.0910\n",
      "Epoch [5/30], Step [116/139], Loss: 0.1215\n",
      "Epoch [5/30], Step [117/139], Loss: 0.1050\n",
      "Epoch [5/30], Step [118/139], Loss: 0.0831\n",
      "Epoch [5/30], Step [119/139], Loss: 0.1080\n",
      "Epoch [5/30], Step [120/139], Loss: 0.0947\n",
      "Epoch [5/30], Step [121/139], Loss: 0.1134\n",
      "Epoch [5/30], Step [122/139], Loss: 0.1564\n",
      "Epoch [5/30], Step [123/139], Loss: 0.0977\n",
      "Epoch [5/30], Step [124/139], Loss: 0.1088\n",
      "Epoch [5/30], Step [125/139], Loss: 0.0831\n",
      "Epoch [5/30], Step [126/139], Loss: 0.1643\n",
      "Epoch [5/30], Step [127/139], Loss: 0.1382\n",
      "Epoch [5/30], Step [128/139], Loss: 0.0987\n",
      "Epoch [5/30], Step [129/139], Loss: 0.1323\n",
      "Epoch [5/30], Step [130/139], Loss: 0.1127\n",
      "Epoch [5/30], Step [131/139], Loss: 0.0868\n",
      "Epoch [5/30], Step [132/139], Loss: 0.0913\n",
      "Epoch [5/30], Step [133/139], Loss: 0.1282\n",
      "Epoch [5/30], Step [134/139], Loss: 0.1086\n",
      "Epoch [5/30], Step [135/139], Loss: 0.1347\n",
      "Epoch [5/30], Step [136/139], Loss: 0.0946\n",
      "Epoch [5/30], Step [137/139], Loss: 0.0826\n",
      "Epoch [5/30], Step [138/139], Loss: 0.1266\n",
      "Epoch [5/30], Step [139/139], Loss: 0.0937\n",
      "Start validation #5\n",
      "Validation #5  Average Loss: 0.1048\n",
      "Best performance at epoch: 5\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [6/30], Step [1/139], Loss: 0.1088\n",
      "Epoch [6/30], Step [2/139], Loss: 0.0943\n",
      "Epoch [6/30], Step [3/139], Loss: 0.1003\n",
      "Epoch [6/30], Step [4/139], Loss: 0.1025\n",
      "Epoch [6/30], Step [5/139], Loss: 0.0811\n",
      "Epoch [6/30], Step [6/139], Loss: 0.1130\n",
      "Epoch [6/30], Step [7/139], Loss: 0.1087\n",
      "Epoch [6/30], Step [8/139], Loss: 0.0999\n",
      "Epoch [6/30], Step [9/139], Loss: 0.1000\n",
      "Epoch [6/30], Step [10/139], Loss: 0.1551\n",
      "Epoch [6/30], Step [11/139], Loss: 0.1060\n",
      "Epoch [6/30], Step [12/139], Loss: 0.0845\n",
      "Epoch [6/30], Step [13/139], Loss: 0.1013\n",
      "Epoch [6/30], Step [14/139], Loss: 0.1206\n",
      "Epoch [6/30], Step [15/139], Loss: 0.0840\n",
      "Epoch [6/30], Step [16/139], Loss: 0.1539\n",
      "Epoch [6/30], Step [17/139], Loss: 0.1306\n",
      "Epoch [6/30], Step [18/139], Loss: 0.1292\n",
      "Epoch [6/30], Step [19/139], Loss: 0.0922\n",
      "Epoch [6/30], Step [20/139], Loss: 0.1020\n",
      "Epoch [6/30], Step [21/139], Loss: 0.1328\n",
      "Epoch [6/30], Step [22/139], Loss: 0.1073\n",
      "Epoch [6/30], Step [23/139], Loss: 0.1034\n",
      "Epoch [6/30], Step [24/139], Loss: 0.0932\n",
      "Epoch [6/30], Step [25/139], Loss: 0.1186\n",
      "Epoch [6/30], Step [26/139], Loss: 0.1044\n",
      "Epoch [6/30], Step [27/139], Loss: 0.1093\n",
      "Epoch [6/30], Step [28/139], Loss: 0.1019\n",
      "Epoch [6/30], Step [29/139], Loss: 0.0869\n",
      "Epoch [6/30], Step [30/139], Loss: 0.1200\n",
      "Epoch [6/30], Step [31/139], Loss: 0.1397\n",
      "Epoch [6/30], Step [32/139], Loss: 0.0990\n",
      "Epoch [6/30], Step [33/139], Loss: 0.0824\n",
      "Epoch [6/30], Step [34/139], Loss: 0.0952\n",
      "Epoch [6/30], Step [35/139], Loss: 0.1107\n",
      "Epoch [6/30], Step [36/139], Loss: 0.1223\n",
      "Epoch [6/30], Step [37/139], Loss: 0.0921\n",
      "Epoch [6/30], Step [38/139], Loss: 0.1036\n",
      "Epoch [6/30], Step [39/139], Loss: 0.0918\n",
      "Epoch [6/30], Step [40/139], Loss: 0.1125\n",
      "Epoch [6/30], Step [41/139], Loss: 0.0910\n",
      "Epoch [6/30], Step [42/139], Loss: 0.1076\n",
      "Epoch [6/30], Step [43/139], Loss: 0.1149\n",
      "Epoch [6/30], Step [44/139], Loss: 0.0974\n",
      "Epoch [6/30], Step [45/139], Loss: 0.1231\n",
      "Epoch [6/30], Step [46/139], Loss: 0.0804\n",
      "Epoch [6/30], Step [47/139], Loss: 0.0968\n",
      "Epoch [6/30], Step [48/139], Loss: 0.1356\n",
      "Epoch [6/30], Step [49/139], Loss: 0.0756\n",
      "Epoch [6/30], Step [50/139], Loss: 0.0857\n",
      "Epoch [6/30], Step [51/139], Loss: 0.1012\n",
      "Epoch [6/30], Step [52/139], Loss: 0.1079\n",
      "Epoch [6/30], Step [53/139], Loss: 0.1036\n",
      "Epoch [6/30], Step [54/139], Loss: 0.1156\n",
      "Epoch [6/30], Step [55/139], Loss: 0.0828\n",
      "Epoch [6/30], Step [56/139], Loss: 0.0927\n",
      "Epoch [6/30], Step [57/139], Loss: 0.1334\n",
      "Epoch [6/30], Step [58/139], Loss: 0.1108\n",
      "Epoch [6/30], Step [59/139], Loss: 0.0929\n",
      "Epoch [6/30], Step [60/139], Loss: 0.1219\n",
      "Epoch [6/30], Step [61/139], Loss: 0.1296\n",
      "Epoch [6/30], Step [62/139], Loss: 0.0811\n",
      "Epoch [6/30], Step [63/139], Loss: 0.1032\n",
      "Epoch [6/30], Step [64/139], Loss: 0.1010\n",
      "Epoch [6/30], Step [65/139], Loss: 0.1200\n",
      "Epoch [6/30], Step [66/139], Loss: 0.0818\n",
      "Epoch [6/30], Step [67/139], Loss: 0.0909\n",
      "Epoch [6/30], Step [68/139], Loss: 0.1360\n",
      "Epoch [6/30], Step [69/139], Loss: 0.1232\n",
      "Epoch [6/30], Step [70/139], Loss: 0.0854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/30], Step [71/139], Loss: 0.1049\n",
      "Epoch [6/30], Step [72/139], Loss: 0.0756\n",
      "Epoch [6/30], Step [73/139], Loss: 0.1293\n",
      "Epoch [6/30], Step [74/139], Loss: 0.1334\n",
      "Epoch [6/30], Step [75/139], Loss: 0.0904\n",
      "Epoch [6/30], Step [76/139], Loss: 0.0958\n",
      "Epoch [6/30], Step [77/139], Loss: 0.1010\n",
      "Epoch [6/30], Step [78/139], Loss: 0.0965\n",
      "Epoch [6/30], Step [79/139], Loss: 0.1233\n",
      "Epoch [6/30], Step [80/139], Loss: 0.1120\n",
      "Epoch [6/30], Step [81/139], Loss: 0.0921\n",
      "Epoch [6/30], Step [82/139], Loss: 0.0967\n",
      "Epoch [6/30], Step [83/139], Loss: 0.1207\n",
      "Epoch [6/30], Step [84/139], Loss: 0.0874\n",
      "Epoch [6/30], Step [85/139], Loss: 0.1132\n",
      "Epoch [6/30], Step [86/139], Loss: 0.1049\n",
      "Epoch [6/30], Step [87/139], Loss: 0.1396\n",
      "Epoch [6/30], Step [88/139], Loss: 0.1269\n",
      "Epoch [6/30], Step [89/139], Loss: 0.0965\n",
      "Epoch [6/30], Step [90/139], Loss: 0.1224\n",
      "Epoch [6/30], Step [91/139], Loss: 0.1151\n",
      "Epoch [6/30], Step [92/139], Loss: 0.1219\n",
      "Epoch [6/30], Step [93/139], Loss: 0.0942\n",
      "Epoch [6/30], Step [94/139], Loss: 0.0947\n",
      "Epoch [6/30], Step [95/139], Loss: 0.0994\n",
      "Epoch [6/30], Step [96/139], Loss: 0.1096\n",
      "Epoch [6/30], Step [97/139], Loss: 0.1099\n",
      "Epoch [6/30], Step [98/139], Loss: 0.0891\n",
      "Epoch [6/30], Step [99/139], Loss: 0.1271\n",
      "Epoch [6/30], Step [100/139], Loss: 0.0861\n",
      "Epoch [6/30], Step [101/139], Loss: 0.0983\n",
      "Epoch [6/30], Step [102/139], Loss: 0.0897\n",
      "Epoch [6/30], Step [103/139], Loss: 0.1288\n",
      "Epoch [6/30], Step [104/139], Loss: 0.0970\n",
      "Epoch [6/30], Step [105/139], Loss: 0.1126\n",
      "Epoch [6/30], Step [106/139], Loss: 0.1054\n",
      "Epoch [6/30], Step [107/139], Loss: 0.1225\n",
      "Epoch [6/30], Step [108/139], Loss: 0.1032\n",
      "Epoch [6/30], Step [109/139], Loss: 0.1281\n",
      "Epoch [6/30], Step [110/139], Loss: 0.1235\n",
      "Epoch [6/30], Step [111/139], Loss: 0.0881\n",
      "Epoch [6/30], Step [112/139], Loss: 0.0843\n",
      "Epoch [6/30], Step [113/139], Loss: 0.1032\n",
      "Epoch [6/30], Step [114/139], Loss: 0.1189\n",
      "Epoch [6/30], Step [115/139], Loss: 0.1089\n",
      "Epoch [6/30], Step [116/139], Loss: 0.0879\n",
      "Epoch [6/30], Step [117/139], Loss: 0.1096\n",
      "Epoch [6/30], Step [118/139], Loss: 0.1050\n",
      "Epoch [6/30], Step [119/139], Loss: 0.0915\n",
      "Epoch [6/30], Step [120/139], Loss: 0.0912\n",
      "Epoch [6/30], Step [121/139], Loss: 0.1179\n",
      "Epoch [6/30], Step [122/139], Loss: 0.1233\n",
      "Epoch [6/30], Step [123/139], Loss: 0.0661\n",
      "Epoch [6/30], Step [124/139], Loss: 0.0965\n",
      "Epoch [6/30], Step [125/139], Loss: 0.0941\n",
      "Epoch [6/30], Step [126/139], Loss: 0.0953\n",
      "Epoch [6/30], Step [127/139], Loss: 0.1291\n",
      "Epoch [6/30], Step [128/139], Loss: 0.0911\n",
      "Epoch [6/30], Step [129/139], Loss: 0.1055\n",
      "Epoch [6/30], Step [130/139], Loss: 0.1251\n",
      "Epoch [6/30], Step [131/139], Loss: 0.0931\n",
      "Epoch [6/30], Step [132/139], Loss: 0.0906\n",
      "Epoch [6/30], Step [133/139], Loss: 0.1367\n",
      "Epoch [6/30], Step [134/139], Loss: 0.1085\n",
      "Epoch [6/30], Step [135/139], Loss: 0.1145\n",
      "Epoch [6/30], Step [136/139], Loss: 0.1074\n",
      "Epoch [6/30], Step [137/139], Loss: 0.1075\n",
      "Epoch [6/30], Step [138/139], Loss: 0.1058\n",
      "Epoch [6/30], Step [139/139], Loss: 0.1216\n",
      "Start validation #6\n",
      "Validation #6  Average Loss: 0.1020\n",
      "Best performance at epoch: 6\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [7/30], Step [1/139], Loss: 0.1163\n",
      "Epoch [7/30], Step [2/139], Loss: 0.0840\n",
      "Epoch [7/30], Step [3/139], Loss: 0.0878\n",
      "Epoch [7/30], Step [4/139], Loss: 0.0990\n",
      "Epoch [7/30], Step [5/139], Loss: 0.0938\n",
      "Epoch [7/30], Step [6/139], Loss: 0.1155\n",
      "Epoch [7/30], Step [7/139], Loss: 0.1208\n",
      "Epoch [7/30], Step [8/139], Loss: 0.1143\n",
      "Epoch [7/30], Step [9/139], Loss: 0.1033\n",
      "Epoch [7/30], Step [10/139], Loss: 0.1085\n",
      "Epoch [7/30], Step [11/139], Loss: 0.0957\n",
      "Epoch [7/30], Step [12/139], Loss: 0.1043\n",
      "Epoch [7/30], Step [13/139], Loss: 0.0972\n",
      "Epoch [7/30], Step [14/139], Loss: 0.1163\n",
      "Epoch [7/30], Step [15/139], Loss: 0.1029\n",
      "Epoch [7/30], Step [16/139], Loss: 0.0883\n",
      "Epoch [7/30], Step [17/139], Loss: 0.1137\n",
      "Epoch [7/30], Step [18/139], Loss: 0.0887\n",
      "Epoch [7/30], Step [19/139], Loss: 0.1078\n",
      "Epoch [7/30], Step [20/139], Loss: 0.1102\n",
      "Epoch [7/30], Step [21/139], Loss: 0.1006\n",
      "Epoch [7/30], Step [22/139], Loss: 0.0976\n",
      "Epoch [7/30], Step [23/139], Loss: 0.1075\n",
      "Epoch [7/30], Step [24/139], Loss: 0.1146\n",
      "Epoch [7/30], Step [25/139], Loss: 0.0976\n",
      "Epoch [7/30], Step [26/139], Loss: 0.1290\n",
      "Epoch [7/30], Step [27/139], Loss: 0.1145\n",
      "Epoch [7/30], Step [28/139], Loss: 0.0999\n",
      "Epoch [7/30], Step [29/139], Loss: 0.0917\n",
      "Epoch [7/30], Step [30/139], Loss: 0.1123\n",
      "Epoch [7/30], Step [31/139], Loss: 0.0903\n",
      "Epoch [7/30], Step [32/139], Loss: 0.0917\n",
      "Epoch [7/30], Step [33/139], Loss: 0.0819\n",
      "Epoch [7/30], Step [34/139], Loss: 0.0930\n",
      "Epoch [7/30], Step [35/139], Loss: 0.0788\n",
      "Epoch [7/30], Step [36/139], Loss: 0.1049\n",
      "Epoch [7/30], Step [37/139], Loss: 0.1342\n",
      "Epoch [7/30], Step [38/139], Loss: 0.1115\n",
      "Epoch [7/30], Step [39/139], Loss: 0.1026\n",
      "Epoch [7/30], Step [40/139], Loss: 0.0917\n",
      "Epoch [7/30], Step [41/139], Loss: 0.1018\n",
      "Epoch [7/30], Step [42/139], Loss: 0.1284\n",
      "Epoch [7/30], Step [43/139], Loss: 0.0917\n",
      "Epoch [7/30], Step [44/139], Loss: 0.1381\n",
      "Epoch [7/30], Step [45/139], Loss: 0.0791\n",
      "Epoch [7/30], Step [46/139], Loss: 0.1042\n",
      "Epoch [7/30], Step [47/139], Loss: 0.0962\n",
      "Epoch [7/30], Step [48/139], Loss: 0.0916\n",
      "Epoch [7/30], Step [49/139], Loss: 0.1092\n",
      "Epoch [7/30], Step [50/139], Loss: 0.0827\n",
      "Epoch [7/30], Step [51/139], Loss: 0.1011\n",
      "Epoch [7/30], Step [52/139], Loss: 0.1185\n",
      "Epoch [7/30], Step [53/139], Loss: 0.1005\n",
      "Epoch [7/30], Step [54/139], Loss: 0.0804\n",
      "Epoch [7/30], Step [55/139], Loss: 0.1211\n",
      "Epoch [7/30], Step [56/139], Loss: 0.0830\n",
      "Epoch [7/30], Step [57/139], Loss: 0.1162\n",
      "Epoch [7/30], Step [58/139], Loss: 0.1075\n",
      "Epoch [7/30], Step [59/139], Loss: 0.1124\n",
      "Epoch [7/30], Step [60/139], Loss: 0.0989\n",
      "Epoch [7/30], Step [61/139], Loss: 0.1007\n",
      "Epoch [7/30], Step [62/139], Loss: 0.0755\n",
      "Epoch [7/30], Step [63/139], Loss: 0.0845\n",
      "Epoch [7/30], Step [64/139], Loss: 0.1204\n",
      "Epoch [7/30], Step [65/139], Loss: 0.0873\n",
      "Epoch [7/30], Step [66/139], Loss: 0.1006\n",
      "Epoch [7/30], Step [67/139], Loss: 0.1330\n",
      "Epoch [7/30], Step [68/139], Loss: 0.0864\n",
      "Epoch [7/30], Step [69/139], Loss: 0.1059\n",
      "Epoch [7/30], Step [70/139], Loss: 0.0847\n",
      "Epoch [7/30], Step [71/139], Loss: 0.0965\n",
      "Epoch [7/30], Step [72/139], Loss: 0.1110\n",
      "Epoch [7/30], Step [73/139], Loss: 0.1243\n",
      "Epoch [7/30], Step [74/139], Loss: 0.1172\n",
      "Epoch [7/30], Step [75/139], Loss: 0.0782\n",
      "Epoch [7/30], Step [76/139], Loss: 0.1171\n",
      "Epoch [7/30], Step [77/139], Loss: 0.0872\n",
      "Epoch [7/30], Step [78/139], Loss: 0.0642\n",
      "Epoch [7/30], Step [79/139], Loss: 0.1109\n",
      "Epoch [7/30], Step [80/139], Loss: 0.1312\n",
      "Epoch [7/30], Step [81/139], Loss: 0.1140\n",
      "Epoch [7/30], Step [82/139], Loss: 0.0808\n",
      "Epoch [7/30], Step [83/139], Loss: 0.0855\n",
      "Epoch [7/30], Step [84/139], Loss: 0.0578\n",
      "Epoch [7/30], Step [85/139], Loss: 0.0631\n",
      "Epoch [7/30], Step [86/139], Loss: 0.1191\n",
      "Epoch [7/30], Step [87/139], Loss: 0.1265\n",
      "Epoch [7/30], Step [88/139], Loss: 0.1315\n",
      "Epoch [7/30], Step [89/139], Loss: 0.0841\n",
      "Epoch [7/30], Step [90/139], Loss: 0.0897\n",
      "Epoch [7/30], Step [91/139], Loss: 0.1850\n",
      "Epoch [7/30], Step [92/139], Loss: 0.1046\n",
      "Epoch [7/30], Step [93/139], Loss: 0.0748\n",
      "Epoch [7/30], Step [94/139], Loss: 0.0898\n",
      "Epoch [7/30], Step [95/139], Loss: 0.1061\n",
      "Epoch [7/30], Step [96/139], Loss: 0.1329\n",
      "Epoch [7/30], Step [97/139], Loss: 0.1075\n",
      "Epoch [7/30], Step [98/139], Loss: 0.1326\n",
      "Epoch [7/30], Step [99/139], Loss: 0.1067\n",
      "Epoch [7/30], Step [100/139], Loss: 0.0996\n",
      "Epoch [7/30], Step [101/139], Loss: 0.1012\n",
      "Epoch [7/30], Step [102/139], Loss: 0.1271\n",
      "Epoch [7/30], Step [103/139], Loss: 0.0954\n",
      "Epoch [7/30], Step [104/139], Loss: 0.1218\n",
      "Epoch [7/30], Step [105/139], Loss: 0.1029\n",
      "Epoch [7/30], Step [106/139], Loss: 0.1023\n",
      "Epoch [7/30], Step [107/139], Loss: 0.0975\n",
      "Epoch [7/30], Step [108/139], Loss: 0.0989\n",
      "Epoch [7/30], Step [109/139], Loss: 0.1207\n",
      "Epoch [7/30], Step [110/139], Loss: 0.0926\n",
      "Epoch [7/30], Step [111/139], Loss: 0.0814\n",
      "Epoch [7/30], Step [112/139], Loss: 0.1079\n",
      "Epoch [7/30], Step [113/139], Loss: 0.1042\n",
      "Epoch [7/30], Step [114/139], Loss: 0.0930\n",
      "Epoch [7/30], Step [115/139], Loss: 0.0841\n",
      "Epoch [7/30], Step [116/139], Loss: 0.0933\n",
      "Epoch [7/30], Step [117/139], Loss: 0.1143\n",
      "Epoch [7/30], Step [118/139], Loss: 0.0865\n",
      "Epoch [7/30], Step [119/139], Loss: 0.0876\n",
      "Epoch [7/30], Step [120/139], Loss: 0.0825\n",
      "Epoch [7/30], Step [121/139], Loss: 0.0958\n",
      "Epoch [7/30], Step [122/139], Loss: 0.1157\n",
      "Epoch [7/30], Step [123/139], Loss: 0.0949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30], Step [124/139], Loss: 0.1155\n",
      "Epoch [7/30], Step [125/139], Loss: 0.0846\n",
      "Epoch [7/30], Step [126/139], Loss: 0.1000\n",
      "Epoch [7/30], Step [127/139], Loss: 0.0798\n",
      "Epoch [7/30], Step [128/139], Loss: 0.0834\n",
      "Epoch [7/30], Step [129/139], Loss: 0.0822\n",
      "Epoch [7/30], Step [130/139], Loss: 0.0772\n",
      "Epoch [7/30], Step [131/139], Loss: 0.1137\n",
      "Epoch [7/30], Step [132/139], Loss: 0.1135\n",
      "Epoch [7/30], Step [133/139], Loss: 0.1038\n",
      "Epoch [7/30], Step [134/139], Loss: 0.1040\n",
      "Epoch [7/30], Step [135/139], Loss: 0.0878\n",
      "Epoch [7/30], Step [136/139], Loss: 0.1044\n",
      "Epoch [7/30], Step [137/139], Loss: 0.0778\n",
      "Epoch [7/30], Step [138/139], Loss: 0.0853\n",
      "Epoch [7/30], Step [139/139], Loss: 0.1083\n",
      "Start validation #7\n",
      "Validation #7  Average Loss: 0.0995\n",
      "Best performance at epoch: 7\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [8/30], Step [1/139], Loss: 0.0842\n",
      "Epoch [8/30], Step [2/139], Loss: 0.0860\n",
      "Epoch [8/30], Step [3/139], Loss: 0.0893\n",
      "Epoch [8/30], Step [4/139], Loss: 0.0892\n",
      "Epoch [8/30], Step [5/139], Loss: 0.1065\n",
      "Epoch [8/30], Step [6/139], Loss: 0.0892\n",
      "Epoch [8/30], Step [7/139], Loss: 0.1178\n",
      "Epoch [8/30], Step [8/139], Loss: 0.0885\n",
      "Epoch [8/30], Step [9/139], Loss: 0.0861\n",
      "Epoch [8/30], Step [10/139], Loss: 0.1025\n",
      "Epoch [8/30], Step [11/139], Loss: 0.0911\n",
      "Epoch [8/30], Step [12/139], Loss: 0.1041\n",
      "Epoch [8/30], Step [13/139], Loss: 0.1134\n",
      "Epoch [8/30], Step [14/139], Loss: 0.1030\n",
      "Epoch [8/30], Step [15/139], Loss: 0.0841\n",
      "Epoch [8/30], Step [16/139], Loss: 0.0841\n",
      "Epoch [8/30], Step [17/139], Loss: 0.1027\n",
      "Epoch [8/30], Step [18/139], Loss: 0.0847\n",
      "Epoch [8/30], Step [19/139], Loss: 0.0864\n",
      "Epoch [8/30], Step [20/139], Loss: 0.0867\n",
      "Epoch [8/30], Step [21/139], Loss: 0.0822\n",
      "Epoch [8/30], Step [22/139], Loss: 0.1033\n",
      "Epoch [8/30], Step [23/139], Loss: 0.0843\n",
      "Epoch [8/30], Step [24/139], Loss: 0.0795\n",
      "Epoch [8/30], Step [25/139], Loss: 0.0984\n",
      "Epoch [8/30], Step [26/139], Loss: 0.1084\n",
      "Epoch [8/30], Step [27/139], Loss: 0.0644\n",
      "Epoch [8/30], Step [28/139], Loss: 0.0951\n",
      "Epoch [8/30], Step [29/139], Loss: 0.0976\n",
      "Epoch [8/30], Step [30/139], Loss: 0.1106\n",
      "Epoch [8/30], Step [31/139], Loss: 0.0993\n",
      "Epoch [8/30], Step [32/139], Loss: 0.1037\n",
      "Epoch [8/30], Step [33/139], Loss: 0.1060\n",
      "Epoch [8/30], Step [34/139], Loss: 0.1044\n",
      "Epoch [8/30], Step [35/139], Loss: 0.0916\n",
      "Epoch [8/30], Step [36/139], Loss: 0.0926\n",
      "Epoch [8/30], Step [37/139], Loss: 0.0904\n",
      "Epoch [8/30], Step [38/139], Loss: 0.0959\n",
      "Epoch [8/30], Step [39/139], Loss: 0.0920\n",
      "Epoch [8/30], Step [40/139], Loss: 0.0826\n",
      "Epoch [8/30], Step [41/139], Loss: 0.0832\n",
      "Epoch [8/30], Step [42/139], Loss: 0.0936\n",
      "Epoch [8/30], Step [43/139], Loss: 0.1044\n",
      "Epoch [8/30], Step [44/139], Loss: 0.0962\n",
      "Epoch [8/30], Step [45/139], Loss: 0.1080\n",
      "Epoch [8/30], Step [46/139], Loss: 0.0981\n",
      "Epoch [8/30], Step [47/139], Loss: 0.1099\n",
      "Epoch [8/30], Step [48/139], Loss: 0.1184\n",
      "Epoch [8/30], Step [49/139], Loss: 0.1005\n",
      "Epoch [8/30], Step [50/139], Loss: 0.0897\n",
      "Epoch [8/30], Step [51/139], Loss: 0.1279\n",
      "Epoch [8/30], Step [52/139], Loss: 0.1064\n",
      "Epoch [8/30], Step [53/139], Loss: 0.1002\n",
      "Epoch [8/30], Step [54/139], Loss: 0.1117\n",
      "Epoch [8/30], Step [55/139], Loss: 0.1395\n",
      "Epoch [8/30], Step [56/139], Loss: 0.0908\n",
      "Epoch [8/30], Step [57/139], Loss: 0.1090\n",
      "Epoch [8/30], Step [58/139], Loss: 0.0815\n",
      "Epoch [8/30], Step [59/139], Loss: 0.1226\n",
      "Epoch [8/30], Step [60/139], Loss: 0.1280\n",
      "Epoch [8/30], Step [61/139], Loss: 0.0969\n",
      "Epoch [8/30], Step [62/139], Loss: 0.0862\n",
      "Epoch [8/30], Step [63/139], Loss: 0.0931\n",
      "Epoch [8/30], Step [64/139], Loss: 0.0914\n",
      "Epoch [8/30], Step [65/139], Loss: 0.1170\n",
      "Epoch [8/30], Step [66/139], Loss: 0.1061\n",
      "Epoch [8/30], Step [67/139], Loss: 0.0959\n",
      "Epoch [8/30], Step [68/139], Loss: 0.1125\n",
      "Epoch [8/30], Step [69/139], Loss: 0.0959\n",
      "Epoch [8/30], Step [70/139], Loss: 0.0794\n",
      "Epoch [8/30], Step [71/139], Loss: 0.1022\n",
      "Epoch [8/30], Step [72/139], Loss: 0.0932\n",
      "Epoch [8/30], Step [73/139], Loss: 0.0852\n",
      "Epoch [8/30], Step [74/139], Loss: 0.0889\n",
      "Epoch [8/30], Step [75/139], Loss: 0.0943\n",
      "Epoch [8/30], Step [76/139], Loss: 0.0946\n",
      "Epoch [8/30], Step [77/139], Loss: 0.0910\n",
      "Epoch [8/30], Step [78/139], Loss: 0.0927\n",
      "Epoch [8/30], Step [79/139], Loss: 0.0987\n",
      "Epoch [8/30], Step [80/139], Loss: 0.1118\n",
      "Epoch [8/30], Step [81/139], Loss: 0.1254\n",
      "Epoch [8/30], Step [82/139], Loss: 0.1144\n",
      "Epoch [8/30], Step [83/139], Loss: 0.0715\n",
      "Epoch [8/30], Step [84/139], Loss: 0.0860\n",
      "Epoch [8/30], Step [85/139], Loss: 0.0892\n",
      "Epoch [8/30], Step [86/139], Loss: 0.1030\n",
      "Epoch [8/30], Step [87/139], Loss: 0.0863\n",
      "Epoch [8/30], Step [88/139], Loss: 0.1114\n",
      "Epoch [8/30], Step [89/139], Loss: 0.1173\n",
      "Epoch [8/30], Step [90/139], Loss: 0.1236\n",
      "Epoch [8/30], Step [91/139], Loss: 0.0992\n",
      "Epoch [8/30], Step [92/139], Loss: 0.1015\n",
      "Epoch [8/30], Step [93/139], Loss: 0.1105\n",
      "Epoch [8/30], Step [94/139], Loss: 0.0936\n",
      "Epoch [8/30], Step [95/139], Loss: 0.1074\n",
      "Epoch [8/30], Step [96/139], Loss: 0.0953\n",
      "Epoch [8/30], Step [97/139], Loss: 0.1223\n",
      "Epoch [8/30], Step [98/139], Loss: 0.0913\n",
      "Epoch [8/30], Step [99/139], Loss: 0.0958\n",
      "Epoch [8/30], Step [100/139], Loss: 0.1036\n",
      "Epoch [8/30], Step [101/139], Loss: 0.0862\n",
      "Epoch [8/30], Step [102/139], Loss: 0.0858\n",
      "Epoch [8/30], Step [103/139], Loss: 0.1007\n",
      "Epoch [8/30], Step [104/139], Loss: 0.0836\n",
      "Epoch [8/30], Step [105/139], Loss: 0.1101\n",
      "Epoch [8/30], Step [106/139], Loss: 0.0915\n",
      "Epoch [8/30], Step [107/139], Loss: 0.0945\n",
      "Epoch [8/30], Step [108/139], Loss: 0.1030\n",
      "Epoch [8/30], Step [109/139], Loss: 0.0903\n",
      "Epoch [8/30], Step [110/139], Loss: 0.0908\n",
      "Epoch [8/30], Step [111/139], Loss: 0.0901\n",
      "Epoch [8/30], Step [112/139], Loss: 0.0828\n",
      "Epoch [8/30], Step [113/139], Loss: 0.1248\n",
      "Epoch [8/30], Step [114/139], Loss: 0.0931\n",
      "Epoch [8/30], Step [115/139], Loss: 0.0997\n",
      "Epoch [8/30], Step [116/139], Loss: 0.0935\n",
      "Epoch [8/30], Step [117/139], Loss: 0.0697\n",
      "Epoch [8/30], Step [118/139], Loss: 0.1069\n",
      "Epoch [8/30], Step [119/139], Loss: 0.0950\n",
      "Epoch [8/30], Step [120/139], Loss: 0.1491\n",
      "Epoch [8/30], Step [121/139], Loss: 0.1030\n",
      "Epoch [8/30], Step [122/139], Loss: 0.0833\n",
      "Epoch [8/30], Step [123/139], Loss: 0.1120\n",
      "Epoch [8/30], Step [124/139], Loss: 0.0971\n",
      "Epoch [8/30], Step [125/139], Loss: 0.1181\n",
      "Epoch [8/30], Step [126/139], Loss: 0.0867\n",
      "Epoch [8/30], Step [127/139], Loss: 0.0774\n",
      "Epoch [8/30], Step [128/139], Loss: 0.1162\n",
      "Epoch [8/30], Step [129/139], Loss: 0.0824\n",
      "Epoch [8/30], Step [130/139], Loss: 0.0945\n",
      "Epoch [8/30], Step [131/139], Loss: 0.1012\n",
      "Epoch [8/30], Step [132/139], Loss: 0.0732\n",
      "Epoch [8/30], Step [133/139], Loss: 0.1059\n",
      "Epoch [8/30], Step [134/139], Loss: 0.0810\n",
      "Epoch [8/30], Step [135/139], Loss: 0.1062\n",
      "Epoch [8/30], Step [136/139], Loss: 0.0825\n",
      "Epoch [8/30], Step [137/139], Loss: 0.1302\n",
      "Epoch [8/30], Step [138/139], Loss: 0.0865\n",
      "Epoch [8/30], Step [139/139], Loss: 0.0941\n",
      "Start validation #8\n",
      "Validation #8  Average Loss: 0.0965\n",
      "Best performance at epoch: 8\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [9/30], Step [1/139], Loss: 0.0986\n",
      "Epoch [9/30], Step [2/139], Loss: 0.1120\n",
      "Epoch [9/30], Step [3/139], Loss: 0.0922\n",
      "Epoch [9/30], Step [4/139], Loss: 0.0963\n",
      "Epoch [9/30], Step [5/139], Loss: 0.1167\n",
      "Epoch [9/30], Step [6/139], Loss: 0.1003\n",
      "Epoch [9/30], Step [7/139], Loss: 0.0895\n",
      "Epoch [9/30], Step [8/139], Loss: 0.0759\n",
      "Epoch [9/30], Step [9/139], Loss: 0.1086\n",
      "Epoch [9/30], Step [10/139], Loss: 0.0779\n",
      "Epoch [9/30], Step [11/139], Loss: 0.0708\n",
      "Epoch [9/30], Step [12/139], Loss: 0.1051\n",
      "Epoch [9/30], Step [13/139], Loss: 0.1067\n",
      "Epoch [9/30], Step [14/139], Loss: 0.1085\n",
      "Epoch [9/30], Step [15/139], Loss: 0.1005\n",
      "Epoch [9/30], Step [16/139], Loss: 0.1018\n",
      "Epoch [9/30], Step [17/139], Loss: 0.1187\n",
      "Epoch [9/30], Step [18/139], Loss: 0.0892\n",
      "Epoch [9/30], Step [19/139], Loss: 0.0975\n",
      "Epoch [9/30], Step [20/139], Loss: 0.0983\n",
      "Epoch [9/30], Step [21/139], Loss: 0.1074\n",
      "Epoch [9/30], Step [22/139], Loss: 0.0967\n",
      "Epoch [9/30], Step [23/139], Loss: 0.1311\n",
      "Epoch [9/30], Step [24/139], Loss: 0.1132\n",
      "Epoch [9/30], Step [25/139], Loss: 0.0908\n",
      "Epoch [9/30], Step [26/139], Loss: 0.1094\n",
      "Epoch [9/30], Step [27/139], Loss: 0.0896\n",
      "Epoch [9/30], Step [28/139], Loss: 0.0926\n",
      "Epoch [9/30], Step [29/139], Loss: 0.0748\n",
      "Epoch [9/30], Step [30/139], Loss: 0.0938\n",
      "Epoch [9/30], Step [31/139], Loss: 0.0836\n",
      "Epoch [9/30], Step [32/139], Loss: 0.1172\n",
      "Epoch [9/30], Step [33/139], Loss: 0.1193\n",
      "Epoch [9/30], Step [34/139], Loss: 0.0796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/30], Step [35/139], Loss: 0.0998\n",
      "Epoch [9/30], Step [36/139], Loss: 0.1266\n",
      "Epoch [9/30], Step [37/139], Loss: 0.1107\n",
      "Epoch [9/30], Step [38/139], Loss: 0.0848\n",
      "Epoch [9/30], Step [39/139], Loss: 0.0744\n",
      "Epoch [9/30], Step [40/139], Loss: 0.0647\n",
      "Epoch [9/30], Step [41/139], Loss: 0.1030\n",
      "Epoch [9/30], Step [42/139], Loss: 0.1166\n",
      "Epoch [9/30], Step [43/139], Loss: 0.0731\n",
      "Epoch [9/30], Step [44/139], Loss: 0.1082\n",
      "Epoch [9/30], Step [45/139], Loss: 0.1183\n",
      "Epoch [9/30], Step [46/139], Loss: 0.0799\n",
      "Epoch [9/30], Step [47/139], Loss: 0.1205\n",
      "Epoch [9/30], Step [48/139], Loss: 0.0991\n",
      "Epoch [9/30], Step [49/139], Loss: 0.0841\n",
      "Epoch [9/30], Step [50/139], Loss: 0.0834\n",
      "Epoch [9/30], Step [51/139], Loss: 0.0831\n",
      "Epoch [9/30], Step [52/139], Loss: 0.0811\n",
      "Epoch [9/30], Step [53/139], Loss: 0.0850\n",
      "Epoch [9/30], Step [54/139], Loss: 0.1096\n",
      "Epoch [9/30], Step [55/139], Loss: 0.0987\n",
      "Epoch [9/30], Step [56/139], Loss: 0.0982\n",
      "Epoch [9/30], Step [57/139], Loss: 0.1026\n",
      "Epoch [9/30], Step [58/139], Loss: 0.1029\n",
      "Epoch [9/30], Step [59/139], Loss: 0.0855\n",
      "Epoch [9/30], Step [60/139], Loss: 0.0927\n",
      "Epoch [9/30], Step [61/139], Loss: 0.1173\n",
      "Epoch [9/30], Step [62/139], Loss: 0.1032\n",
      "Epoch [9/30], Step [63/139], Loss: 0.0730\n",
      "Epoch [9/30], Step [64/139], Loss: 0.1209\n",
      "Epoch [9/30], Step [65/139], Loss: 0.0796\n",
      "Epoch [9/30], Step [66/139], Loss: 0.0853\n",
      "Epoch [9/30], Step [67/139], Loss: 0.0922\n",
      "Epoch [9/30], Step [68/139], Loss: 0.1244\n",
      "Epoch [9/30], Step [69/139], Loss: 0.0755\n",
      "Epoch [9/30], Step [70/139], Loss: 0.1234\n",
      "Epoch [9/30], Step [71/139], Loss: 0.0943\n",
      "Epoch [9/30], Step [72/139], Loss: 0.0961\n",
      "Epoch [9/30], Step [73/139], Loss: 0.0884\n",
      "Epoch [9/30], Step [74/139], Loss: 0.0886\n",
      "Epoch [9/30], Step [75/139], Loss: 0.0735\n",
      "Epoch [9/30], Step [76/139], Loss: 0.0973\n",
      "Epoch [9/30], Step [77/139], Loss: 0.1063\n",
      "Epoch [9/30], Step [78/139], Loss: 0.0808\n",
      "Epoch [9/30], Step [79/139], Loss: 0.0743\n",
      "Epoch [9/30], Step [80/139], Loss: 0.1140\n",
      "Epoch [9/30], Step [81/139], Loss: 0.0942\n",
      "Epoch [9/30], Step [82/139], Loss: 0.1159\n",
      "Epoch [9/30], Step [83/139], Loss: 0.0996\n",
      "Epoch [9/30], Step [84/139], Loss: 0.1754\n",
      "Epoch [9/30], Step [85/139], Loss: 0.0838\n",
      "Epoch [9/30], Step [86/139], Loss: 0.1104\n",
      "Epoch [9/30], Step [87/139], Loss: 0.1090\n",
      "Epoch [9/30], Step [88/139], Loss: 0.1120\n",
      "Epoch [9/30], Step [89/139], Loss: 0.0952\n",
      "Epoch [9/30], Step [90/139], Loss: 0.0997\n",
      "Epoch [9/30], Step [91/139], Loss: 0.1114\n",
      "Epoch [9/30], Step [92/139], Loss: 0.0757\n",
      "Epoch [9/30], Step [93/139], Loss: 0.0756\n",
      "Epoch [9/30], Step [94/139], Loss: 0.0975\n",
      "Epoch [9/30], Step [95/139], Loss: 0.1013\n",
      "Epoch [9/30], Step [96/139], Loss: 0.0867\n",
      "Epoch [9/30], Step [97/139], Loss: 0.1127\n",
      "Epoch [9/30], Step [98/139], Loss: 0.0900\n",
      "Epoch [9/30], Step [99/139], Loss: 0.0921\n",
      "Epoch [9/30], Step [100/139], Loss: 0.0811\n",
      "Epoch [9/30], Step [101/139], Loss: 0.0778\n",
      "Epoch [9/30], Step [102/139], Loss: 0.0980\n",
      "Epoch [9/30], Step [103/139], Loss: 0.0709\n",
      "Epoch [9/30], Step [104/139], Loss: 0.0800\n",
      "Epoch [9/30], Step [105/139], Loss: 0.0896\n",
      "Epoch [9/30], Step [106/139], Loss: 0.1025\n",
      "Epoch [9/30], Step [107/139], Loss: 0.0631\n",
      "Epoch [9/30], Step [108/139], Loss: 0.0937\n",
      "Epoch [9/30], Step [109/139], Loss: 0.0757\n",
      "Epoch [9/30], Step [110/139], Loss: 0.0796\n",
      "Epoch [9/30], Step [111/139], Loss: 0.0836\n",
      "Epoch [9/30], Step [112/139], Loss: 0.0919\n",
      "Epoch [9/30], Step [113/139], Loss: 0.0757\n",
      "Epoch [9/30], Step [114/139], Loss: 0.0811\n",
      "Epoch [9/30], Step [115/139], Loss: 0.0766\n",
      "Epoch [9/30], Step [116/139], Loss: 0.1165\n",
      "Epoch [9/30], Step [117/139], Loss: 0.0705\n",
      "Epoch [9/30], Step [118/139], Loss: 0.0917\n",
      "Epoch [9/30], Step [119/139], Loss: 0.1136\n",
      "Epoch [9/30], Step [120/139], Loss: 0.0975\n",
      "Epoch [9/30], Step [121/139], Loss: 0.0672\n",
      "Epoch [9/30], Step [122/139], Loss: 0.0672\n",
      "Epoch [9/30], Step [123/139], Loss: 0.0977\n",
      "Epoch [9/30], Step [124/139], Loss: 0.0951\n",
      "Epoch [9/30], Step [125/139], Loss: 0.0911\n",
      "Epoch [9/30], Step [126/139], Loss: 0.1147\n",
      "Epoch [9/30], Step [127/139], Loss: 0.0971\n",
      "Epoch [9/30], Step [128/139], Loss: 0.1004\n",
      "Epoch [9/30], Step [129/139], Loss: 0.0854\n",
      "Epoch [9/30], Step [130/139], Loss: 0.1012\n",
      "Epoch [9/30], Step [131/139], Loss: 0.0911\n",
      "Epoch [9/30], Step [132/139], Loss: 0.0831\n",
      "Epoch [9/30], Step [133/139], Loss: 0.1003\n",
      "Epoch [9/30], Step [134/139], Loss: 0.0850\n",
      "Epoch [9/30], Step [135/139], Loss: 0.1317\n",
      "Epoch [9/30], Step [136/139], Loss: 0.0948\n",
      "Epoch [9/30], Step [137/139], Loss: 0.1002\n",
      "Epoch [9/30], Step [138/139], Loss: 0.0911\n",
      "Epoch [9/30], Step [139/139], Loss: 0.0521\n",
      "Start validation #9\n",
      "Validation #9  Average Loss: 0.0953\n",
      "Best performance at epoch: 9\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [10/30], Step [1/139], Loss: 0.0737\n",
      "Epoch [10/30], Step [2/139], Loss: 0.0955\n",
      "Epoch [10/30], Step [3/139], Loss: 0.0847\n",
      "Epoch [10/30], Step [4/139], Loss: 0.1103\n",
      "Epoch [10/30], Step [5/139], Loss: 0.0792\n",
      "Epoch [10/30], Step [6/139], Loss: 0.1223\n",
      "Epoch [10/30], Step [7/139], Loss: 0.1350\n",
      "Epoch [10/30], Step [8/139], Loss: 0.0879\n",
      "Epoch [10/30], Step [9/139], Loss: 0.0710\n",
      "Epoch [10/30], Step [10/139], Loss: 0.0894\n",
      "Epoch [10/30], Step [11/139], Loss: 0.0883\n",
      "Epoch [10/30], Step [12/139], Loss: 0.1119\n",
      "Epoch [10/30], Step [13/139], Loss: 0.1206\n",
      "Epoch [10/30], Step [14/139], Loss: 0.0888\n",
      "Epoch [10/30], Step [15/139], Loss: 0.0716\n",
      "Epoch [10/30], Step [16/139], Loss: 0.0890\n",
      "Epoch [10/30], Step [17/139], Loss: 0.0842\n",
      "Epoch [10/30], Step [18/139], Loss: 0.0834\n",
      "Epoch [10/30], Step [19/139], Loss: 0.0894\n",
      "Epoch [10/30], Step [20/139], Loss: 0.1459\n",
      "Epoch [10/30], Step [21/139], Loss: 0.1104\n",
      "Epoch [10/30], Step [22/139], Loss: 0.0904\n",
      "Epoch [10/30], Step [23/139], Loss: 0.0795\n",
      "Epoch [10/30], Step [24/139], Loss: 0.0919\n",
      "Epoch [10/30], Step [25/139], Loss: 0.0839\n",
      "Epoch [10/30], Step [26/139], Loss: 0.0967\n",
      "Epoch [10/30], Step [27/139], Loss: 0.0902\n",
      "Epoch [10/30], Step [28/139], Loss: 0.0981\n",
      "Epoch [10/30], Step [29/139], Loss: 0.0824\n",
      "Epoch [10/30], Step [30/139], Loss: 0.1083\n",
      "Epoch [10/30], Step [31/139], Loss: 0.1007\n",
      "Epoch [10/30], Step [32/139], Loss: 0.0831\n",
      "Epoch [10/30], Step [33/139], Loss: 0.0771\n",
      "Epoch [10/30], Step [34/139], Loss: 0.0943\n",
      "Epoch [10/30], Step [35/139], Loss: 0.0778\n",
      "Epoch [10/30], Step [36/139], Loss: 0.1160\n",
      "Epoch [10/30], Step [37/139], Loss: 0.0910\n",
      "Epoch [10/30], Step [38/139], Loss: 0.1029\n",
      "Epoch [10/30], Step [39/139], Loss: 0.0993\n",
      "Epoch [10/30], Step [40/139], Loss: 0.0873\n",
      "Epoch [10/30], Step [41/139], Loss: 0.0767\n",
      "Epoch [10/30], Step [42/139], Loss: 0.0635\n",
      "Epoch [10/30], Step [43/139], Loss: 0.0643\n",
      "Epoch [10/30], Step [44/139], Loss: 0.0840\n",
      "Epoch [10/30], Step [45/139], Loss: 0.0916\n",
      "Epoch [10/30], Step [46/139], Loss: 0.0811\n",
      "Epoch [10/30], Step [47/139], Loss: 0.0853\n",
      "Epoch [10/30], Step [48/139], Loss: 0.0877\n",
      "Epoch [10/30], Step [49/139], Loss: 0.0921\n",
      "Epoch [10/30], Step [50/139], Loss: 0.1204\n",
      "Epoch [10/30], Step [51/139], Loss: 0.0885\n",
      "Epoch [10/30], Step [52/139], Loss: 0.0883\n",
      "Epoch [10/30], Step [53/139], Loss: 0.0878\n",
      "Epoch [10/30], Step [54/139], Loss: 0.1095\n",
      "Epoch [10/30], Step [55/139], Loss: 0.0885\n",
      "Epoch [10/30], Step [56/139], Loss: 0.0783\n",
      "Epoch [10/30], Step [57/139], Loss: 0.0980\n",
      "Epoch [10/30], Step [58/139], Loss: 0.1149\n",
      "Epoch [10/30], Step [59/139], Loss: 0.1198\n",
      "Epoch [10/30], Step [60/139], Loss: 0.0736\n",
      "Epoch [10/30], Step [61/139], Loss: 0.0897\n",
      "Epoch [10/30], Step [62/139], Loss: 0.1146\n",
      "Epoch [10/30], Step [63/139], Loss: 0.0788\n",
      "Epoch [10/30], Step [64/139], Loss: 0.0942\n",
      "Epoch [10/30], Step [65/139], Loss: 0.0958\n",
      "Epoch [10/30], Step [66/139], Loss: 0.0852\n",
      "Epoch [10/30], Step [67/139], Loss: 0.0985\n",
      "Epoch [10/30], Step [68/139], Loss: 0.0875\n",
      "Epoch [10/30], Step [69/139], Loss: 0.0718\n",
      "Epoch [10/30], Step [70/139], Loss: 0.0859\n",
      "Epoch [10/30], Step [71/139], Loss: 0.0835\n",
      "Epoch [10/30], Step [72/139], Loss: 0.0757\n",
      "Epoch [10/30], Step [73/139], Loss: 0.0812\n",
      "Epoch [10/30], Step [74/139], Loss: 0.1027\n",
      "Epoch [10/30], Step [75/139], Loss: 0.0797\n",
      "Epoch [10/30], Step [76/139], Loss: 0.0682\n",
      "Epoch [10/30], Step [77/139], Loss: 0.1268\n",
      "Epoch [10/30], Step [78/139], Loss: 0.0925\n",
      "Epoch [10/30], Step [79/139], Loss: 0.0989\n",
      "Epoch [10/30], Step [80/139], Loss: 0.1021\n",
      "Epoch [10/30], Step [81/139], Loss: 0.0884\n",
      "Epoch [10/30], Step [82/139], Loss: 0.0924\n",
      "Epoch [10/30], Step [83/139], Loss: 0.0854\n",
      "Epoch [10/30], Step [84/139], Loss: 0.0674\n",
      "Epoch [10/30], Step [85/139], Loss: 0.0921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Step [86/139], Loss: 0.0887\n",
      "Epoch [10/30], Step [87/139], Loss: 0.1145\n",
      "Epoch [10/30], Step [88/139], Loss: 0.1128\n",
      "Epoch [10/30], Step [89/139], Loss: 0.0943\n",
      "Epoch [10/30], Step [90/139], Loss: 0.1025\n",
      "Epoch [10/30], Step [91/139], Loss: 0.1030\n",
      "Epoch [10/30], Step [92/139], Loss: 0.0836\n",
      "Epoch [10/30], Step [93/139], Loss: 0.0969\n",
      "Epoch [10/30], Step [94/139], Loss: 0.0973\n",
      "Epoch [10/30], Step [95/139], Loss: 0.1121\n",
      "Epoch [10/30], Step [96/139], Loss: 0.1111\n",
      "Epoch [10/30], Step [97/139], Loss: 0.0941\n",
      "Epoch [10/30], Step [98/139], Loss: 0.1166\n",
      "Epoch [10/30], Step [99/139], Loss: 0.1028\n",
      "Epoch [10/30], Step [100/139], Loss: 0.1059\n",
      "Epoch [10/30], Step [101/139], Loss: 0.0961\n",
      "Epoch [10/30], Step [102/139], Loss: 0.1011\n",
      "Epoch [10/30], Step [103/139], Loss: 0.1196\n",
      "Epoch [10/30], Step [104/139], Loss: 0.0790\n",
      "Epoch [10/30], Step [105/139], Loss: 0.0710\n",
      "Epoch [10/30], Step [106/139], Loss: 0.0682\n",
      "Epoch [10/30], Step [107/139], Loss: 0.0769\n",
      "Epoch [10/30], Step [108/139], Loss: 0.0845\n",
      "Epoch [10/30], Step [109/139], Loss: 0.0962\n",
      "Epoch [10/30], Step [110/139], Loss: 0.0771\n",
      "Epoch [10/30], Step [111/139], Loss: 0.1123\n",
      "Epoch [10/30], Step [112/139], Loss: 0.0852\n",
      "Epoch [10/30], Step [113/139], Loss: 0.0800\n",
      "Epoch [10/30], Step [114/139], Loss: 0.0689\n",
      "Epoch [10/30], Step [115/139], Loss: 0.0967\n",
      "Epoch [10/30], Step [116/139], Loss: 0.0845\n",
      "Epoch [10/30], Step [117/139], Loss: 0.0979\n",
      "Epoch [10/30], Step [118/139], Loss: 0.0962\n",
      "Epoch [10/30], Step [119/139], Loss: 0.1148\n",
      "Epoch [10/30], Step [120/139], Loss: 0.0991\n",
      "Epoch [10/30], Step [121/139], Loss: 0.0708\n",
      "Epoch [10/30], Step [122/139], Loss: 0.0743\n",
      "Epoch [10/30], Step [123/139], Loss: 0.1026\n",
      "Epoch [10/30], Step [124/139], Loss: 0.0843\n",
      "Epoch [10/30], Step [125/139], Loss: 0.0746\n",
      "Epoch [10/30], Step [126/139], Loss: 0.1147\n",
      "Epoch [10/30], Step [127/139], Loss: 0.0718\n",
      "Epoch [10/30], Step [128/139], Loss: 0.0994\n",
      "Epoch [10/30], Step [129/139], Loss: 0.0961\n",
      "Epoch [10/30], Step [130/139], Loss: 0.1077\n",
      "Epoch [10/30], Step [131/139], Loss: 0.0873\n",
      "Epoch [10/30], Step [132/139], Loss: 0.0911\n",
      "Epoch [10/30], Step [133/139], Loss: 0.0829\n",
      "Epoch [10/30], Step [134/139], Loss: 0.0933\n",
      "Epoch [10/30], Step [135/139], Loss: 0.1310\n",
      "Epoch [10/30], Step [136/139], Loss: 0.0997\n",
      "Epoch [10/30], Step [137/139], Loss: 0.1017\n",
      "Epoch [10/30], Step [138/139], Loss: 0.0931\n",
      "Epoch [10/30], Step [139/139], Loss: 0.0926\n",
      "Start validation #10\n",
      "Validation #10  Average Loss: 0.0920\n",
      "Best performance at epoch: 10\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [11/30], Step [1/139], Loss: 0.0921\n",
      "Epoch [11/30], Step [2/139], Loss: 0.0833\n",
      "Epoch [11/30], Step [3/139], Loss: 0.1018\n",
      "Epoch [11/30], Step [4/139], Loss: 0.0917\n",
      "Epoch [11/30], Step [5/139], Loss: 0.0549\n",
      "Epoch [11/30], Step [6/139], Loss: 0.0979\n",
      "Epoch [11/30], Step [7/139], Loss: 0.0931\n",
      "Epoch [11/30], Step [8/139], Loss: 0.0961\n",
      "Epoch [11/30], Step [9/139], Loss: 0.0917\n",
      "Epoch [11/30], Step [10/139], Loss: 0.0815\n",
      "Epoch [11/30], Step [11/139], Loss: 0.0930\n",
      "Epoch [11/30], Step [12/139], Loss: 0.0933\n",
      "Epoch [11/30], Step [13/139], Loss: 0.1120\n",
      "Epoch [11/30], Step [14/139], Loss: 0.1189\n",
      "Epoch [11/30], Step [15/139], Loss: 0.0936\n",
      "Epoch [11/30], Step [16/139], Loss: 0.1115\n",
      "Epoch [11/30], Step [17/139], Loss: 0.0801\n",
      "Epoch [11/30], Step [18/139], Loss: 0.0976\n",
      "Epoch [11/30], Step [19/139], Loss: 0.1052\n",
      "Epoch [11/30], Step [20/139], Loss: 0.0771\n",
      "Epoch [11/30], Step [21/139], Loss: 0.0885\n",
      "Epoch [11/30], Step [22/139], Loss: 0.1081\n",
      "Epoch [11/30], Step [23/139], Loss: 0.0691\n",
      "Epoch [11/30], Step [24/139], Loss: 0.1292\n",
      "Epoch [11/30], Step [25/139], Loss: 0.1179\n",
      "Epoch [11/30], Step [26/139], Loss: 0.1087\n",
      "Epoch [11/30], Step [27/139], Loss: 0.1010\n",
      "Epoch [11/30], Step [28/139], Loss: 0.0846\n",
      "Epoch [11/30], Step [29/139], Loss: 0.0815\n",
      "Epoch [11/30], Step [30/139], Loss: 0.1031\n",
      "Epoch [11/30], Step [31/139], Loss: 0.0866\n",
      "Epoch [11/30], Step [32/139], Loss: 0.0973\n",
      "Epoch [11/30], Step [33/139], Loss: 0.0851\n",
      "Epoch [11/30], Step [34/139], Loss: 0.0853\n",
      "Epoch [11/30], Step [35/139], Loss: 0.0718\n",
      "Epoch [11/30], Step [36/139], Loss: 0.0904\n",
      "Epoch [11/30], Step [37/139], Loss: 0.0749\n",
      "Epoch [11/30], Step [38/139], Loss: 0.0894\n",
      "Epoch [11/30], Step [39/139], Loss: 0.0789\n",
      "Epoch [11/30], Step [40/139], Loss: 0.0905\n",
      "Epoch [11/30], Step [41/139], Loss: 0.1069\n",
      "Epoch [11/30], Step [42/139], Loss: 0.0951\n",
      "Epoch [11/30], Step [43/139], Loss: 0.0870\n",
      "Epoch [11/30], Step [44/139], Loss: 0.0805\n",
      "Epoch [11/30], Step [45/139], Loss: 0.0930\n",
      "Epoch [11/30], Step [46/139], Loss: 0.1018\n",
      "Epoch [11/30], Step [47/139], Loss: 0.0851\n",
      "Epoch [11/30], Step [48/139], Loss: 0.0759\n",
      "Epoch [11/30], Step [49/139], Loss: 0.0892\n",
      "Epoch [11/30], Step [50/139], Loss: 0.0864\n",
      "Epoch [11/30], Step [51/139], Loss: 0.0973\n",
      "Epoch [11/30], Step [52/139], Loss: 0.0948\n",
      "Epoch [11/30], Step [53/139], Loss: 0.0732\n",
      "Epoch [11/30], Step [54/139], Loss: 0.0663\n",
      "Epoch [11/30], Step [55/139], Loss: 0.0697\n",
      "Epoch [11/30], Step [56/139], Loss: 0.0747\n",
      "Epoch [11/30], Step [57/139], Loss: 0.0720\n",
      "Epoch [11/30], Step [58/139], Loss: 0.0967\n",
      "Epoch [11/30], Step [59/139], Loss: 0.0956\n",
      "Epoch [11/30], Step [60/139], Loss: 0.1078\n",
      "Epoch [11/30], Step [61/139], Loss: 0.0768\n",
      "Epoch [11/30], Step [62/139], Loss: 0.0978\n",
      "Epoch [11/30], Step [63/139], Loss: 0.1066\n",
      "Epoch [11/30], Step [64/139], Loss: 0.0900\n",
      "Epoch [11/30], Step [65/139], Loss: 0.1233\n",
      "Epoch [11/30], Step [66/139], Loss: 0.0934\n",
      "Epoch [11/30], Step [67/139], Loss: 0.0903\n",
      "Epoch [11/30], Step [68/139], Loss: 0.0745\n",
      "Epoch [11/30], Step [69/139], Loss: 0.1037\n",
      "Epoch [11/30], Step [70/139], Loss: 0.0827\n",
      "Epoch [11/30], Step [71/139], Loss: 0.0851\n",
      "Epoch [11/30], Step [72/139], Loss: 0.0834\n",
      "Epoch [11/30], Step [73/139], Loss: 0.0735\n",
      "Epoch [11/30], Step [74/139], Loss: 0.0708\n",
      "Epoch [11/30], Step [75/139], Loss: 0.0663\n",
      "Epoch [11/30], Step [76/139], Loss: 0.1007\n",
      "Epoch [11/30], Step [77/139], Loss: 0.1011\n",
      "Epoch [11/30], Step [78/139], Loss: 0.1141\n",
      "Epoch [11/30], Step [79/139], Loss: 0.0859\n",
      "Epoch [11/30], Step [80/139], Loss: 0.1269\n",
      "Epoch [11/30], Step [81/139], Loss: 0.0943\n",
      "Epoch [11/30], Step [82/139], Loss: 0.1123\n",
      "Epoch [11/30], Step [83/139], Loss: 0.0912\n",
      "Epoch [11/30], Step [84/139], Loss: 0.0866\n",
      "Epoch [11/30], Step [85/139], Loss: 0.0768\n",
      "Epoch [11/30], Step [86/139], Loss: 0.0722\n",
      "Epoch [11/30], Step [87/139], Loss: 0.0927\n",
      "Epoch [11/30], Step [88/139], Loss: 0.1338\n",
      "Epoch [11/30], Step [89/139], Loss: 0.0916\n",
      "Epoch [11/30], Step [90/139], Loss: 0.0964\n",
      "Epoch [11/30], Step [91/139], Loss: 0.1072\n",
      "Epoch [11/30], Step [92/139], Loss: 0.1032\n",
      "Epoch [11/30], Step [93/139], Loss: 0.0650\n",
      "Epoch [11/30], Step [94/139], Loss: 0.0944\n",
      "Epoch [11/30], Step [95/139], Loss: 0.1056\n",
      "Epoch [11/30], Step [96/139], Loss: 0.0891\n",
      "Epoch [11/30], Step [97/139], Loss: 0.0791\n",
      "Epoch [11/30], Step [98/139], Loss: 0.0799\n",
      "Epoch [11/30], Step [99/139], Loss: 0.0890\n",
      "Epoch [11/30], Step [100/139], Loss: 0.0852\n",
      "Epoch [11/30], Step [101/139], Loss: 0.0982\n",
      "Epoch [11/30], Step [102/139], Loss: 0.0700\n",
      "Epoch [11/30], Step [103/139], Loss: 0.1262\n",
      "Epoch [11/30], Step [104/139], Loss: 0.0854\n",
      "Epoch [11/30], Step [105/139], Loss: 0.0815\n",
      "Epoch [11/30], Step [106/139], Loss: 0.0745\n",
      "Epoch [11/30], Step [107/139], Loss: 0.1129\n",
      "Epoch [11/30], Step [108/139], Loss: 0.0848\n",
      "Epoch [11/30], Step [109/139], Loss: 0.0793\n",
      "Epoch [11/30], Step [110/139], Loss: 0.1079\n",
      "Epoch [11/30], Step [111/139], Loss: 0.1175\n",
      "Epoch [11/30], Step [112/139], Loss: 0.0639\n",
      "Epoch [11/30], Step [113/139], Loss: 0.0735\n",
      "Epoch [11/30], Step [114/139], Loss: 0.0715\n",
      "Epoch [11/30], Step [115/139], Loss: 0.0962\n",
      "Epoch [11/30], Step [116/139], Loss: 0.0949\n",
      "Epoch [11/30], Step [117/139], Loss: 0.0716\n",
      "Epoch [11/30], Step [118/139], Loss: 0.0964\n",
      "Epoch [11/30], Step [119/139], Loss: 0.0996\n",
      "Epoch [11/30], Step [120/139], Loss: 0.0833\n",
      "Epoch [11/30], Step [121/139], Loss: 0.0831\n",
      "Epoch [11/30], Step [122/139], Loss: 0.0840\n",
      "Epoch [11/30], Step [123/139], Loss: 0.0791\n",
      "Epoch [11/30], Step [124/139], Loss: 0.0847\n",
      "Epoch [11/30], Step [125/139], Loss: 0.1084\n",
      "Epoch [11/30], Step [126/139], Loss: 0.0866\n",
      "Epoch [11/30], Step [127/139], Loss: 0.1074\n",
      "Epoch [11/30], Step [128/139], Loss: 0.0720\n",
      "Epoch [11/30], Step [129/139], Loss: 0.1023\n",
      "Epoch [11/30], Step [130/139], Loss: 0.0921\n",
      "Epoch [11/30], Step [131/139], Loss: 0.1110\n",
      "Epoch [11/30], Step [132/139], Loss: 0.0887\n",
      "Epoch [11/30], Step [133/139], Loss: 0.0895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30], Step [134/139], Loss: 0.0961\n",
      "Epoch [11/30], Step [135/139], Loss: 0.0845\n",
      "Epoch [11/30], Step [136/139], Loss: 0.0837\n",
      "Epoch [11/30], Step [137/139], Loss: 0.0686\n",
      "Epoch [11/30], Step [138/139], Loss: 0.0851\n",
      "Epoch [11/30], Step [139/139], Loss: 0.0876\n",
      "Start validation #11\n",
      "Validation #11  Average Loss: 0.0903\n",
      "Best performance at epoch: 11\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [12/30], Step [1/139], Loss: 0.0990\n",
      "Epoch [12/30], Step [2/139], Loss: 0.0828\n",
      "Epoch [12/30], Step [3/139], Loss: 0.0616\n",
      "Epoch [12/30], Step [4/139], Loss: 0.0962\n",
      "Epoch [12/30], Step [5/139], Loss: 0.1003\n",
      "Epoch [12/30], Step [6/139], Loss: 0.1001\n",
      "Epoch [12/30], Step [7/139], Loss: 0.0883\n",
      "Epoch [12/30], Step [8/139], Loss: 0.0640\n",
      "Epoch [12/30], Step [9/139], Loss: 0.0736\n",
      "Epoch [12/30], Step [10/139], Loss: 0.0662\n",
      "Epoch [12/30], Step [11/139], Loss: 0.0897\n",
      "Epoch [12/30], Step [12/139], Loss: 0.0743\n",
      "Epoch [12/30], Step [13/139], Loss: 0.0820\n",
      "Epoch [12/30], Step [14/139], Loss: 0.1167\n",
      "Epoch [12/30], Step [15/139], Loss: 0.0929\n",
      "Epoch [12/30], Step [16/139], Loss: 0.1117\n",
      "Epoch [12/30], Step [17/139], Loss: 0.0659\n",
      "Epoch [12/30], Step [18/139], Loss: 0.0756\n",
      "Epoch [12/30], Step [19/139], Loss: 0.0766\n",
      "Epoch [12/30], Step [20/139], Loss: 0.0921\n",
      "Epoch [12/30], Step [21/139], Loss: 0.1105\n",
      "Epoch [12/30], Step [22/139], Loss: 0.0736\n",
      "Epoch [12/30], Step [23/139], Loss: 0.0641\n",
      "Epoch [12/30], Step [24/139], Loss: 0.1014\n",
      "Epoch [12/30], Step [25/139], Loss: 0.0722\n",
      "Epoch [12/30], Step [26/139], Loss: 0.0814\n",
      "Epoch [12/30], Step [27/139], Loss: 0.1158\n",
      "Epoch [12/30], Step [28/139], Loss: 0.0997\n",
      "Epoch [12/30], Step [29/139], Loss: 0.0804\n",
      "Epoch [12/30], Step [30/139], Loss: 0.0817\n",
      "Epoch [12/30], Step [31/139], Loss: 0.1165\n",
      "Epoch [12/30], Step [32/139], Loss: 0.0861\n",
      "Epoch [12/30], Step [33/139], Loss: 0.0745\n",
      "Epoch [12/30], Step [34/139], Loss: 0.1026\n",
      "Epoch [12/30], Step [35/139], Loss: 0.0820\n",
      "Epoch [12/30], Step [36/139], Loss: 0.0938\n",
      "Epoch [12/30], Step [37/139], Loss: 0.0685\n",
      "Epoch [12/30], Step [38/139], Loss: 0.0666\n",
      "Epoch [12/30], Step [39/139], Loss: 0.0917\n",
      "Epoch [12/30], Step [40/139], Loss: 0.0885\n",
      "Epoch [12/30], Step [41/139], Loss: 0.1052\n",
      "Epoch [12/30], Step [42/139], Loss: 0.0982\n",
      "Epoch [12/30], Step [43/139], Loss: 0.1030\n",
      "Epoch [12/30], Step [44/139], Loss: 0.0790\n",
      "Epoch [12/30], Step [45/139], Loss: 0.0828\n",
      "Epoch [12/30], Step [46/139], Loss: 0.0896\n",
      "Epoch [12/30], Step [47/139], Loss: 0.0947\n",
      "Epoch [12/30], Step [48/139], Loss: 0.1118\n",
      "Epoch [12/30], Step [49/139], Loss: 0.0984\n",
      "Epoch [12/30], Step [50/139], Loss: 0.0816\n",
      "Epoch [12/30], Step [51/139], Loss: 0.0787\n",
      "Epoch [12/30], Step [52/139], Loss: 0.0933\n",
      "Epoch [12/30], Step [53/139], Loss: 0.0837\n",
      "Epoch [12/30], Step [54/139], Loss: 0.0907\n",
      "Epoch [12/30], Step [55/139], Loss: 0.0839\n",
      "Epoch [12/30], Step [56/139], Loss: 0.0746\n",
      "Epoch [12/30], Step [57/139], Loss: 0.0794\n",
      "Epoch [12/30], Step [58/139], Loss: 0.0825\n",
      "Epoch [12/30], Step [59/139], Loss: 0.0996\n",
      "Epoch [12/30], Step [60/139], Loss: 0.0953\n",
      "Epoch [12/30], Step [61/139], Loss: 0.0730\n",
      "Epoch [12/30], Step [62/139], Loss: 0.0901\n",
      "Epoch [12/30], Step [63/139], Loss: 0.0754\n",
      "Epoch [12/30], Step [64/139], Loss: 0.0698\n",
      "Epoch [12/30], Step [65/139], Loss: 0.1003\n",
      "Epoch [12/30], Step [66/139], Loss: 0.0905\n",
      "Epoch [12/30], Step [67/139], Loss: 0.0974\n",
      "Epoch [12/30], Step [68/139], Loss: 0.0903\n",
      "Epoch [12/30], Step [69/139], Loss: 0.0766\n",
      "Epoch [12/30], Step [70/139], Loss: 0.1032\n",
      "Epoch [12/30], Step [71/139], Loss: 0.0877\n",
      "Epoch [12/30], Step [72/139], Loss: 0.0933\n",
      "Epoch [12/30], Step [73/139], Loss: 0.0850\n",
      "Epoch [12/30], Step [74/139], Loss: 0.0853\n",
      "Epoch [12/30], Step [75/139], Loss: 0.0712\n",
      "Epoch [12/30], Step [76/139], Loss: 0.1043\n",
      "Epoch [12/30], Step [77/139], Loss: 0.0908\n",
      "Epoch [12/30], Step [78/139], Loss: 0.0667\n",
      "Epoch [12/30], Step [79/139], Loss: 0.0821\n",
      "Epoch [12/30], Step [80/139], Loss: 0.1025\n",
      "Epoch [12/30], Step [81/139], Loss: 0.0815\n",
      "Epoch [12/30], Step [82/139], Loss: 0.1112\n",
      "Epoch [12/30], Step [83/139], Loss: 0.1144\n",
      "Epoch [12/30], Step [84/139], Loss: 0.0668\n",
      "Epoch [12/30], Step [85/139], Loss: 0.1109\n",
      "Epoch [12/30], Step [86/139], Loss: 0.0842\n",
      "Epoch [12/30], Step [87/139], Loss: 0.1137\n",
      "Epoch [12/30], Step [88/139], Loss: 0.1206\n",
      "Epoch [12/30], Step [89/139], Loss: 0.0913\n",
      "Epoch [12/30], Step [90/139], Loss: 0.0610\n",
      "Epoch [12/30], Step [91/139], Loss: 0.0803\n",
      "Epoch [12/30], Step [92/139], Loss: 0.0711\n",
      "Epoch [12/30], Step [93/139], Loss: 0.0883\n",
      "Epoch [12/30], Step [94/139], Loss: 0.1031\n",
      "Epoch [12/30], Step [95/139], Loss: 0.1024\n",
      "Epoch [12/30], Step [96/139], Loss: 0.0746\n",
      "Epoch [12/30], Step [97/139], Loss: 0.0779\n",
      "Epoch [12/30], Step [98/139], Loss: 0.0923\n",
      "Epoch [12/30], Step [99/139], Loss: 0.0637\n",
      "Epoch [12/30], Step [100/139], Loss: 0.1157\n",
      "Epoch [12/30], Step [101/139], Loss: 0.1176\n",
      "Epoch [12/30], Step [102/139], Loss: 0.0819\n",
      "Epoch [12/30], Step [103/139], Loss: 0.1023\n",
      "Epoch [12/30], Step [104/139], Loss: 0.0768\n",
      "Epoch [12/30], Step [105/139], Loss: 0.0793\n",
      "Epoch [12/30], Step [106/139], Loss: 0.0752\n",
      "Epoch [12/30], Step [107/139], Loss: 0.0824\n",
      "Epoch [12/30], Step [108/139], Loss: 0.0886\n",
      "Epoch [12/30], Step [109/139], Loss: 0.0961\n",
      "Epoch [12/30], Step [110/139], Loss: 0.0748\n",
      "Epoch [12/30], Step [111/139], Loss: 0.0997\n",
      "Epoch [12/30], Step [112/139], Loss: 0.0995\n",
      "Epoch [12/30], Step [113/139], Loss: 0.1082\n",
      "Epoch [12/30], Step [114/139], Loss: 0.0817\n",
      "Epoch [12/30], Step [115/139], Loss: 0.0862\n",
      "Epoch [12/30], Step [116/139], Loss: 0.0853\n",
      "Epoch [12/30], Step [117/139], Loss: 0.0964\n",
      "Epoch [12/30], Step [118/139], Loss: 0.1070\n",
      "Epoch [12/30], Step [119/139], Loss: 0.0707\n",
      "Epoch [12/30], Step [120/139], Loss: 0.1056\n",
      "Epoch [12/30], Step [121/139], Loss: 0.0750\n",
      "Epoch [12/30], Step [122/139], Loss: 0.0802\n",
      "Epoch [12/30], Step [123/139], Loss: 0.0828\n",
      "Epoch [12/30], Step [124/139], Loss: 0.0748\n",
      "Epoch [12/30], Step [125/139], Loss: 0.1016\n",
      "Epoch [12/30], Step [126/139], Loss: 0.0734\n",
      "Epoch [12/30], Step [127/139], Loss: 0.1112\n",
      "Epoch [12/30], Step [128/139], Loss: 0.0887\n",
      "Epoch [12/30], Step [129/139], Loss: 0.1018\n",
      "Epoch [12/30], Step [130/139], Loss: 0.0835\n",
      "Epoch [12/30], Step [131/139], Loss: 0.0926\n",
      "Epoch [12/30], Step [132/139], Loss: 0.0788\n",
      "Epoch [12/30], Step [133/139], Loss: 0.1331\n",
      "Epoch [12/30], Step [134/139], Loss: 0.0970\n",
      "Epoch [12/30], Step [135/139], Loss: 0.0826\n",
      "Epoch [12/30], Step [136/139], Loss: 0.0991\n",
      "Epoch [12/30], Step [137/139], Loss: 0.1086\n",
      "Epoch [12/30], Step [138/139], Loss: 0.0752\n",
      "Epoch [12/30], Step [139/139], Loss: 0.0957\n",
      "Start validation #12\n",
      "Validation #12  Average Loss: 0.0883\n",
      "Best performance at epoch: 12\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [13/30], Step [1/139], Loss: 0.0537\n",
      "Epoch [13/30], Step [2/139], Loss: 0.0853\n",
      "Epoch [13/30], Step [3/139], Loss: 0.0642\n",
      "Epoch [13/30], Step [4/139], Loss: 0.0966\n",
      "Epoch [13/30], Step [5/139], Loss: 0.1048\n",
      "Epoch [13/30], Step [6/139], Loss: 0.1036\n",
      "Epoch [13/30], Step [7/139], Loss: 0.0788\n",
      "Epoch [13/30], Step [8/139], Loss: 0.0957\n",
      "Epoch [13/30], Step [9/139], Loss: 0.0997\n",
      "Epoch [13/30], Step [10/139], Loss: 0.0875\n",
      "Epoch [13/30], Step [11/139], Loss: 0.0953\n",
      "Epoch [13/30], Step [12/139], Loss: 0.0758\n",
      "Epoch [13/30], Step [13/139], Loss: 0.0747\n",
      "Epoch [13/30], Step [14/139], Loss: 0.0802\n",
      "Epoch [13/30], Step [15/139], Loss: 0.0777\n",
      "Epoch [13/30], Step [16/139], Loss: 0.0921\n",
      "Epoch [13/30], Step [17/139], Loss: 0.0820\n",
      "Epoch [13/30], Step [18/139], Loss: 0.0921\n",
      "Epoch [13/30], Step [19/139], Loss: 0.0566\n",
      "Epoch [13/30], Step [20/139], Loss: 0.1237\n",
      "Epoch [13/30], Step [21/139], Loss: 0.0874\n",
      "Epoch [13/30], Step [22/139], Loss: 0.0902\n",
      "Epoch [13/30], Step [23/139], Loss: 0.0761\n",
      "Epoch [13/30], Step [24/139], Loss: 0.1066\n",
      "Epoch [13/30], Step [25/139], Loss: 0.1011\n",
      "Epoch [13/30], Step [26/139], Loss: 0.0594\n",
      "Epoch [13/30], Step [27/139], Loss: 0.0707\n",
      "Epoch [13/30], Step [28/139], Loss: 0.0706\n",
      "Epoch [13/30], Step [29/139], Loss: 0.1074\n",
      "Epoch [13/30], Step [30/139], Loss: 0.0869\n",
      "Epoch [13/30], Step [31/139], Loss: 0.0896\n",
      "Epoch [13/30], Step [32/139], Loss: 0.1236\n",
      "Epoch [13/30], Step [33/139], Loss: 0.1124\n",
      "Epoch [13/30], Step [34/139], Loss: 0.1245\n",
      "Epoch [13/30], Step [35/139], Loss: 0.0933\n",
      "Epoch [13/30], Step [36/139], Loss: 0.0867\n",
      "Epoch [13/30], Step [37/139], Loss: 0.0804\n",
      "Epoch [13/30], Step [38/139], Loss: 0.0697\n",
      "Epoch [13/30], Step [39/139], Loss: 0.0725\n",
      "Epoch [13/30], Step [40/139], Loss: 0.0772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/30], Step [41/139], Loss: 0.0707\n",
      "Epoch [13/30], Step [42/139], Loss: 0.0881\n",
      "Epoch [13/30], Step [43/139], Loss: 0.0942\n",
      "Epoch [13/30], Step [44/139], Loss: 0.0733\n",
      "Epoch [13/30], Step [45/139], Loss: 0.0877\n",
      "Epoch [13/30], Step [46/139], Loss: 0.0829\n",
      "Epoch [13/30], Step [47/139], Loss: 0.0622\n",
      "Epoch [13/30], Step [48/139], Loss: 0.1165\n",
      "Epoch [13/30], Step [49/139], Loss: 0.0782\n",
      "Epoch [13/30], Step [50/139], Loss: 0.1034\n",
      "Epoch [13/30], Step [51/139], Loss: 0.0990\n",
      "Epoch [13/30], Step [52/139], Loss: 0.0988\n",
      "Epoch [13/30], Step [53/139], Loss: 0.0915\n",
      "Epoch [13/30], Step [54/139], Loss: 0.0658\n",
      "Epoch [13/30], Step [55/139], Loss: 0.0984\n",
      "Epoch [13/30], Step [56/139], Loss: 0.0693\n",
      "Epoch [13/30], Step [57/139], Loss: 0.0934\n",
      "Epoch [13/30], Step [58/139], Loss: 0.0812\n",
      "Epoch [13/30], Step [59/139], Loss: 0.0608\n",
      "Epoch [13/30], Step [60/139], Loss: 0.0688\n",
      "Epoch [13/30], Step [61/139], Loss: 0.1174\n",
      "Epoch [13/30], Step [62/139], Loss: 0.0983\n",
      "Epoch [13/30], Step [63/139], Loss: 0.0842\n",
      "Epoch [13/30], Step [64/139], Loss: 0.0627\n",
      "Epoch [13/30], Step [65/139], Loss: 0.0617\n",
      "Epoch [13/30], Step [66/139], Loss: 0.0724\n",
      "Epoch [13/30], Step [67/139], Loss: 0.0820\n",
      "Epoch [13/30], Step [68/139], Loss: 0.0932\n",
      "Epoch [13/30], Step [69/139], Loss: 0.0939\n",
      "Epoch [13/30], Step [70/139], Loss: 0.0885\n",
      "Epoch [13/30], Step [71/139], Loss: 0.0780\n",
      "Epoch [13/30], Step [72/139], Loss: 0.0753\n",
      "Epoch [13/30], Step [73/139], Loss: 0.0811\n",
      "Epoch [13/30], Step [74/139], Loss: 0.0951\n",
      "Epoch [13/30], Step [75/139], Loss: 0.0735\n",
      "Epoch [13/30], Step [76/139], Loss: 0.1126\n",
      "Epoch [13/30], Step [77/139], Loss: 0.0728\n",
      "Epoch [13/30], Step [78/139], Loss: 0.1002\n",
      "Epoch [13/30], Step [79/139], Loss: 0.0945\n",
      "Epoch [13/30], Step [80/139], Loss: 0.1313\n",
      "Epoch [13/30], Step [81/139], Loss: 0.0828\n",
      "Epoch [13/30], Step [82/139], Loss: 0.0877\n",
      "Epoch [13/30], Step [83/139], Loss: 0.0743\n",
      "Epoch [13/30], Step [84/139], Loss: 0.0857\n",
      "Epoch [13/30], Step [85/139], Loss: 0.0857\n",
      "Epoch [13/30], Step [86/139], Loss: 0.1012\n",
      "Epoch [13/30], Step [87/139], Loss: 0.0799\n",
      "Epoch [13/30], Step [88/139], Loss: 0.1260\n",
      "Epoch [13/30], Step [89/139], Loss: 0.0852\n",
      "Epoch [13/30], Step [90/139], Loss: 0.0963\n",
      "Epoch [13/30], Step [91/139], Loss: 0.0642\n",
      "Epoch [13/30], Step [92/139], Loss: 0.0869\n",
      "Epoch [13/30], Step [93/139], Loss: 0.0950\n",
      "Epoch [13/30], Step [94/139], Loss: 0.1089\n",
      "Epoch [13/30], Step [95/139], Loss: 0.0674\n",
      "Epoch [13/30], Step [96/139], Loss: 0.0946\n",
      "Epoch [13/30], Step [97/139], Loss: 0.0765\n",
      "Epoch [13/30], Step [98/139], Loss: 0.0790\n",
      "Epoch [13/30], Step [99/139], Loss: 0.0702\n",
      "Epoch [13/30], Step [100/139], Loss: 0.0961\n",
      "Epoch [13/30], Step [101/139], Loss: 0.0706\n",
      "Epoch [13/30], Step [102/139], Loss: 0.0625\n",
      "Epoch [13/30], Step [103/139], Loss: 0.0687\n",
      "Epoch [13/30], Step [104/139], Loss: 0.0729\n",
      "Epoch [13/30], Step [105/139], Loss: 0.1091\n",
      "Epoch [13/30], Step [106/139], Loss: 0.0869\n",
      "Epoch [13/30], Step [107/139], Loss: 0.0789\n",
      "Epoch [13/30], Step [108/139], Loss: 0.0775\n",
      "Epoch [13/30], Step [109/139], Loss: 0.0870\n",
      "Epoch [13/30], Step [110/139], Loss: 0.0664\n",
      "Epoch [13/30], Step [111/139], Loss: 0.0754\n",
      "Epoch [13/30], Step [112/139], Loss: 0.1101\n",
      "Epoch [13/30], Step [113/139], Loss: 0.0712\n",
      "Epoch [13/30], Step [114/139], Loss: 0.0778\n",
      "Epoch [13/30], Step [115/139], Loss: 0.0679\n",
      "Epoch [13/30], Step [116/139], Loss: 0.0989\n",
      "Epoch [13/30], Step [117/139], Loss: 0.1062\n",
      "Epoch [13/30], Step [118/139], Loss: 0.0819\n",
      "Epoch [13/30], Step [119/139], Loss: 0.0871\n",
      "Epoch [13/30], Step [120/139], Loss: 0.1047\n",
      "Epoch [13/30], Step [121/139], Loss: 0.1279\n",
      "Epoch [13/30], Step [122/139], Loss: 0.1098\n",
      "Epoch [13/30], Step [123/139], Loss: 0.0935\n",
      "Epoch [13/30], Step [124/139], Loss: 0.1029\n",
      "Epoch [13/30], Step [125/139], Loss: 0.0734\n",
      "Epoch [13/30], Step [126/139], Loss: 0.0853\n",
      "Epoch [13/30], Step [127/139], Loss: 0.0821\n",
      "Epoch [13/30], Step [128/139], Loss: 0.0790\n",
      "Epoch [13/30], Step [129/139], Loss: 0.0836\n",
      "Epoch [13/30], Step [130/139], Loss: 0.1095\n",
      "Epoch [13/30], Step [131/139], Loss: 0.0995\n",
      "Epoch [13/30], Step [132/139], Loss: 0.0933\n",
      "Epoch [13/30], Step [133/139], Loss: 0.0822\n",
      "Epoch [13/30], Step [134/139], Loss: 0.0833\n",
      "Epoch [13/30], Step [135/139], Loss: 0.0738\n",
      "Epoch [13/30], Step [136/139], Loss: 0.0920\n",
      "Epoch [13/30], Step [137/139], Loss: 0.0854\n",
      "Epoch [13/30], Step [138/139], Loss: 0.1330\n",
      "Epoch [13/30], Step [139/139], Loss: 0.0955\n",
      "Start validation #13\n",
      "Validation #13  Average Loss: 0.0857\n",
      "Best performance at epoch: 13\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [14/30], Step [1/139], Loss: 0.0787\n",
      "Epoch [14/30], Step [2/139], Loss: 0.0873\n",
      "Epoch [14/30], Step [3/139], Loss: 0.1027\n",
      "Epoch [14/30], Step [4/139], Loss: 0.0860\n",
      "Epoch [14/30], Step [5/139], Loss: 0.0947\n",
      "Epoch [14/30], Step [6/139], Loss: 0.0941\n",
      "Epoch [14/30], Step [7/139], Loss: 0.1266\n",
      "Epoch [14/30], Step [8/139], Loss: 0.0802\n",
      "Epoch [14/30], Step [9/139], Loss: 0.0818\n",
      "Epoch [14/30], Step [10/139], Loss: 0.0779\n",
      "Epoch [14/30], Step [11/139], Loss: 0.1018\n",
      "Epoch [14/30], Step [12/139], Loss: 0.1118\n",
      "Epoch [14/30], Step [13/139], Loss: 0.0771\n",
      "Epoch [14/30], Step [14/139], Loss: 0.0788\n",
      "Epoch [14/30], Step [15/139], Loss: 0.0722\n",
      "Epoch [14/30], Step [16/139], Loss: 0.1166\n",
      "Epoch [14/30], Step [17/139], Loss: 0.0818\n",
      "Epoch [14/30], Step [18/139], Loss: 0.1002\n",
      "Epoch [14/30], Step [19/139], Loss: 0.0787\n",
      "Epoch [14/30], Step [20/139], Loss: 0.0845\n",
      "Epoch [14/30], Step [21/139], Loss: 0.1077\n",
      "Epoch [14/30], Step [22/139], Loss: 0.1150\n",
      "Epoch [14/30], Step [23/139], Loss: 0.1019\n",
      "Epoch [14/30], Step [24/139], Loss: 0.0618\n",
      "Epoch [14/30], Step [25/139], Loss: 0.0546\n",
      "Epoch [14/30], Step [26/139], Loss: 0.0727\n",
      "Epoch [14/30], Step [27/139], Loss: 0.0716\n",
      "Epoch [14/30], Step [28/139], Loss: 0.0604\n",
      "Epoch [14/30], Step [29/139], Loss: 0.0778\n",
      "Epoch [14/30], Step [30/139], Loss: 0.1000\n",
      "Epoch [14/30], Step [31/139], Loss: 0.1141\n",
      "Epoch [14/30], Step [32/139], Loss: 0.0803\n",
      "Epoch [14/30], Step [33/139], Loss: 0.1081\n",
      "Epoch [14/30], Step [34/139], Loss: 0.0670\n",
      "Epoch [14/30], Step [35/139], Loss: 0.0726\n",
      "Epoch [14/30], Step [36/139], Loss: 0.0967\n",
      "Epoch [14/30], Step [37/139], Loss: 0.1082\n",
      "Epoch [14/30], Step [38/139], Loss: 0.0548\n",
      "Epoch [14/30], Step [39/139], Loss: 0.0981\n",
      "Epoch [14/30], Step [40/139], Loss: 0.0579\n",
      "Epoch [14/30], Step [41/139], Loss: 0.0755\n",
      "Epoch [14/30], Step [42/139], Loss: 0.1166\n",
      "Epoch [14/30], Step [43/139], Loss: 0.0720\n",
      "Epoch [14/30], Step [44/139], Loss: 0.0721\n",
      "Epoch [14/30], Step [45/139], Loss: 0.1102\n",
      "Epoch [14/30], Step [46/139], Loss: 0.0996\n",
      "Epoch [14/30], Step [47/139], Loss: 0.1083\n",
      "Epoch [14/30], Step [48/139], Loss: 0.1043\n",
      "Epoch [14/30], Step [49/139], Loss: 0.0987\n",
      "Epoch [14/30], Step [50/139], Loss: 0.0947\n",
      "Epoch [14/30], Step [51/139], Loss: 0.0556\n",
      "Epoch [14/30], Step [52/139], Loss: 0.0994\n",
      "Epoch [14/30], Step [53/139], Loss: 0.0950\n",
      "Epoch [14/30], Step [54/139], Loss: 0.0735\n",
      "Epoch [14/30], Step [55/139], Loss: 0.0865\n",
      "Epoch [14/30], Step [56/139], Loss: 0.1172\n",
      "Epoch [14/30], Step [57/139], Loss: 0.0868\n",
      "Epoch [14/30], Step [58/139], Loss: 0.0919\n",
      "Epoch [14/30], Step [59/139], Loss: 0.0752\n",
      "Epoch [14/30], Step [60/139], Loss: 0.0689\n",
      "Epoch [14/30], Step [61/139], Loss: 0.0746\n",
      "Epoch [14/30], Step [62/139], Loss: 0.0984\n",
      "Epoch [14/30], Step [63/139], Loss: 0.0653\n",
      "Epoch [14/30], Step [64/139], Loss: 0.1200\n",
      "Epoch [14/30], Step [65/139], Loss: 0.0782\n",
      "Epoch [14/30], Step [66/139], Loss: 0.1045\n",
      "Epoch [14/30], Step [67/139], Loss: 0.0632\n",
      "Epoch [14/30], Step [68/139], Loss: 0.0670\n",
      "Epoch [14/30], Step [69/139], Loss: 0.0749\n",
      "Epoch [14/30], Step [70/139], Loss: 0.0824\n",
      "Epoch [14/30], Step [71/139], Loss: 0.0720\n",
      "Epoch [14/30], Step [72/139], Loss: 0.0899\n",
      "Epoch [14/30], Step [73/139], Loss: 0.0740\n",
      "Epoch [14/30], Step [74/139], Loss: 0.0929\n",
      "Epoch [14/30], Step [75/139], Loss: 0.1016\n",
      "Epoch [14/30], Step [76/139], Loss: 0.0827\n",
      "Epoch [14/30], Step [77/139], Loss: 0.0830\n",
      "Epoch [14/30], Step [78/139], Loss: 0.0659\n",
      "Epoch [14/30], Step [79/139], Loss: 0.0836\n",
      "Epoch [14/30], Step [80/139], Loss: 0.0984\n",
      "Epoch [14/30], Step [81/139], Loss: 0.0739\n",
      "Epoch [14/30], Step [82/139], Loss: 0.0837\n",
      "Epoch [14/30], Step [83/139], Loss: 0.0917\n",
      "Epoch [14/30], Step [84/139], Loss: 0.0866\n",
      "Epoch [14/30], Step [85/139], Loss: 0.0975\n",
      "Epoch [14/30], Step [86/139], Loss: 0.0854\n",
      "Epoch [14/30], Step [87/139], Loss: 0.0648\n",
      "Epoch [14/30], Step [88/139], Loss: 0.0780\n",
      "Epoch [14/30], Step [89/139], Loss: 0.0828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/30], Step [90/139], Loss: 0.0886\n",
      "Epoch [14/30], Step [91/139], Loss: 0.0698\n",
      "Epoch [14/30], Step [92/139], Loss: 0.0830\n",
      "Epoch [14/30], Step [93/139], Loss: 0.0628\n",
      "Epoch [14/30], Step [94/139], Loss: 0.0554\n",
      "Epoch [14/30], Step [95/139], Loss: 0.0955\n",
      "Epoch [14/30], Step [96/139], Loss: 0.0830\n",
      "Epoch [14/30], Step [97/139], Loss: 0.0876\n",
      "Epoch [14/30], Step [98/139], Loss: 0.0637\n",
      "Epoch [14/30], Step [99/139], Loss: 0.1025\n",
      "Epoch [14/30], Step [100/139], Loss: 0.0797\n",
      "Epoch [14/30], Step [101/139], Loss: 0.0714\n",
      "Epoch [14/30], Step [102/139], Loss: 0.0819\n",
      "Epoch [14/30], Step [103/139], Loss: 0.0831\n",
      "Epoch [14/30], Step [104/139], Loss: 0.0940\n",
      "Epoch [14/30], Step [105/139], Loss: 0.0745\n",
      "Epoch [14/30], Step [106/139], Loss: 0.0809\n",
      "Epoch [14/30], Step [107/139], Loss: 0.0844\n",
      "Epoch [14/30], Step [108/139], Loss: 0.0776\n",
      "Epoch [14/30], Step [109/139], Loss: 0.0698\n",
      "Epoch [14/30], Step [110/139], Loss: 0.0836\n",
      "Epoch [14/30], Step [111/139], Loss: 0.0804\n",
      "Epoch [14/30], Step [112/139], Loss: 0.0977\n",
      "Epoch [14/30], Step [113/139], Loss: 0.1072\n",
      "Epoch [14/30], Step [114/139], Loss: 0.0941\n",
      "Epoch [14/30], Step [115/139], Loss: 0.0845\n",
      "Epoch [14/30], Step [116/139], Loss: 0.0953\n",
      "Epoch [14/30], Step [117/139], Loss: 0.0588\n",
      "Epoch [14/30], Step [118/139], Loss: 0.0921\n",
      "Epoch [14/30], Step [119/139], Loss: 0.0908\n",
      "Epoch [14/30], Step [120/139], Loss: 0.0885\n",
      "Epoch [14/30], Step [121/139], Loss: 0.0734\n",
      "Epoch [14/30], Step [122/139], Loss: 0.0880\n",
      "Epoch [14/30], Step [123/139], Loss: 0.0751\n",
      "Epoch [14/30], Step [124/139], Loss: 0.1203\n",
      "Epoch [14/30], Step [125/139], Loss: 0.0889\n",
      "Epoch [14/30], Step [126/139], Loss: 0.1235\n",
      "Epoch [14/30], Step [127/139], Loss: 0.0634\n",
      "Epoch [14/30], Step [128/139], Loss: 0.0781\n",
      "Epoch [14/30], Step [129/139], Loss: 0.0759\n",
      "Epoch [14/30], Step [130/139], Loss: 0.0966\n",
      "Epoch [14/30], Step [131/139], Loss: 0.0972\n",
      "Epoch [14/30], Step [132/139], Loss: 0.1260\n",
      "Epoch [14/30], Step [133/139], Loss: 0.0781\n",
      "Epoch [14/30], Step [134/139], Loss: 0.0532\n",
      "Epoch [14/30], Step [135/139], Loss: 0.0541\n",
      "Epoch [14/30], Step [136/139], Loss: 0.0884\n",
      "Epoch [14/30], Step [137/139], Loss: 0.0777\n",
      "Epoch [14/30], Step [138/139], Loss: 0.0662\n",
      "Epoch [14/30], Step [139/139], Loss: 0.1385\n",
      "Start validation #14\n",
      "Validation #14  Average Loss: 0.0850\n",
      "Best performance at epoch: 14\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [15/30], Step [1/139], Loss: 0.0943\n",
      "Epoch [15/30], Step [2/139], Loss: 0.0886\n",
      "Epoch [15/30], Step [3/139], Loss: 0.0656\n",
      "Epoch [15/30], Step [4/139], Loss: 0.0860\n",
      "Epoch [15/30], Step [5/139], Loss: 0.0607\n",
      "Epoch [15/30], Step [6/139], Loss: 0.0822\n",
      "Epoch [15/30], Step [7/139], Loss: 0.0751\n",
      "Epoch [15/30], Step [8/139], Loss: 0.1039\n",
      "Epoch [15/30], Step [9/139], Loss: 0.0842\n",
      "Epoch [15/30], Step [10/139], Loss: 0.0691\n",
      "Epoch [15/30], Step [11/139], Loss: 0.0974\n",
      "Epoch [15/30], Step [12/139], Loss: 0.0663\n",
      "Epoch [15/30], Step [13/139], Loss: 0.0870\n",
      "Epoch [15/30], Step [14/139], Loss: 0.0817\n",
      "Epoch [15/30], Step [15/139], Loss: 0.0827\n",
      "Epoch [15/30], Step [16/139], Loss: 0.0932\n",
      "Epoch [15/30], Step [17/139], Loss: 0.1052\n",
      "Epoch [15/30], Step [18/139], Loss: 0.0694\n",
      "Epoch [15/30], Step [19/139], Loss: 0.0726\n",
      "Epoch [15/30], Step [20/139], Loss: 0.1108\n",
      "Epoch [15/30], Step [21/139], Loss: 0.0809\n",
      "Epoch [15/30], Step [22/139], Loss: 0.1048\n",
      "Epoch [15/30], Step [23/139], Loss: 0.0787\n",
      "Epoch [15/30], Step [24/139], Loss: 0.0810\n",
      "Epoch [15/30], Step [25/139], Loss: 0.0752\n",
      "Epoch [15/30], Step [26/139], Loss: 0.0692\n",
      "Epoch [15/30], Step [27/139], Loss: 0.0674\n",
      "Epoch [15/30], Step [28/139], Loss: 0.0755\n",
      "Epoch [15/30], Step [29/139], Loss: 0.0969\n",
      "Epoch [15/30], Step [30/139], Loss: 0.0693\n",
      "Epoch [15/30], Step [31/139], Loss: 0.1116\n",
      "Epoch [15/30], Step [32/139], Loss: 0.1024\n",
      "Epoch [15/30], Step [33/139], Loss: 0.0953\n",
      "Epoch [15/30], Step [34/139], Loss: 0.1095\n",
      "Epoch [15/30], Step [35/139], Loss: 0.0735\n",
      "Epoch [15/30], Step [36/139], Loss: 0.0953\n",
      "Epoch [15/30], Step [37/139], Loss: 0.1028\n",
      "Epoch [15/30], Step [38/139], Loss: 0.0935\n",
      "Epoch [15/30], Step [39/139], Loss: 0.0893\n",
      "Epoch [15/30], Step [40/139], Loss: 0.0651\n",
      "Epoch [15/30], Step [41/139], Loss: 0.1211\n",
      "Epoch [15/30], Step [42/139], Loss: 0.0911\n",
      "Epoch [15/30], Step [43/139], Loss: 0.0726\n",
      "Epoch [15/30], Step [44/139], Loss: 0.1028\n",
      "Epoch [15/30], Step [45/139], Loss: 0.0872\n",
      "Epoch [15/30], Step [46/139], Loss: 0.0570\n",
      "Epoch [15/30], Step [47/139], Loss: 0.0573\n",
      "Epoch [15/30], Step [48/139], Loss: 0.0835\n",
      "Epoch [15/30], Step [49/139], Loss: 0.0607\n",
      "Epoch [15/30], Step [50/139], Loss: 0.0888\n",
      "Epoch [15/30], Step [51/139], Loss: 0.0850\n",
      "Epoch [15/30], Step [52/139], Loss: 0.0922\n",
      "Epoch [15/30], Step [53/139], Loss: 0.0801\n",
      "Epoch [15/30], Step [54/139], Loss: 0.0555\n",
      "Epoch [15/30], Step [55/139], Loss: 0.0825\n",
      "Epoch [15/30], Step [56/139], Loss: 0.0678\n",
      "Epoch [15/30], Step [57/139], Loss: 0.0733\n",
      "Epoch [15/30], Step [58/139], Loss: 0.0695\n",
      "Epoch [15/30], Step [59/139], Loss: 0.0805\n",
      "Epoch [15/30], Step [60/139], Loss: 0.0709\n",
      "Epoch [15/30], Step [61/139], Loss: 0.0870\n",
      "Epoch [15/30], Step [62/139], Loss: 0.0994\n",
      "Epoch [15/30], Step [63/139], Loss: 0.0691\n",
      "Epoch [15/30], Step [64/139], Loss: 0.0907\n",
      "Epoch [15/30], Step [65/139], Loss: 0.0843\n",
      "Epoch [15/30], Step [66/139], Loss: 0.1282\n",
      "Epoch [15/30], Step [67/139], Loss: 0.0969\n",
      "Epoch [15/30], Step [68/139], Loss: 0.0744\n",
      "Epoch [15/30], Step [69/139], Loss: 0.0504\n",
      "Epoch [15/30], Step [70/139], Loss: 0.0983\n",
      "Epoch [15/30], Step [71/139], Loss: 0.0782\n",
      "Epoch [15/30], Step [72/139], Loss: 0.0984\n",
      "Epoch [15/30], Step [73/139], Loss: 0.0684\n",
      "Epoch [15/30], Step [74/139], Loss: 0.0959\n",
      "Epoch [15/30], Step [75/139], Loss: 0.0667\n",
      "Epoch [15/30], Step [76/139], Loss: 0.0900\n",
      "Epoch [15/30], Step [77/139], Loss: 0.0536\n",
      "Epoch [15/30], Step [78/139], Loss: 0.1159\n",
      "Epoch [15/30], Step [79/139], Loss: 0.0776\n",
      "Epoch [15/30], Step [80/139], Loss: 0.0775\n",
      "Epoch [15/30], Step [81/139], Loss: 0.0802\n",
      "Epoch [15/30], Step [82/139], Loss: 0.0657\n",
      "Epoch [15/30], Step [83/139], Loss: 0.0812\n",
      "Epoch [15/30], Step [84/139], Loss: 0.0972\n",
      "Epoch [15/30], Step [85/139], Loss: 0.0694\n",
      "Epoch [15/30], Step [86/139], Loss: 0.0770\n",
      "Epoch [15/30], Step [87/139], Loss: 0.0719\n",
      "Epoch [15/30], Step [88/139], Loss: 0.0863\n",
      "Epoch [15/30], Step [89/139], Loss: 0.0878\n",
      "Epoch [15/30], Step [90/139], Loss: 0.1039\n",
      "Epoch [15/30], Step [91/139], Loss: 0.0970\n",
      "Epoch [15/30], Step [92/139], Loss: 0.0756\n",
      "Epoch [15/30], Step [93/139], Loss: 0.0729\n",
      "Epoch [15/30], Step [94/139], Loss: 0.0872\n",
      "Epoch [15/30], Step [95/139], Loss: 0.0889\n",
      "Epoch [15/30], Step [96/139], Loss: 0.0822\n",
      "Epoch [15/30], Step [97/139], Loss: 0.0705\n",
      "Epoch [15/30], Step [98/139], Loss: 0.1047\n",
      "Epoch [15/30], Step [99/139], Loss: 0.0865\n",
      "Epoch [15/30], Step [100/139], Loss: 0.1100\n",
      "Epoch [15/30], Step [101/139], Loss: 0.0763\n",
      "Epoch [15/30], Step [102/139], Loss: 0.0968\n",
      "Epoch [15/30], Step [103/139], Loss: 0.0657\n",
      "Epoch [15/30], Step [104/139], Loss: 0.0897\n",
      "Epoch [15/30], Step [105/139], Loss: 0.0696\n",
      "Epoch [15/30], Step [106/139], Loss: 0.1061\n",
      "Epoch [15/30], Step [107/139], Loss: 0.1043\n",
      "Epoch [15/30], Step [108/139], Loss: 0.0814\n",
      "Epoch [15/30], Step [109/139], Loss: 0.1020\n",
      "Epoch [15/30], Step [110/139], Loss: 0.0924\n",
      "Epoch [15/30], Step [111/139], Loss: 0.0681\n",
      "Epoch [15/30], Step [112/139], Loss: 0.0882\n",
      "Epoch [15/30], Step [113/139], Loss: 0.0814\n",
      "Epoch [15/30], Step [114/139], Loss: 0.1000\n",
      "Epoch [15/30], Step [115/139], Loss: 0.0884\n",
      "Epoch [15/30], Step [116/139], Loss: 0.0688\n",
      "Epoch [15/30], Step [117/139], Loss: 0.0584\n",
      "Epoch [15/30], Step [118/139], Loss: 0.0823\n",
      "Epoch [15/30], Step [119/139], Loss: 0.0843\n",
      "Epoch [15/30], Step [120/139], Loss: 0.0717\n",
      "Epoch [15/30], Step [121/139], Loss: 0.0871\n",
      "Epoch [15/30], Step [122/139], Loss: 0.0784\n",
      "Epoch [15/30], Step [123/139], Loss: 0.0969\n",
      "Epoch [15/30], Step [124/139], Loss: 0.0962\n",
      "Epoch [15/30], Step [125/139], Loss: 0.0910\n",
      "Epoch [15/30], Step [126/139], Loss: 0.0846\n",
      "Epoch [15/30], Step [127/139], Loss: 0.0831\n",
      "Epoch [15/30], Step [128/139], Loss: 0.1062\n",
      "Epoch [15/30], Step [129/139], Loss: 0.0755\n",
      "Epoch [15/30], Step [130/139], Loss: 0.0873\n",
      "Epoch [15/30], Step [131/139], Loss: 0.0987\n",
      "Epoch [15/30], Step [132/139], Loss: 0.0904\n",
      "Epoch [15/30], Step [133/139], Loss: 0.0621\n",
      "Epoch [15/30], Step [134/139], Loss: 0.1053\n",
      "Epoch [15/30], Step [135/139], Loss: 0.0917\n",
      "Epoch [15/30], Step [136/139], Loss: 0.0702\n",
      "Epoch [15/30], Step [137/139], Loss: 0.0923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30], Step [138/139], Loss: 0.0711\n",
      "Epoch [15/30], Step [139/139], Loss: 0.1107\n",
      "Start validation #15\n",
      "Validation #15  Average Loss: 0.0822\n",
      "Best performance at epoch: 15\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [16/30], Step [1/139], Loss: 0.0657\n",
      "Epoch [16/30], Step [2/139], Loss: 0.0868\n",
      "Epoch [16/30], Step [3/139], Loss: 0.0781\n",
      "Epoch [16/30], Step [4/139], Loss: 0.1119\n",
      "Epoch [16/30], Step [5/139], Loss: 0.1232\n",
      "Epoch [16/30], Step [6/139], Loss: 0.0787\n",
      "Epoch [16/30], Step [7/139], Loss: 0.0713\n",
      "Epoch [16/30], Step [8/139], Loss: 0.1120\n",
      "Epoch [16/30], Step [9/139], Loss: 0.0590\n",
      "Epoch [16/30], Step [10/139], Loss: 0.0975\n",
      "Epoch [16/30], Step [11/139], Loss: 0.0627\n",
      "Epoch [16/30], Step [12/139], Loss: 0.0786\n",
      "Epoch [16/30], Step [13/139], Loss: 0.1005\n",
      "Epoch [16/30], Step [14/139], Loss: 0.0777\n",
      "Epoch [16/30], Step [15/139], Loss: 0.0628\n",
      "Epoch [16/30], Step [16/139], Loss: 0.0714\n",
      "Epoch [16/30], Step [17/139], Loss: 0.0824\n",
      "Epoch [16/30], Step [18/139], Loss: 0.0774\n",
      "Epoch [16/30], Step [19/139], Loss: 0.0807\n",
      "Epoch [16/30], Step [20/139], Loss: 0.0838\n",
      "Epoch [16/30], Step [21/139], Loss: 0.0764\n",
      "Epoch [16/30], Step [22/139], Loss: 0.0716\n",
      "Epoch [16/30], Step [23/139], Loss: 0.0960\n",
      "Epoch [16/30], Step [24/139], Loss: 0.0812\n",
      "Epoch [16/30], Step [25/139], Loss: 0.0501\n",
      "Epoch [16/30], Step [26/139], Loss: 0.0661\n",
      "Epoch [16/30], Step [27/139], Loss: 0.0546\n",
      "Epoch [16/30], Step [28/139], Loss: 0.1090\n",
      "Epoch [16/30], Step [29/139], Loss: 0.0768\n",
      "Epoch [16/30], Step [30/139], Loss: 0.1062\n",
      "Epoch [16/30], Step [31/139], Loss: 0.0885\n",
      "Epoch [16/30], Step [32/139], Loss: 0.0944\n",
      "Epoch [16/30], Step [33/139], Loss: 0.0597\n",
      "Epoch [16/30], Step [34/139], Loss: 0.0722\n",
      "Epoch [16/30], Step [35/139], Loss: 0.0690\n",
      "Epoch [16/30], Step [36/139], Loss: 0.0701\n",
      "Epoch [16/30], Step [37/139], Loss: 0.0740\n",
      "Epoch [16/30], Step [38/139], Loss: 0.0716\n",
      "Epoch [16/30], Step [39/139], Loss: 0.0996\n",
      "Epoch [16/30], Step [40/139], Loss: 0.1001\n",
      "Epoch [16/30], Step [41/139], Loss: 0.0984\n",
      "Epoch [16/30], Step [42/139], Loss: 0.0852\n",
      "Epoch [16/30], Step [43/139], Loss: 0.0907\n",
      "Epoch [16/30], Step [44/139], Loss: 0.0707\n",
      "Epoch [16/30], Step [45/139], Loss: 0.1098\n",
      "Epoch [16/30], Step [46/139], Loss: 0.0885\n",
      "Epoch [16/30], Step [47/139], Loss: 0.0822\n",
      "Epoch [16/30], Step [48/139], Loss: 0.0676\n",
      "Epoch [16/30], Step [49/139], Loss: 0.0667\n",
      "Epoch [16/30], Step [50/139], Loss: 0.0836\n",
      "Epoch [16/30], Step [51/139], Loss: 0.0790\n",
      "Epoch [16/30], Step [52/139], Loss: 0.1087\n",
      "Epoch [16/30], Step [53/139], Loss: 0.0785\n",
      "Epoch [16/30], Step [54/139], Loss: 0.0981\n",
      "Epoch [16/30], Step [55/139], Loss: 0.0800\n",
      "Epoch [16/30], Step [56/139], Loss: 0.0804\n",
      "Epoch [16/30], Step [57/139], Loss: 0.0916\n",
      "Epoch [16/30], Step [58/139], Loss: 0.0676\n",
      "Epoch [16/30], Step [59/139], Loss: 0.0740\n",
      "Epoch [16/30], Step [60/139], Loss: 0.0946\n",
      "Epoch [16/30], Step [61/139], Loss: 0.0800\n",
      "Epoch [16/30], Step [62/139], Loss: 0.0784\n",
      "Epoch [16/30], Step [63/139], Loss: 0.0841\n",
      "Epoch [16/30], Step [64/139], Loss: 0.1229\n",
      "Epoch [16/30], Step [65/139], Loss: 0.0759\n",
      "Epoch [16/30], Step [66/139], Loss: 0.0672\n",
      "Epoch [16/30], Step [67/139], Loss: 0.0844\n",
      "Epoch [16/30], Step [68/139], Loss: 0.0840\n",
      "Epoch [16/30], Step [69/139], Loss: 0.0646\n",
      "Epoch [16/30], Step [70/139], Loss: 0.0695\n",
      "Epoch [16/30], Step [71/139], Loss: 0.0580\n",
      "Epoch [16/30], Step [72/139], Loss: 0.0857\n",
      "Epoch [16/30], Step [73/139], Loss: 0.0943\n",
      "Epoch [16/30], Step [74/139], Loss: 0.0743\n",
      "Epoch [16/30], Step [75/139], Loss: 0.0777\n",
      "Epoch [16/30], Step [76/139], Loss: 0.0739\n",
      "Epoch [16/30], Step [77/139], Loss: 0.0653\n",
      "Epoch [16/30], Step [78/139], Loss: 0.0641\n",
      "Epoch [16/30], Step [79/139], Loss: 0.0848\n",
      "Epoch [16/30], Step [80/139], Loss: 0.0901\n",
      "Epoch [16/30], Step [81/139], Loss: 0.0831\n",
      "Epoch [16/30], Step [82/139], Loss: 0.0701\n",
      "Epoch [16/30], Step [83/139], Loss: 0.0825\n",
      "Epoch [16/30], Step [84/139], Loss: 0.0879\n",
      "Epoch [16/30], Step [85/139], Loss: 0.0647\n",
      "Epoch [16/30], Step [86/139], Loss: 0.0774\n",
      "Epoch [16/30], Step [87/139], Loss: 0.0822\n",
      "Epoch [16/30], Step [88/139], Loss: 0.0591\n",
      "Epoch [16/30], Step [89/139], Loss: 0.0694\n",
      "Epoch [16/30], Step [90/139], Loss: 0.0807\n",
      "Epoch [16/30], Step [91/139], Loss: 0.1662\n",
      "Epoch [16/30], Step [92/139], Loss: 0.0744\n",
      "Epoch [16/30], Step [93/139], Loss: 0.0716\n",
      "Epoch [16/30], Step [94/139], Loss: 0.0998\n",
      "Epoch [16/30], Step [95/139], Loss: 0.0823\n",
      "Epoch [16/30], Step [96/139], Loss: 0.0725\n",
      "Epoch [16/30], Step [97/139], Loss: 0.0799\n",
      "Epoch [16/30], Step [98/139], Loss: 0.0913\n",
      "Epoch [16/30], Step [99/139], Loss: 0.0852\n",
      "Epoch [16/30], Step [100/139], Loss: 0.1273\n",
      "Epoch [16/30], Step [101/139], Loss: 0.0749\n",
      "Epoch [16/30], Step [102/139], Loss: 0.0904\n",
      "Epoch [16/30], Step [103/139], Loss: 0.0912\n",
      "Epoch [16/30], Step [104/139], Loss: 0.0813\n",
      "Epoch [16/30], Step [105/139], Loss: 0.0780\n",
      "Epoch [16/30], Step [106/139], Loss: 0.0990\n",
      "Epoch [16/30], Step [107/139], Loss: 0.0860\n",
      "Epoch [16/30], Step [108/139], Loss: 0.0872\n",
      "Epoch [16/30], Step [109/139], Loss: 0.0586\n",
      "Epoch [16/30], Step [110/139], Loss: 0.0949\n",
      "Epoch [16/30], Step [111/139], Loss: 0.1019\n",
      "Epoch [16/30], Step [112/139], Loss: 0.0936\n",
      "Epoch [16/30], Step [113/139], Loss: 0.0759\n",
      "Epoch [16/30], Step [114/139], Loss: 0.0757\n",
      "Epoch [16/30], Step [115/139], Loss: 0.0894\n",
      "Epoch [16/30], Step [116/139], Loss: 0.0965\n",
      "Epoch [16/30], Step [117/139], Loss: 0.0870\n",
      "Epoch [16/30], Step [118/139], Loss: 0.0888\n",
      "Epoch [16/30], Step [119/139], Loss: 0.0832\n",
      "Epoch [16/30], Step [120/139], Loss: 0.0659\n",
      "Epoch [16/30], Step [121/139], Loss: 0.0793\n",
      "Epoch [16/30], Step [122/139], Loss: 0.0928\n",
      "Epoch [16/30], Step [123/139], Loss: 0.1031\n",
      "Epoch [16/30], Step [124/139], Loss: 0.1009\n",
      "Epoch [16/30], Step [125/139], Loss: 0.0954\n",
      "Epoch [16/30], Step [126/139], Loss: 0.0913\n",
      "Epoch [16/30], Step [127/139], Loss: 0.0784\n",
      "Epoch [16/30], Step [128/139], Loss: 0.0667\n",
      "Epoch [16/30], Step [129/139], Loss: 0.0701\n",
      "Epoch [16/30], Step [130/139], Loss: 0.1107\n",
      "Epoch [16/30], Step [131/139], Loss: 0.0818\n",
      "Epoch [16/30], Step [132/139], Loss: 0.0874\n",
      "Epoch [16/30], Step [133/139], Loss: 0.0955\n",
      "Epoch [16/30], Step [134/139], Loss: 0.0673\n",
      "Epoch [16/30], Step [135/139], Loss: 0.0901\n",
      "Epoch [16/30], Step [136/139], Loss: 0.0817\n",
      "Epoch [16/30], Step [137/139], Loss: 0.0877\n",
      "Epoch [16/30], Step [138/139], Loss: 0.0769\n",
      "Epoch [16/30], Step [139/139], Loss: 0.0974\n",
      "Start validation #16\n",
      "Validation #16  Average Loss: 0.0817\n",
      "Best performance at epoch: 16\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [17/30], Step [1/139], Loss: 0.0630\n",
      "Epoch [17/30], Step [2/139], Loss: 0.0791\n",
      "Epoch [17/30], Step [3/139], Loss: 0.0887\n",
      "Epoch [17/30], Step [4/139], Loss: 0.0589\n",
      "Epoch [17/30], Step [5/139], Loss: 0.0707\n",
      "Epoch [17/30], Step [6/139], Loss: 0.0936\n",
      "Epoch [17/30], Step [7/139], Loss: 0.0492\n",
      "Epoch [17/30], Step [8/139], Loss: 0.0888\n",
      "Epoch [17/30], Step [9/139], Loss: 0.1000\n",
      "Epoch [17/30], Step [10/139], Loss: 0.0933\n",
      "Epoch [17/30], Step [11/139], Loss: 0.1000\n",
      "Epoch [17/30], Step [12/139], Loss: 0.0937\n",
      "Epoch [17/30], Step [13/139], Loss: 0.0656\n",
      "Epoch [17/30], Step [14/139], Loss: 0.0953\n",
      "Epoch [17/30], Step [15/139], Loss: 0.0759\n",
      "Epoch [17/30], Step [16/139], Loss: 0.1102\n",
      "Epoch [17/30], Step [17/139], Loss: 0.0706\n",
      "Epoch [17/30], Step [18/139], Loss: 0.0646\n",
      "Epoch [17/30], Step [19/139], Loss: 0.0830\n",
      "Epoch [17/30], Step [20/139], Loss: 0.0685\n",
      "Epoch [17/30], Step [21/139], Loss: 0.0649\n",
      "Epoch [17/30], Step [22/139], Loss: 0.0789\n",
      "Epoch [17/30], Step [23/139], Loss: 0.1523\n",
      "Epoch [17/30], Step [24/139], Loss: 0.0781\n",
      "Epoch [17/30], Step [25/139], Loss: 0.0836\n",
      "Epoch [17/30], Step [26/139], Loss: 0.0924\n",
      "Epoch [17/30], Step [27/139], Loss: 0.0678\n",
      "Epoch [17/30], Step [28/139], Loss: 0.0810\n",
      "Epoch [17/30], Step [29/139], Loss: 0.1099\n",
      "Epoch [17/30], Step [30/139], Loss: 0.0741\n",
      "Epoch [17/30], Step [31/139], Loss: 0.1028\n",
      "Epoch [17/30], Step [32/139], Loss: 0.0824\n",
      "Epoch [17/30], Step [33/139], Loss: 0.0810\n",
      "Epoch [17/30], Step [34/139], Loss: 0.0669\n",
      "Epoch [17/30], Step [35/139], Loss: 0.0766\n",
      "Epoch [17/30], Step [36/139], Loss: 0.0732\n",
      "Epoch [17/30], Step [37/139], Loss: 0.0870\n",
      "Epoch [17/30], Step [38/139], Loss: 0.0563\n",
      "Epoch [17/30], Step [39/139], Loss: 0.0936\n",
      "Epoch [17/30], Step [40/139], Loss: 0.0790\n",
      "Epoch [17/30], Step [41/139], Loss: 0.0730\n",
      "Epoch [17/30], Step [42/139], Loss: 0.0621\n",
      "Epoch [17/30], Step [43/139], Loss: 0.0877\n",
      "Epoch [17/30], Step [44/139], Loss: 0.0777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/30], Step [45/139], Loss: 0.0978\n",
      "Epoch [17/30], Step [46/139], Loss: 0.0797\n",
      "Epoch [17/30], Step [47/139], Loss: 0.0790\n",
      "Epoch [17/30], Step [48/139], Loss: 0.0845\n",
      "Epoch [17/30], Step [49/139], Loss: 0.0745\n",
      "Epoch [17/30], Step [50/139], Loss: 0.1010\n",
      "Epoch [17/30], Step [51/139], Loss: 0.1010\n",
      "Epoch [17/30], Step [52/139], Loss: 0.0766\n",
      "Epoch [17/30], Step [53/139], Loss: 0.0758\n",
      "Epoch [17/30], Step [54/139], Loss: 0.0721\n",
      "Epoch [17/30], Step [55/139], Loss: 0.0713\n",
      "Epoch [17/30], Step [56/139], Loss: 0.0722\n",
      "Epoch [17/30], Step [57/139], Loss: 0.1007\n",
      "Epoch [17/30], Step [58/139], Loss: 0.1069\n",
      "Epoch [17/30], Step [59/139], Loss: 0.0783\n",
      "Epoch [17/30], Step [60/139], Loss: 0.0749\n",
      "Epoch [17/30], Step [61/139], Loss: 0.0770\n",
      "Epoch [17/30], Step [62/139], Loss: 0.0953\n",
      "Epoch [17/30], Step [63/139], Loss: 0.1099\n",
      "Epoch [17/30], Step [64/139], Loss: 0.0863\n",
      "Epoch [17/30], Step [65/139], Loss: 0.0819\n",
      "Epoch [17/30], Step [66/139], Loss: 0.1028\n",
      "Epoch [17/30], Step [67/139], Loss: 0.0834\n",
      "Epoch [17/30], Step [68/139], Loss: 0.1014\n",
      "Epoch [17/30], Step [69/139], Loss: 0.0733\n",
      "Epoch [17/30], Step [70/139], Loss: 0.0850\n",
      "Epoch [17/30], Step [71/139], Loss: 0.0698\n",
      "Epoch [17/30], Step [72/139], Loss: 0.0930\n",
      "Epoch [17/30], Step [73/139], Loss: 0.0663\n",
      "Epoch [17/30], Step [74/139], Loss: 0.0940\n",
      "Epoch [17/30], Step [75/139], Loss: 0.0868\n",
      "Epoch [17/30], Step [76/139], Loss: 0.0868\n",
      "Epoch [17/30], Step [77/139], Loss: 0.0677\n",
      "Epoch [17/30], Step [78/139], Loss: 0.0664\n",
      "Epoch [17/30], Step [79/139], Loss: 0.1142\n",
      "Epoch [17/30], Step [80/139], Loss: 0.0619\n",
      "Epoch [17/30], Step [81/139], Loss: 0.0679\n",
      "Epoch [17/30], Step [82/139], Loss: 0.0710\n",
      "Epoch [17/30], Step [83/139], Loss: 0.0747\n",
      "Epoch [17/30], Step [84/139], Loss: 0.0703\n",
      "Epoch [17/30], Step [85/139], Loss: 0.0596\n",
      "Epoch [17/30], Step [86/139], Loss: 0.0825\n",
      "Epoch [17/30], Step [87/139], Loss: 0.0957\n",
      "Epoch [17/30], Step [88/139], Loss: 0.0638\n",
      "Epoch [17/30], Step [89/139], Loss: 0.0643\n",
      "Epoch [17/30], Step [90/139], Loss: 0.1010\n",
      "Epoch [17/30], Step [91/139], Loss: 0.0752\n",
      "Epoch [17/30], Step [92/139], Loss: 0.0808\n",
      "Epoch [17/30], Step [93/139], Loss: 0.0918\n",
      "Epoch [17/30], Step [94/139], Loss: 0.0914\n",
      "Epoch [17/30], Step [95/139], Loss: 0.0582\n",
      "Epoch [17/30], Step [96/139], Loss: 0.1018\n",
      "Epoch [17/30], Step [97/139], Loss: 0.1232\n",
      "Epoch [17/30], Step [98/139], Loss: 0.0844\n",
      "Epoch [17/30], Step [99/139], Loss: 0.0808\n",
      "Epoch [17/30], Step [100/139], Loss: 0.1035\n",
      "Epoch [17/30], Step [101/139], Loss: 0.0679\n",
      "Epoch [17/30], Step [102/139], Loss: 0.0833\n",
      "Epoch [17/30], Step [103/139], Loss: 0.1026\n",
      "Epoch [17/30], Step [104/139], Loss: 0.0785\n",
      "Epoch [17/30], Step [105/139], Loss: 0.0660\n",
      "Epoch [17/30], Step [106/139], Loss: 0.0647\n",
      "Epoch [17/30], Step [107/139], Loss: 0.0862\n",
      "Epoch [17/30], Step [108/139], Loss: 0.0906\n",
      "Epoch [17/30], Step [109/139], Loss: 0.0735\n",
      "Epoch [17/30], Step [110/139], Loss: 0.0888\n",
      "Epoch [17/30], Step [111/139], Loss: 0.0731\n",
      "Epoch [17/30], Step [112/139], Loss: 0.0797\n",
      "Epoch [17/30], Step [113/139], Loss: 0.0626\n",
      "Epoch [17/30], Step [114/139], Loss: 0.0704\n",
      "Epoch [17/30], Step [115/139], Loss: 0.0683\n",
      "Epoch [17/30], Step [116/139], Loss: 0.1073\n",
      "Epoch [17/30], Step [117/139], Loss: 0.0791\n",
      "Epoch [17/30], Step [118/139], Loss: 0.1060\n",
      "Epoch [17/30], Step [119/139], Loss: 0.0773\n",
      "Epoch [17/30], Step [120/139], Loss: 0.0727\n",
      "Epoch [17/30], Step [121/139], Loss: 0.0686\n",
      "Epoch [17/30], Step [122/139], Loss: 0.1173\n",
      "Epoch [17/30], Step [123/139], Loss: 0.0775\n",
      "Epoch [17/30], Step [124/139], Loss: 0.0821\n",
      "Epoch [17/30], Step [125/139], Loss: 0.0761\n",
      "Epoch [17/30], Step [126/139], Loss: 0.0803\n",
      "Epoch [17/30], Step [127/139], Loss: 0.0968\n",
      "Epoch [17/30], Step [128/139], Loss: 0.0698\n",
      "Epoch [17/30], Step [129/139], Loss: 0.1152\n",
      "Epoch [17/30], Step [130/139], Loss: 0.0928\n",
      "Epoch [17/30], Step [131/139], Loss: 0.0833\n",
      "Epoch [17/30], Step [132/139], Loss: 0.0790\n",
      "Epoch [17/30], Step [133/139], Loss: 0.0775\n",
      "Epoch [17/30], Step [134/139], Loss: 0.0561\n",
      "Epoch [17/30], Step [135/139], Loss: 0.0813\n",
      "Epoch [17/30], Step [136/139], Loss: 0.0669\n",
      "Epoch [17/30], Step [137/139], Loss: 0.0721\n",
      "Epoch [17/30], Step [138/139], Loss: 0.0789\n",
      "Epoch [17/30], Step [139/139], Loss: 0.0784\n",
      "Start validation #17\n",
      "Validation #17  Average Loss: 0.0800\n",
      "Best performance at epoch: 17\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [18/30], Step [1/139], Loss: 0.0585\n",
      "Epoch [18/30], Step [2/139], Loss: 0.0697\n",
      "Epoch [18/30], Step [3/139], Loss: 0.0807\n",
      "Epoch [18/30], Step [4/139], Loss: 0.0604\n",
      "Epoch [18/30], Step [5/139], Loss: 0.0884\n",
      "Epoch [18/30], Step [6/139], Loss: 0.0935\n",
      "Epoch [18/30], Step [7/139], Loss: 0.0713\n",
      "Epoch [18/30], Step [8/139], Loss: 0.0851\n",
      "Epoch [18/30], Step [9/139], Loss: 0.0802\n",
      "Epoch [18/30], Step [10/139], Loss: 0.1037\n",
      "Epoch [18/30], Step [11/139], Loss: 0.0980\n",
      "Epoch [18/30], Step [12/139], Loss: 0.0829\n",
      "Epoch [18/30], Step [13/139], Loss: 0.0747\n",
      "Epoch [18/30], Step [14/139], Loss: 0.0863\n",
      "Epoch [18/30], Step [15/139], Loss: 0.0848\n",
      "Epoch [18/30], Step [16/139], Loss: 0.1000\n",
      "Epoch [18/30], Step [17/139], Loss: 0.0799\n",
      "Epoch [18/30], Step [18/139], Loss: 0.0887\n",
      "Epoch [18/30], Step [19/139], Loss: 0.1040\n",
      "Epoch [18/30], Step [20/139], Loss: 0.0689\n",
      "Epoch [18/30], Step [21/139], Loss: 0.0790\n",
      "Epoch [18/30], Step [22/139], Loss: 0.0696\n",
      "Epoch [18/30], Step [23/139], Loss: 0.0897\n",
      "Epoch [18/30], Step [24/139], Loss: 0.0791\n",
      "Epoch [18/30], Step [25/139], Loss: 0.0776\n",
      "Epoch [18/30], Step [26/139], Loss: 0.0769\n",
      "Epoch [18/30], Step [27/139], Loss: 0.0723\n",
      "Epoch [18/30], Step [28/139], Loss: 0.0737\n",
      "Epoch [18/30], Step [29/139], Loss: 0.0590\n",
      "Epoch [18/30], Step [30/139], Loss: 0.0712\n",
      "Epoch [18/30], Step [31/139], Loss: 0.0893\n",
      "Epoch [18/30], Step [32/139], Loss: 0.1002\n",
      "Epoch [18/30], Step [33/139], Loss: 0.0819\n",
      "Epoch [18/30], Step [34/139], Loss: 0.0607\n",
      "Epoch [18/30], Step [35/139], Loss: 0.0631\n",
      "Epoch [18/30], Step [36/139], Loss: 0.1185\n",
      "Epoch [18/30], Step [37/139], Loss: 0.0891\n",
      "Epoch [18/30], Step [38/139], Loss: 0.0620\n",
      "Epoch [18/30], Step [39/139], Loss: 0.0554\n",
      "Epoch [18/30], Step [40/139], Loss: 0.0743\n",
      "Epoch [18/30], Step [41/139], Loss: 0.0633\n",
      "Epoch [18/30], Step [42/139], Loss: 0.0890\n",
      "Epoch [18/30], Step [43/139], Loss: 0.0910\n",
      "Epoch [18/30], Step [44/139], Loss: 0.0850\n",
      "Epoch [18/30], Step [45/139], Loss: 0.0853\n",
      "Epoch [18/30], Step [46/139], Loss: 0.0853\n",
      "Epoch [18/30], Step [47/139], Loss: 0.0660\n",
      "Epoch [18/30], Step [48/139], Loss: 0.0862\n",
      "Epoch [18/30], Step [49/139], Loss: 0.0706\n",
      "Epoch [18/30], Step [50/139], Loss: 0.0816\n",
      "Epoch [18/30], Step [51/139], Loss: 0.1072\n",
      "Epoch [18/30], Step [52/139], Loss: 0.0967\n",
      "Epoch [18/30], Step [53/139], Loss: 0.0824\n",
      "Epoch [18/30], Step [54/139], Loss: 0.0780\n",
      "Epoch [18/30], Step [55/139], Loss: 0.0852\n",
      "Epoch [18/30], Step [56/139], Loss: 0.0655\n",
      "Epoch [18/30], Step [57/139], Loss: 0.0835\n",
      "Epoch [18/30], Step [58/139], Loss: 0.0772\n",
      "Epoch [18/30], Step [59/139], Loss: 0.0802\n",
      "Epoch [18/30], Step [60/139], Loss: 0.0750\n",
      "Epoch [18/30], Step [61/139], Loss: 0.0612\n",
      "Epoch [18/30], Step [62/139], Loss: 0.0976\n",
      "Epoch [18/30], Step [63/139], Loss: 0.1050\n",
      "Epoch [18/30], Step [64/139], Loss: 0.0540\n",
      "Epoch [18/30], Step [65/139], Loss: 0.0661\n",
      "Epoch [18/30], Step [66/139], Loss: 0.0847\n",
      "Epoch [18/30], Step [67/139], Loss: 0.0683\n",
      "Epoch [18/30], Step [68/139], Loss: 0.0817\n",
      "Epoch [18/30], Step [69/139], Loss: 0.0888\n",
      "Epoch [18/30], Step [70/139], Loss: 0.1003\n",
      "Epoch [18/30], Step [71/139], Loss: 0.1055\n",
      "Epoch [18/30], Step [72/139], Loss: 0.0791\n",
      "Epoch [18/30], Step [73/139], Loss: 0.0824\n",
      "Epoch [18/30], Step [74/139], Loss: 0.1007\n",
      "Epoch [18/30], Step [75/139], Loss: 0.0840\n",
      "Epoch [18/30], Step [76/139], Loss: 0.0738\n",
      "Epoch [18/30], Step [77/139], Loss: 0.0574\n",
      "Epoch [18/30], Step [78/139], Loss: 0.0931\n",
      "Epoch [18/30], Step [79/139], Loss: 0.0650\n",
      "Epoch [18/30], Step [80/139], Loss: 0.1031\n",
      "Epoch [18/30], Step [81/139], Loss: 0.0854\n",
      "Epoch [18/30], Step [82/139], Loss: 0.1002\n",
      "Epoch [18/30], Step [83/139], Loss: 0.0478\n",
      "Epoch [18/30], Step [84/139], Loss: 0.1289\n",
      "Epoch [18/30], Step [85/139], Loss: 0.0692\n",
      "Epoch [18/30], Step [86/139], Loss: 0.0841\n",
      "Epoch [18/30], Step [87/139], Loss: 0.0719\n",
      "Epoch [18/30], Step [88/139], Loss: 0.0845\n",
      "Epoch [18/30], Step [89/139], Loss: 0.0889\n",
      "Epoch [18/30], Step [90/139], Loss: 0.0624\n",
      "Epoch [18/30], Step [91/139], Loss: 0.0680\n",
      "Epoch [18/30], Step [92/139], Loss: 0.0748\n",
      "Epoch [18/30], Step [93/139], Loss: 0.0944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/30], Step [94/139], Loss: 0.0918\n",
      "Epoch [18/30], Step [95/139], Loss: 0.0542\n",
      "Epoch [18/30], Step [96/139], Loss: 0.0691\n",
      "Epoch [18/30], Step [97/139], Loss: 0.0631\n",
      "Epoch [18/30], Step [98/139], Loss: 0.0716\n",
      "Epoch [18/30], Step [99/139], Loss: 0.1122\n",
      "Epoch [18/30], Step [100/139], Loss: 0.1135\n",
      "Epoch [18/30], Step [101/139], Loss: 0.0906\n",
      "Epoch [18/30], Step [102/139], Loss: 0.0770\n",
      "Epoch [18/30], Step [103/139], Loss: 0.0700\n",
      "Epoch [18/30], Step [104/139], Loss: 0.0855\n",
      "Epoch [18/30], Step [105/139], Loss: 0.0942\n",
      "Epoch [18/30], Step [106/139], Loss: 0.0774\n",
      "Epoch [18/30], Step [107/139], Loss: 0.0657\n",
      "Epoch [18/30], Step [108/139], Loss: 0.0771\n",
      "Epoch [18/30], Step [109/139], Loss: 0.0687\n",
      "Epoch [18/30], Step [110/139], Loss: 0.0780\n",
      "Epoch [18/30], Step [111/139], Loss: 0.0707\n",
      "Epoch [18/30], Step [112/139], Loss: 0.1041\n",
      "Epoch [18/30], Step [113/139], Loss: 0.0642\n",
      "Epoch [18/30], Step [114/139], Loss: 0.0911\n",
      "Epoch [18/30], Step [115/139], Loss: 0.1148\n",
      "Epoch [18/30], Step [116/139], Loss: 0.1057\n",
      "Epoch [18/30], Step [117/139], Loss: 0.0827\n",
      "Epoch [18/30], Step [118/139], Loss: 0.0808\n",
      "Epoch [18/30], Step [119/139], Loss: 0.0829\n",
      "Epoch [18/30], Step [120/139], Loss: 0.0881\n",
      "Epoch [18/30], Step [121/139], Loss: 0.1283\n",
      "Epoch [18/30], Step [122/139], Loss: 0.0750\n",
      "Epoch [18/30], Step [123/139], Loss: 0.0764\n",
      "Epoch [18/30], Step [124/139], Loss: 0.0697\n",
      "Epoch [18/30], Step [125/139], Loss: 0.0650\n",
      "Epoch [18/30], Step [126/139], Loss: 0.0818\n",
      "Epoch [18/30], Step [127/139], Loss: 0.0712\n",
      "Epoch [18/30], Step [128/139], Loss: 0.0822\n",
      "Epoch [18/30], Step [129/139], Loss: 0.0854\n",
      "Epoch [18/30], Step [130/139], Loss: 0.0753\n",
      "Epoch [18/30], Step [131/139], Loss: 0.0636\n",
      "Epoch [18/30], Step [132/139], Loss: 0.0838\n",
      "Epoch [18/30], Step [133/139], Loss: 0.0858\n",
      "Epoch [18/30], Step [134/139], Loss: 0.0856\n",
      "Epoch [18/30], Step [135/139], Loss: 0.0950\n",
      "Epoch [18/30], Step [136/139], Loss: 0.0815\n",
      "Epoch [18/30], Step [137/139], Loss: 0.0784\n",
      "Epoch [18/30], Step [138/139], Loss: 0.0735\n",
      "Epoch [18/30], Step [139/139], Loss: 0.0703\n",
      "Start validation #18\n",
      "Validation #18  Average Loss: 0.0782\n",
      "Best performance at epoch: 18\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [19/30], Step [1/139], Loss: 0.0817\n",
      "Epoch [19/30], Step [2/139], Loss: 0.0811\n",
      "Epoch [19/30], Step [3/139], Loss: 0.0966\n",
      "Epoch [19/30], Step [4/139], Loss: 0.0803\n",
      "Epoch [19/30], Step [5/139], Loss: 0.0697\n",
      "Epoch [19/30], Step [6/139], Loss: 0.0728\n",
      "Epoch [19/30], Step [7/139], Loss: 0.0818\n",
      "Epoch [19/30], Step [8/139], Loss: 0.0885\n",
      "Epoch [19/30], Step [9/139], Loss: 0.0566\n",
      "Epoch [19/30], Step [10/139], Loss: 0.1014\n",
      "Epoch [19/30], Step [11/139], Loss: 0.1099\n",
      "Epoch [19/30], Step [12/139], Loss: 0.0779\n",
      "Epoch [19/30], Step [13/139], Loss: 0.0867\n",
      "Epoch [19/30], Step [14/139], Loss: 0.0717\n",
      "Epoch [19/30], Step [15/139], Loss: 0.0911\n",
      "Epoch [19/30], Step [16/139], Loss: 0.0655\n",
      "Epoch [19/30], Step [17/139], Loss: 0.0903\n",
      "Epoch [19/30], Step [18/139], Loss: 0.1001\n",
      "Epoch [19/30], Step [19/139], Loss: 0.1003\n",
      "Epoch [19/30], Step [20/139], Loss: 0.0901\n",
      "Epoch [19/30], Step [21/139], Loss: 0.0780\n",
      "Epoch [19/30], Step [22/139], Loss: 0.0800\n",
      "Epoch [19/30], Step [23/139], Loss: 0.0886\n",
      "Epoch [19/30], Step [24/139], Loss: 0.0991\n",
      "Epoch [19/30], Step [25/139], Loss: 0.0572\n",
      "Epoch [19/30], Step [26/139], Loss: 0.0626\n",
      "Epoch [19/30], Step [27/139], Loss: 0.1128\n",
      "Epoch [19/30], Step [28/139], Loss: 0.0751\n",
      "Epoch [19/30], Step [29/139], Loss: 0.0923\n",
      "Epoch [19/30], Step [30/139], Loss: 0.0642\n",
      "Epoch [19/30], Step [31/139], Loss: 0.1233\n",
      "Epoch [19/30], Step [32/139], Loss: 0.0959\n",
      "Epoch [19/30], Step [33/139], Loss: 0.0994\n",
      "Epoch [19/30], Step [34/139], Loss: 0.0762\n",
      "Epoch [19/30], Step [35/139], Loss: 0.0751\n",
      "Epoch [19/30], Step [36/139], Loss: 0.0607\n",
      "Epoch [19/30], Step [37/139], Loss: 0.1012\n",
      "Epoch [19/30], Step [38/139], Loss: 0.1027\n",
      "Epoch [19/30], Step [39/139], Loss: 0.0905\n",
      "Epoch [19/30], Step [40/139], Loss: 0.0922\n",
      "Epoch [19/30], Step [41/139], Loss: 0.0566\n",
      "Epoch [19/30], Step [42/139], Loss: 0.0615\n",
      "Epoch [19/30], Step [43/139], Loss: 0.0682\n",
      "Epoch [19/30], Step [44/139], Loss: 0.0670\n",
      "Epoch [19/30], Step [45/139], Loss: 0.0839\n",
      "Epoch [19/30], Step [46/139], Loss: 0.0735\n",
      "Epoch [19/30], Step [47/139], Loss: 0.0644\n",
      "Epoch [19/30], Step [48/139], Loss: 0.0860\n",
      "Epoch [19/30], Step [49/139], Loss: 0.0952\n",
      "Epoch [19/30], Step [50/139], Loss: 0.0561\n",
      "Epoch [19/30], Step [51/139], Loss: 0.0798\n",
      "Epoch [19/30], Step [52/139], Loss: 0.0852\n",
      "Epoch [19/30], Step [53/139], Loss: 0.0513\n",
      "Epoch [19/30], Step [54/139], Loss: 0.0627\n",
      "Epoch [19/30], Step [55/139], Loss: 0.1111\n",
      "Epoch [19/30], Step [56/139], Loss: 0.0765\n",
      "Epoch [19/30], Step [57/139], Loss: 0.0616\n",
      "Epoch [19/30], Step [58/139], Loss: 0.0840\n",
      "Epoch [19/30], Step [59/139], Loss: 0.0741\n",
      "Epoch [19/30], Step [60/139], Loss: 0.0693\n",
      "Epoch [19/30], Step [61/139], Loss: 0.0769\n",
      "Epoch [19/30], Step [62/139], Loss: 0.0599\n",
      "Epoch [19/30], Step [63/139], Loss: 0.0811\n",
      "Epoch [19/30], Step [64/139], Loss: 0.0823\n",
      "Epoch [19/30], Step [65/139], Loss: 0.0821\n",
      "Epoch [19/30], Step [66/139], Loss: 0.1005\n",
      "Epoch [19/30], Step [67/139], Loss: 0.0929\n",
      "Epoch [19/30], Step [68/139], Loss: 0.0633\n",
      "Epoch [19/30], Step [69/139], Loss: 0.0679\n",
      "Epoch [19/30], Step [70/139], Loss: 0.0703\n",
      "Epoch [19/30], Step [71/139], Loss: 0.0722\n",
      "Epoch [19/30], Step [72/139], Loss: 0.0799\n",
      "Epoch [19/30], Step [73/139], Loss: 0.0655\n",
      "Epoch [19/30], Step [74/139], Loss: 0.0682\n",
      "Epoch [19/30], Step [75/139], Loss: 0.0849\n",
      "Epoch [19/30], Step [76/139], Loss: 0.0881\n",
      "Epoch [19/30], Step [77/139], Loss: 0.0808\n",
      "Epoch [19/30], Step [78/139], Loss: 0.0819\n",
      "Epoch [19/30], Step [79/139], Loss: 0.0868\n",
      "Epoch [19/30], Step [80/139], Loss: 0.0646\n",
      "Epoch [19/30], Step [81/139], Loss: 0.0892\n",
      "Epoch [19/30], Step [82/139], Loss: 0.0969\n",
      "Epoch [19/30], Step [83/139], Loss: 0.0817\n",
      "Epoch [19/30], Step [84/139], Loss: 0.0926\n",
      "Epoch [19/30], Step [85/139], Loss: 0.0684\n",
      "Epoch [19/30], Step [86/139], Loss: 0.0756\n",
      "Epoch [19/30], Step [87/139], Loss: 0.0740\n",
      "Epoch [19/30], Step [88/139], Loss: 0.0932\n",
      "Epoch [19/30], Step [89/139], Loss: 0.0932\n",
      "Epoch [19/30], Step [90/139], Loss: 0.0665\n",
      "Epoch [19/30], Step [91/139], Loss: 0.0756\n",
      "Epoch [19/30], Step [92/139], Loss: 0.0709\n",
      "Epoch [19/30], Step [93/139], Loss: 0.0655\n",
      "Epoch [19/30], Step [94/139], Loss: 0.0572\n",
      "Epoch [19/30], Step [95/139], Loss: 0.0875\n",
      "Epoch [19/30], Step [96/139], Loss: 0.0671\n",
      "Epoch [19/30], Step [97/139], Loss: 0.0862\n",
      "Epoch [19/30], Step [98/139], Loss: 0.0753\n",
      "Epoch [19/30], Step [99/139], Loss: 0.0747\n",
      "Epoch [19/30], Step [100/139], Loss: 0.0915\n",
      "Epoch [19/30], Step [101/139], Loss: 0.0873\n",
      "Epoch [19/30], Step [102/139], Loss: 0.0573\n",
      "Epoch [19/30], Step [103/139], Loss: 0.0651\n",
      "Epoch [19/30], Step [104/139], Loss: 0.0915\n",
      "Epoch [19/30], Step [105/139], Loss: 0.0885\n",
      "Epoch [19/30], Step [106/139], Loss: 0.1108\n",
      "Epoch [19/30], Step [107/139], Loss: 0.0918\n",
      "Epoch [19/30], Step [108/139], Loss: 0.0926\n",
      "Epoch [19/30], Step [109/139], Loss: 0.0787\n",
      "Epoch [19/30], Step [110/139], Loss: 0.0736\n",
      "Epoch [19/30], Step [111/139], Loss: 0.0798\n",
      "Epoch [19/30], Step [112/139], Loss: 0.0877\n",
      "Epoch [19/30], Step [113/139], Loss: 0.1027\n",
      "Epoch [19/30], Step [114/139], Loss: 0.0757\n",
      "Epoch [19/30], Step [115/139], Loss: 0.0522\n",
      "Epoch [19/30], Step [116/139], Loss: 0.0854\n",
      "Epoch [19/30], Step [117/139], Loss: 0.1213\n",
      "Epoch [19/30], Step [118/139], Loss: 0.1048\n",
      "Epoch [19/30], Step [119/139], Loss: 0.0873\n",
      "Epoch [19/30], Step [120/139], Loss: 0.0526\n",
      "Epoch [19/30], Step [121/139], Loss: 0.0746\n",
      "Epoch [19/30], Step [122/139], Loss: 0.0830\n",
      "Epoch [19/30], Step [123/139], Loss: 0.0609\n",
      "Epoch [19/30], Step [124/139], Loss: 0.0788\n",
      "Epoch [19/30], Step [125/139], Loss: 0.0722\n",
      "Epoch [19/30], Step [126/139], Loss: 0.0924\n",
      "Epoch [19/30], Step [127/139], Loss: 0.0781\n",
      "Epoch [19/30], Step [128/139], Loss: 0.0901\n",
      "Epoch [19/30], Step [129/139], Loss: 0.0707\n",
      "Epoch [19/30], Step [130/139], Loss: 0.0724\n",
      "Epoch [19/30], Step [131/139], Loss: 0.0559\n",
      "Epoch [19/30], Step [132/139], Loss: 0.0906\n",
      "Epoch [19/30], Step [133/139], Loss: 0.0722\n",
      "Epoch [19/30], Step [134/139], Loss: 0.0811\n",
      "Epoch [19/30], Step [135/139], Loss: 0.0630\n",
      "Epoch [19/30], Step [136/139], Loss: 0.0597\n",
      "Epoch [19/30], Step [137/139], Loss: 0.0836\n",
      "Epoch [19/30], Step [138/139], Loss: 0.0717\n",
      "Epoch [19/30], Step [139/139], Loss: 0.1206\n",
      "Start validation #19\n",
      "Validation #19  Average Loss: 0.0779\n",
      "Best performance at epoch: 19\n",
      "Save model in ./saved/LSTM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30], Step [1/139], Loss: 0.0776\n",
      "Epoch [20/30], Step [2/139], Loss: 0.1020\n",
      "Epoch [20/30], Step [3/139], Loss: 0.0829\n",
      "Epoch [20/30], Step [4/139], Loss: 0.0904\n",
      "Epoch [20/30], Step [5/139], Loss: 0.0912\n",
      "Epoch [20/30], Step [6/139], Loss: 0.0795\n",
      "Epoch [20/30], Step [7/139], Loss: 0.0727\n",
      "Epoch [20/30], Step [8/139], Loss: 0.0971\n",
      "Epoch [20/30], Step [9/139], Loss: 0.0761\n",
      "Epoch [20/30], Step [10/139], Loss: 0.0743\n",
      "Epoch [20/30], Step [11/139], Loss: 0.0737\n",
      "Epoch [20/30], Step [12/139], Loss: 0.0825\n",
      "Epoch [20/30], Step [13/139], Loss: 0.0777\n",
      "Epoch [20/30], Step [14/139], Loss: 0.0887\n",
      "Epoch [20/30], Step [15/139], Loss: 0.0739\n",
      "Epoch [20/30], Step [16/139], Loss: 0.0917\n",
      "Epoch [20/30], Step [17/139], Loss: 0.0715\n",
      "Epoch [20/30], Step [18/139], Loss: 0.0827\n",
      "Epoch [20/30], Step [19/139], Loss: 0.0750\n",
      "Epoch [20/30], Step [20/139], Loss: 0.0891\n",
      "Epoch [20/30], Step [21/139], Loss: 0.0654\n",
      "Epoch [20/30], Step [22/139], Loss: 0.0773\n",
      "Epoch [20/30], Step [23/139], Loss: 0.0615\n",
      "Epoch [20/30], Step [24/139], Loss: 0.0858\n",
      "Epoch [20/30], Step [25/139], Loss: 0.0662\n",
      "Epoch [20/30], Step [26/139], Loss: 0.0736\n",
      "Epoch [20/30], Step [27/139], Loss: 0.0799\n",
      "Epoch [20/30], Step [28/139], Loss: 0.0765\n",
      "Epoch [20/30], Step [29/139], Loss: 0.0831\n",
      "Epoch [20/30], Step [30/139], Loss: 0.0846\n",
      "Epoch [20/30], Step [31/139], Loss: 0.0763\n",
      "Epoch [20/30], Step [32/139], Loss: 0.0716\n",
      "Epoch [20/30], Step [33/139], Loss: 0.0734\n",
      "Epoch [20/30], Step [34/139], Loss: 0.0790\n",
      "Epoch [20/30], Step [35/139], Loss: 0.0809\n",
      "Epoch [20/30], Step [36/139], Loss: 0.0800\n",
      "Epoch [20/30], Step [37/139], Loss: 0.0905\n",
      "Epoch [20/30], Step [38/139], Loss: 0.0845\n",
      "Epoch [20/30], Step [39/139], Loss: 0.0571\n",
      "Epoch [20/30], Step [40/139], Loss: 0.0918\n",
      "Epoch [20/30], Step [41/139], Loss: 0.0632\n",
      "Epoch [20/30], Step [42/139], Loss: 0.0595\n",
      "Epoch [20/30], Step [43/139], Loss: 0.0890\n",
      "Epoch [20/30], Step [44/139], Loss: 0.0600\n",
      "Epoch [20/30], Step [45/139], Loss: 0.0612\n",
      "Epoch [20/30], Step [46/139], Loss: 0.0679\n",
      "Epoch [20/30], Step [47/139], Loss: 0.0892\n",
      "Epoch [20/30], Step [48/139], Loss: 0.0723\n",
      "Epoch [20/30], Step [49/139], Loss: 0.0816\n",
      "Epoch [20/30], Step [50/139], Loss: 0.0677\n",
      "Epoch [20/30], Step [51/139], Loss: 0.0729\n",
      "Epoch [20/30], Step [52/139], Loss: 0.0715\n",
      "Epoch [20/30], Step [53/139], Loss: 0.0598\n",
      "Epoch [20/30], Step [54/139], Loss: 0.0698\n",
      "Epoch [20/30], Step [55/139], Loss: 0.0761\n",
      "Epoch [20/30], Step [56/139], Loss: 0.0737\n",
      "Epoch [20/30], Step [57/139], Loss: 0.0925\n",
      "Epoch [20/30], Step [58/139], Loss: 0.0588\n",
      "Epoch [20/30], Step [59/139], Loss: 0.0787\n",
      "Epoch [20/30], Step [60/139], Loss: 0.0796\n",
      "Epoch [20/30], Step [61/139], Loss: 0.0890\n",
      "Epoch [20/30], Step [62/139], Loss: 0.1017\n",
      "Epoch [20/30], Step [63/139], Loss: 0.0873\n",
      "Epoch [20/30], Step [64/139], Loss: 0.1008\n",
      "Epoch [20/30], Step [65/139], Loss: 0.0992\n",
      "Epoch [20/30], Step [66/139], Loss: 0.0942\n",
      "Epoch [20/30], Step [67/139], Loss: 0.0645\n",
      "Epoch [20/30], Step [68/139], Loss: 0.1012\n",
      "Epoch [20/30], Step [69/139], Loss: 0.0740\n",
      "Epoch [20/30], Step [70/139], Loss: 0.0756\n",
      "Epoch [20/30], Step [71/139], Loss: 0.0623\n",
      "Epoch [20/30], Step [72/139], Loss: 0.1107\n",
      "Epoch [20/30], Step [73/139], Loss: 0.0906\n",
      "Epoch [20/30], Step [74/139], Loss: 0.0689\n",
      "Epoch [20/30], Step [75/139], Loss: 0.0764\n",
      "Epoch [20/30], Step [76/139], Loss: 0.0799\n",
      "Epoch [20/30], Step [77/139], Loss: 0.1045\n",
      "Epoch [20/30], Step [78/139], Loss: 0.0717\n",
      "Epoch [20/30], Step [79/139], Loss: 0.0838\n",
      "Epoch [20/30], Step [80/139], Loss: 0.0827\n",
      "Epoch [20/30], Step [81/139], Loss: 0.0830\n",
      "Epoch [20/30], Step [82/139], Loss: 0.1053\n",
      "Epoch [20/30], Step [83/139], Loss: 0.0692\n",
      "Epoch [20/30], Step [84/139], Loss: 0.0852\n",
      "Epoch [20/30], Step [85/139], Loss: 0.0867\n",
      "Epoch [20/30], Step [86/139], Loss: 0.0798\n",
      "Epoch [20/30], Step [87/139], Loss: 0.0997\n",
      "Epoch [20/30], Step [88/139], Loss: 0.1095\n",
      "Epoch [20/30], Step [89/139], Loss: 0.0931\n",
      "Epoch [20/30], Step [90/139], Loss: 0.0599\n",
      "Epoch [20/30], Step [91/139], Loss: 0.0776\n",
      "Epoch [20/30], Step [92/139], Loss: 0.0539\n",
      "Epoch [20/30], Step [93/139], Loss: 0.0653\n",
      "Epoch [20/30], Step [94/139], Loss: 0.0919\n",
      "Epoch [20/30], Step [95/139], Loss: 0.0738\n",
      "Epoch [20/30], Step [96/139], Loss: 0.0855\n",
      "Epoch [20/30], Step [97/139], Loss: 0.0907\n",
      "Epoch [20/30], Step [98/139], Loss: 0.0856\n",
      "Epoch [20/30], Step [99/139], Loss: 0.0941\n",
      "Epoch [20/30], Step [100/139], Loss: 0.0666\n",
      "Epoch [20/30], Step [101/139], Loss: 0.0865\n",
      "Epoch [20/30], Step [102/139], Loss: 0.1177\n",
      "Epoch [20/30], Step [103/139], Loss: 0.0706\n",
      "Epoch [20/30], Step [104/139], Loss: 0.0824\n",
      "Epoch [20/30], Step [105/139], Loss: 0.0580\n",
      "Epoch [20/30], Step [106/139], Loss: 0.0692\n",
      "Epoch [20/30], Step [107/139], Loss: 0.0685\n",
      "Epoch [20/30], Step [108/139], Loss: 0.0997\n",
      "Epoch [20/30], Step [109/139], Loss: 0.1229\n",
      "Epoch [20/30], Step [110/139], Loss: 0.0846\n",
      "Epoch [20/30], Step [111/139], Loss: 0.0987\n",
      "Epoch [20/30], Step [112/139], Loss: 0.0670\n",
      "Epoch [20/30], Step [113/139], Loss: 0.0689\n",
      "Epoch [20/30], Step [114/139], Loss: 0.0673\n",
      "Epoch [20/30], Step [115/139], Loss: 0.0782\n",
      "Epoch [20/30], Step [116/139], Loss: 0.0734\n",
      "Epoch [20/30], Step [117/139], Loss: 0.0716\n",
      "Epoch [20/30], Step [118/139], Loss: 0.0801\n",
      "Epoch [20/30], Step [119/139], Loss: 0.0653\n",
      "Epoch [20/30], Step [120/139], Loss: 0.0515\n",
      "Epoch [20/30], Step [121/139], Loss: 0.0570\n",
      "Epoch [20/30], Step [122/139], Loss: 0.0807\n",
      "Epoch [20/30], Step [123/139], Loss: 0.1009\n",
      "Epoch [20/30], Step [124/139], Loss: 0.0719\n",
      "Epoch [20/30], Step [125/139], Loss: 0.0992\n",
      "Epoch [20/30], Step [126/139], Loss: 0.0702\n",
      "Epoch [20/30], Step [127/139], Loss: 0.0769\n",
      "Epoch [20/30], Step [128/139], Loss: 0.0671\n",
      "Epoch [20/30], Step [129/139], Loss: 0.0803\n",
      "Epoch [20/30], Step [130/139], Loss: 0.0649\n",
      "Epoch [20/30], Step [131/139], Loss: 0.0797\n",
      "Epoch [20/30], Step [132/139], Loss: 0.1069\n",
      "Epoch [20/30], Step [133/139], Loss: 0.0946\n",
      "Epoch [20/30], Step [134/139], Loss: 0.0624\n",
      "Epoch [20/30], Step [135/139], Loss: 0.0736\n",
      "Epoch [20/30], Step [136/139], Loss: 0.0735\n",
      "Epoch [20/30], Step [137/139], Loss: 0.0763\n",
      "Epoch [20/30], Step [138/139], Loss: 0.0859\n",
      "Epoch [20/30], Step [139/139], Loss: 0.0663\n",
      "Start validation #20\n",
      "Validation #20  Average Loss: 0.0768\n",
      "Best performance at epoch: 20\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [21/30], Step [1/139], Loss: 0.0848\n",
      "Epoch [21/30], Step [2/139], Loss: 0.0700\n",
      "Epoch [21/30], Step [3/139], Loss: 0.0718\n",
      "Epoch [21/30], Step [4/139], Loss: 0.0926\n",
      "Epoch [21/30], Step [5/139], Loss: 0.0787\n",
      "Epoch [21/30], Step [6/139], Loss: 0.0817\n",
      "Epoch [21/30], Step [7/139], Loss: 0.0731\n",
      "Epoch [21/30], Step [8/139], Loss: 0.0853\n",
      "Epoch [21/30], Step [9/139], Loss: 0.0702\n",
      "Epoch [21/30], Step [10/139], Loss: 0.0766\n",
      "Epoch [21/30], Step [11/139], Loss: 0.0893\n",
      "Epoch [21/30], Step [12/139], Loss: 0.0892\n",
      "Epoch [21/30], Step [13/139], Loss: 0.0883\n",
      "Epoch [21/30], Step [14/139], Loss: 0.0510\n",
      "Epoch [21/30], Step [15/139], Loss: 0.0845\n",
      "Epoch [21/30], Step [16/139], Loss: 0.0846\n",
      "Epoch [21/30], Step [17/139], Loss: 0.0671\n",
      "Epoch [21/30], Step [18/139], Loss: 0.0790\n",
      "Epoch [21/30], Step [19/139], Loss: 0.0939\n",
      "Epoch [21/30], Step [20/139], Loss: 0.0830\n",
      "Epoch [21/30], Step [21/139], Loss: 0.0590\n",
      "Epoch [21/30], Step [22/139], Loss: 0.0726\n",
      "Epoch [21/30], Step [23/139], Loss: 0.0799\n",
      "Epoch [21/30], Step [24/139], Loss: 0.0852\n",
      "Epoch [21/30], Step [25/139], Loss: 0.0456\n",
      "Epoch [21/30], Step [26/139], Loss: 0.0753\n",
      "Epoch [21/30], Step [27/139], Loss: 0.0723\n",
      "Epoch [21/30], Step [28/139], Loss: 0.0729\n",
      "Epoch [21/30], Step [29/139], Loss: 0.0842\n",
      "Epoch [21/30], Step [30/139], Loss: 0.0806\n",
      "Epoch [21/30], Step [31/139], Loss: 0.0800\n",
      "Epoch [21/30], Step [32/139], Loss: 0.0795\n",
      "Epoch [21/30], Step [33/139], Loss: 0.0734\n",
      "Epoch [21/30], Step [34/139], Loss: 0.0798\n",
      "Epoch [21/30], Step [35/139], Loss: 0.0661\n",
      "Epoch [21/30], Step [36/139], Loss: 0.0727\n",
      "Epoch [21/30], Step [37/139], Loss: 0.0728\n",
      "Epoch [21/30], Step [38/139], Loss: 0.0908\n",
      "Epoch [21/30], Step [39/139], Loss: 0.0581\n",
      "Epoch [21/30], Step [40/139], Loss: 0.0448\n",
      "Epoch [21/30], Step [41/139], Loss: 0.0626\n",
      "Epoch [21/30], Step [42/139], Loss: 0.0920\n",
      "Epoch [21/30], Step [43/139], Loss: 0.0960\n",
      "Epoch [21/30], Step [44/139], Loss: 0.0785\n",
      "Epoch [21/30], Step [45/139], Loss: 0.0744\n",
      "Epoch [21/30], Step [46/139], Loss: 0.0727\n",
      "Epoch [21/30], Step [47/139], Loss: 0.0597\n",
      "Epoch [21/30], Step [48/139], Loss: 0.0876\n",
      "Epoch [21/30], Step [49/139], Loss: 0.0673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/30], Step [50/139], Loss: 0.0818\n",
      "Epoch [21/30], Step [51/139], Loss: 0.0643\n",
      "Epoch [21/30], Step [52/139], Loss: 0.0796\n",
      "Epoch [21/30], Step [53/139], Loss: 0.0672\n",
      "Epoch [21/30], Step [54/139], Loss: 0.1000\n",
      "Epoch [21/30], Step [55/139], Loss: 0.0769\n",
      "Epoch [21/30], Step [56/139], Loss: 0.0638\n",
      "Epoch [21/30], Step [57/139], Loss: 0.0532\n",
      "Epoch [21/30], Step [58/139], Loss: 0.0867\n",
      "Epoch [21/30], Step [59/139], Loss: 0.0883\n",
      "Epoch [21/30], Step [60/139], Loss: 0.0808\n",
      "Epoch [21/30], Step [61/139], Loss: 0.0800\n",
      "Epoch [21/30], Step [62/139], Loss: 0.0625\n",
      "Epoch [21/30], Step [63/139], Loss: 0.0863\n",
      "Epoch [21/30], Step [64/139], Loss: 0.0629\n",
      "Epoch [21/30], Step [65/139], Loss: 0.0741\n",
      "Epoch [21/30], Step [66/139], Loss: 0.0886\n",
      "Epoch [21/30], Step [67/139], Loss: 0.0911\n",
      "Epoch [21/30], Step [68/139], Loss: 0.0569\n",
      "Epoch [21/30], Step [69/139], Loss: 0.0566\n",
      "Epoch [21/30], Step [70/139], Loss: 0.0858\n",
      "Epoch [21/30], Step [71/139], Loss: 0.0853\n",
      "Epoch [21/30], Step [72/139], Loss: 0.0755\n",
      "Epoch [21/30], Step [73/139], Loss: 0.1086\n",
      "Epoch [21/30], Step [74/139], Loss: 0.0821\n",
      "Epoch [21/30], Step [75/139], Loss: 0.1099\n",
      "Epoch [21/30], Step [76/139], Loss: 0.0829\n",
      "Epoch [21/30], Step [77/139], Loss: 0.0929\n",
      "Epoch [21/30], Step [78/139], Loss: 0.0635\n",
      "Epoch [21/30], Step [79/139], Loss: 0.0730\n",
      "Epoch [21/30], Step [80/139], Loss: 0.0689\n",
      "Epoch [21/30], Step [81/139], Loss: 0.0712\n",
      "Epoch [21/30], Step [82/139], Loss: 0.1052\n",
      "Epoch [21/30], Step [83/139], Loss: 0.0746\n",
      "Epoch [21/30], Step [84/139], Loss: 0.0736\n",
      "Epoch [21/30], Step [85/139], Loss: 0.0995\n",
      "Epoch [21/30], Step [86/139], Loss: 0.1087\n",
      "Epoch [21/30], Step [87/139], Loss: 0.0916\n",
      "Epoch [21/30], Step [88/139], Loss: 0.1037\n",
      "Epoch [21/30], Step [89/139], Loss: 0.0645\n",
      "Epoch [21/30], Step [90/139], Loss: 0.0826\n",
      "Epoch [21/30], Step [91/139], Loss: 0.0561\n",
      "Epoch [21/30], Step [92/139], Loss: 0.0890\n",
      "Epoch [21/30], Step [93/139], Loss: 0.0840\n",
      "Epoch [21/30], Step [94/139], Loss: 0.0765\n",
      "Epoch [21/30], Step [95/139], Loss: 0.0897\n",
      "Epoch [21/30], Step [96/139], Loss: 0.0581\n",
      "Epoch [21/30], Step [97/139], Loss: 0.1136\n",
      "Epoch [21/30], Step [98/139], Loss: 0.0878\n",
      "Epoch [21/30], Step [99/139], Loss: 0.1007\n",
      "Epoch [21/30], Step [100/139], Loss: 0.0700\n",
      "Epoch [21/30], Step [101/139], Loss: 0.0821\n",
      "Epoch [21/30], Step [102/139], Loss: 0.0813\n",
      "Epoch [21/30], Step [103/139], Loss: 0.0802\n",
      "Epoch [21/30], Step [104/139], Loss: 0.0854\n",
      "Epoch [21/30], Step [105/139], Loss: 0.0593\n",
      "Epoch [21/30], Step [106/139], Loss: 0.0709\n",
      "Epoch [21/30], Step [107/139], Loss: 0.0971\n",
      "Epoch [21/30], Step [108/139], Loss: 0.0724\n",
      "Epoch [21/30], Step [109/139], Loss: 0.0803\n",
      "Epoch [21/30], Step [110/139], Loss: 0.1058\n",
      "Epoch [21/30], Step [111/139], Loss: 0.0920\n",
      "Epoch [21/30], Step [112/139], Loss: 0.0593\n",
      "Epoch [21/30], Step [113/139], Loss: 0.0771\n",
      "Epoch [21/30], Step [114/139], Loss: 0.0602\n",
      "Epoch [21/30], Step [115/139], Loss: 0.0768\n",
      "Epoch [21/30], Step [116/139], Loss: 0.0600\n",
      "Epoch [21/30], Step [117/139], Loss: 0.0976\n",
      "Epoch [21/30], Step [118/139], Loss: 0.0714\n",
      "Epoch [21/30], Step [119/139], Loss: 0.0692\n",
      "Epoch [21/30], Step [120/139], Loss: 0.1001\n",
      "Epoch [21/30], Step [121/139], Loss: 0.0877\n",
      "Epoch [21/30], Step [122/139], Loss: 0.0635\n",
      "Epoch [21/30], Step [123/139], Loss: 0.0547\n",
      "Epoch [21/30], Step [124/139], Loss: 0.0925\n",
      "Epoch [21/30], Step [125/139], Loss: 0.1069\n",
      "Epoch [21/30], Step [126/139], Loss: 0.0839\n",
      "Epoch [21/30], Step [127/139], Loss: 0.1002\n",
      "Epoch [21/30], Step [128/139], Loss: 0.0734\n",
      "Epoch [21/30], Step [129/139], Loss: 0.0640\n",
      "Epoch [21/30], Step [130/139], Loss: 0.0748\n",
      "Epoch [21/30], Step [131/139], Loss: 0.1256\n",
      "Epoch [21/30], Step [132/139], Loss: 0.1010\n",
      "Epoch [21/30], Step [133/139], Loss: 0.0751\n",
      "Epoch [21/30], Step [134/139], Loss: 0.0622\n",
      "Epoch [21/30], Step [135/139], Loss: 0.0803\n",
      "Epoch [21/30], Step [136/139], Loss: 0.0505\n",
      "Epoch [21/30], Step [137/139], Loss: 0.0976\n",
      "Epoch [21/30], Step [138/139], Loss: 0.0664\n",
      "Epoch [21/30], Step [139/139], Loss: 0.0914\n",
      "Start validation #21\n",
      "Validation #21  Average Loss: 0.0766\n",
      "Best performance at epoch: 21\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [22/30], Step [1/139], Loss: 0.0744\n",
      "Epoch [22/30], Step [2/139], Loss: 0.0842\n",
      "Epoch [22/30], Step [3/139], Loss: 0.0775\n",
      "Epoch [22/30], Step [4/139], Loss: 0.0717\n",
      "Epoch [22/30], Step [5/139], Loss: 0.0760\n",
      "Epoch [22/30], Step [6/139], Loss: 0.0716\n",
      "Epoch [22/30], Step [7/139], Loss: 0.0555\n",
      "Epoch [22/30], Step [8/139], Loss: 0.0878\n",
      "Epoch [22/30], Step [9/139], Loss: 0.0670\n",
      "Epoch [22/30], Step [10/139], Loss: 0.0776\n",
      "Epoch [22/30], Step [11/139], Loss: 0.0750\n",
      "Epoch [22/30], Step [12/139], Loss: 0.0847\n",
      "Epoch [22/30], Step [13/139], Loss: 0.0574\n",
      "Epoch [22/30], Step [14/139], Loss: 0.0767\n",
      "Epoch [22/30], Step [15/139], Loss: 0.0864\n",
      "Epoch [22/30], Step [16/139], Loss: 0.0611\n",
      "Epoch [22/30], Step [17/139], Loss: 0.0735\n",
      "Epoch [22/30], Step [18/139], Loss: 0.0980\n",
      "Epoch [22/30], Step [19/139], Loss: 0.0693\n",
      "Epoch [22/30], Step [20/139], Loss: 0.0673\n",
      "Epoch [22/30], Step [21/139], Loss: 0.0749\n",
      "Epoch [22/30], Step [22/139], Loss: 0.0611\n",
      "Epoch [22/30], Step [23/139], Loss: 0.0757\n",
      "Epoch [22/30], Step [24/139], Loss: 0.0795\n",
      "Epoch [22/30], Step [25/139], Loss: 0.0813\n",
      "Epoch [22/30], Step [26/139], Loss: 0.0791\n",
      "Epoch [22/30], Step [27/139], Loss: 0.0760\n",
      "Epoch [22/30], Step [28/139], Loss: 0.0876\n",
      "Epoch [22/30], Step [29/139], Loss: 0.0856\n",
      "Epoch [22/30], Step [30/139], Loss: 0.0488\n",
      "Epoch [22/30], Step [31/139], Loss: 0.0760\n",
      "Epoch [22/30], Step [32/139], Loss: 0.0967\n",
      "Epoch [22/30], Step [33/139], Loss: 0.1419\n",
      "Epoch [22/30], Step [34/139], Loss: 0.0719\n",
      "Epoch [22/30], Step [35/139], Loss: 0.0620\n",
      "Epoch [22/30], Step [36/139], Loss: 0.0798\n",
      "Epoch [22/30], Step [37/139], Loss: 0.1042\n",
      "Epoch [22/30], Step [38/139], Loss: 0.0631\n",
      "Epoch [22/30], Step [39/139], Loss: 0.0592\n",
      "Epoch [22/30], Step [40/139], Loss: 0.0996\n",
      "Epoch [22/30], Step [41/139], Loss: 0.0745\n",
      "Epoch [22/30], Step [42/139], Loss: 0.0729\n",
      "Epoch [22/30], Step [43/139], Loss: 0.0610\n",
      "Epoch [22/30], Step [44/139], Loss: 0.0785\n",
      "Epoch [22/30], Step [45/139], Loss: 0.0761\n",
      "Epoch [22/30], Step [46/139], Loss: 0.0794\n",
      "Epoch [22/30], Step [47/139], Loss: 0.0624\n",
      "Epoch [22/30], Step [48/139], Loss: 0.0637\n",
      "Epoch [22/30], Step [49/139], Loss: 0.0837\n",
      "Epoch [22/30], Step [50/139], Loss: 0.0656\n",
      "Epoch [22/30], Step [51/139], Loss: 0.0730\n",
      "Epoch [22/30], Step [52/139], Loss: 0.0535\n",
      "Epoch [22/30], Step [53/139], Loss: 0.0901\n",
      "Epoch [22/30], Step [54/139], Loss: 0.0970\n",
      "Epoch [22/30], Step [55/139], Loss: 0.0677\n",
      "Epoch [22/30], Step [56/139], Loss: 0.0734\n",
      "Epoch [22/30], Step [57/139], Loss: 0.0798\n",
      "Epoch [22/30], Step [58/139], Loss: 0.0750\n",
      "Epoch [22/30], Step [59/139], Loss: 0.0908\n",
      "Epoch [22/30], Step [60/139], Loss: 0.0738\n",
      "Epoch [22/30], Step [61/139], Loss: 0.0824\n",
      "Epoch [22/30], Step [62/139], Loss: 0.0745\n",
      "Epoch [22/30], Step [63/139], Loss: 0.0716\n",
      "Epoch [22/30], Step [64/139], Loss: 0.0928\n",
      "Epoch [22/30], Step [65/139], Loss: 0.0964\n",
      "Epoch [22/30], Step [66/139], Loss: 0.0850\n",
      "Epoch [22/30], Step [67/139], Loss: 0.0681\n",
      "Epoch [22/30], Step [68/139], Loss: 0.1074\n",
      "Epoch [22/30], Step [69/139], Loss: 0.0473\n",
      "Epoch [22/30], Step [70/139], Loss: 0.0864\n",
      "Epoch [22/30], Step [71/139], Loss: 0.0781\n",
      "Epoch [22/30], Step [72/139], Loss: 0.0972\n",
      "Epoch [22/30], Step [73/139], Loss: 0.0606\n",
      "Epoch [22/30], Step [74/139], Loss: 0.0749\n",
      "Epoch [22/30], Step [75/139], Loss: 0.0795\n",
      "Epoch [22/30], Step [76/139], Loss: 0.0688\n",
      "Epoch [22/30], Step [77/139], Loss: 0.0729\n",
      "Epoch [22/30], Step [78/139], Loss: 0.0854\n",
      "Epoch [22/30], Step [79/139], Loss: 0.0949\n",
      "Epoch [22/30], Step [80/139], Loss: 0.0634\n",
      "Epoch [22/30], Step [81/139], Loss: 0.0727\n",
      "Epoch [22/30], Step [82/139], Loss: 0.0629\n",
      "Epoch [22/30], Step [83/139], Loss: 0.0665\n",
      "Epoch [22/30], Step [84/139], Loss: 0.0692\n",
      "Epoch [22/30], Step [85/139], Loss: 0.0682\n",
      "Epoch [22/30], Step [86/139], Loss: 0.0881\n",
      "Epoch [22/30], Step [87/139], Loss: 0.1005\n",
      "Epoch [22/30], Step [88/139], Loss: 0.0905\n",
      "Epoch [22/30], Step [89/139], Loss: 0.0751\n",
      "Epoch [22/30], Step [90/139], Loss: 0.0708\n",
      "Epoch [22/30], Step [91/139], Loss: 0.0908\n",
      "Epoch [22/30], Step [92/139], Loss: 0.0772\n",
      "Epoch [22/30], Step [93/139], Loss: 0.0955\n",
      "Epoch [22/30], Step [94/139], Loss: 0.0725\n",
      "Epoch [22/30], Step [95/139], Loss: 0.0930\n",
      "Epoch [22/30], Step [96/139], Loss: 0.1071\n",
      "Epoch [22/30], Step [97/139], Loss: 0.0848\n",
      "Epoch [22/30], Step [98/139], Loss: 0.0867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/30], Step [99/139], Loss: 0.0804\n",
      "Epoch [22/30], Step [100/139], Loss: 0.1028\n",
      "Epoch [22/30], Step [101/139], Loss: 0.0812\n",
      "Epoch [22/30], Step [102/139], Loss: 0.0624\n",
      "Epoch [22/30], Step [103/139], Loss: 0.1277\n",
      "Epoch [22/30], Step [104/139], Loss: 0.0735\n",
      "Epoch [22/30], Step [105/139], Loss: 0.0764\n",
      "Epoch [22/30], Step [106/139], Loss: 0.0668\n",
      "Epoch [22/30], Step [107/139], Loss: 0.0976\n",
      "Epoch [22/30], Step [108/139], Loss: 0.0932\n",
      "Epoch [22/30], Step [109/139], Loss: 0.0568\n",
      "Epoch [22/30], Step [110/139], Loss: 0.0957\n",
      "Epoch [22/30], Step [111/139], Loss: 0.0839\n",
      "Epoch [22/30], Step [112/139], Loss: 0.0759\n",
      "Epoch [22/30], Step [113/139], Loss: 0.0763\n",
      "Epoch [22/30], Step [114/139], Loss: 0.0990\n",
      "Epoch [22/30], Step [115/139], Loss: 0.0639\n",
      "Epoch [22/30], Step [116/139], Loss: 0.0634\n",
      "Epoch [22/30], Step [117/139], Loss: 0.0850\n",
      "Epoch [22/30], Step [118/139], Loss: 0.0915\n",
      "Epoch [22/30], Step [119/139], Loss: 0.0542\n",
      "Epoch [22/30], Step [120/139], Loss: 0.0614\n",
      "Epoch [22/30], Step [121/139], Loss: 0.0621\n",
      "Epoch [22/30], Step [122/139], Loss: 0.0655\n",
      "Epoch [22/30], Step [123/139], Loss: 0.0808\n",
      "Epoch [22/30], Step [124/139], Loss: 0.0625\n",
      "Epoch [22/30], Step [125/139], Loss: 0.0860\n",
      "Epoch [22/30], Step [126/139], Loss: 0.0967\n",
      "Epoch [22/30], Step [127/139], Loss: 0.0489\n",
      "Epoch [22/30], Step [128/139], Loss: 0.0737\n",
      "Epoch [22/30], Step [129/139], Loss: 0.0983\n",
      "Epoch [22/30], Step [130/139], Loss: 0.0638\n",
      "Epoch [22/30], Step [131/139], Loss: 0.0714\n",
      "Epoch [22/30], Step [132/139], Loss: 0.0925\n",
      "Epoch [22/30], Step [133/139], Loss: 0.0710\n",
      "Epoch [22/30], Step [134/139], Loss: 0.0862\n",
      "Epoch [22/30], Step [135/139], Loss: 0.0543\n",
      "Epoch [22/30], Step [136/139], Loss: 0.0983\n",
      "Epoch [22/30], Step [137/139], Loss: 0.0893\n",
      "Epoch [22/30], Step [138/139], Loss: 0.0757\n",
      "Epoch [22/30], Step [139/139], Loss: 0.0977\n",
      "Start validation #22\n",
      "Validation #22  Average Loss: 0.0752\n",
      "Best performance at epoch: 22\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [23/30], Step [1/139], Loss: 0.0913\n",
      "Epoch [23/30], Step [2/139], Loss: 0.0697\n",
      "Epoch [23/30], Step [3/139], Loss: 0.0732\n",
      "Epoch [23/30], Step [4/139], Loss: 0.0896\n",
      "Epoch [23/30], Step [5/139], Loss: 0.0679\n",
      "Epoch [23/30], Step [6/139], Loss: 0.0783\n",
      "Epoch [23/30], Step [7/139], Loss: 0.0811\n",
      "Epoch [23/30], Step [8/139], Loss: 0.0932\n",
      "Epoch [23/30], Step [9/139], Loss: 0.0971\n",
      "Epoch [23/30], Step [10/139], Loss: 0.1013\n",
      "Epoch [23/30], Step [11/139], Loss: 0.0664\n",
      "Epoch [23/30], Step [12/139], Loss: 0.0807\n",
      "Epoch [23/30], Step [13/139], Loss: 0.0765\n",
      "Epoch [23/30], Step [14/139], Loss: 0.0731\n",
      "Epoch [23/30], Step [15/139], Loss: 0.0768\n",
      "Epoch [23/30], Step [16/139], Loss: 0.0812\n",
      "Epoch [23/30], Step [17/139], Loss: 0.0971\n",
      "Epoch [23/30], Step [18/139], Loss: 0.0527\n",
      "Epoch [23/30], Step [19/139], Loss: 0.0642\n",
      "Epoch [23/30], Step [20/139], Loss: 0.0699\n",
      "Epoch [23/30], Step [21/139], Loss: 0.0758\n",
      "Epoch [23/30], Step [22/139], Loss: 0.0805\n",
      "Epoch [23/30], Step [23/139], Loss: 0.0724\n",
      "Epoch [23/30], Step [24/139], Loss: 0.0782\n",
      "Epoch [23/30], Step [25/139], Loss: 0.0786\n",
      "Epoch [23/30], Step [26/139], Loss: 0.0826\n",
      "Epoch [23/30], Step [27/139], Loss: 0.0784\n",
      "Epoch [23/30], Step [28/139], Loss: 0.0611\n",
      "Epoch [23/30], Step [29/139], Loss: 0.0645\n",
      "Epoch [23/30], Step [30/139], Loss: 0.0952\n",
      "Epoch [23/30], Step [31/139], Loss: 0.0827\n",
      "Epoch [23/30], Step [32/139], Loss: 0.0820\n",
      "Epoch [23/30], Step [33/139], Loss: 0.0533\n",
      "Epoch [23/30], Step [34/139], Loss: 0.0627\n",
      "Epoch [23/30], Step [35/139], Loss: 0.0677\n",
      "Epoch [23/30], Step [36/139], Loss: 0.0883\n",
      "Epoch [23/30], Step [37/139], Loss: 0.0858\n",
      "Epoch [23/30], Step [38/139], Loss: 0.0869\n",
      "Epoch [23/30], Step [39/139], Loss: 0.0770\n",
      "Epoch [23/30], Step [40/139], Loss: 0.0839\n",
      "Epoch [23/30], Step [41/139], Loss: 0.0924\n",
      "Epoch [23/30], Step [42/139], Loss: 0.0759\n",
      "Epoch [23/30], Step [43/139], Loss: 0.0773\n",
      "Epoch [23/30], Step [44/139], Loss: 0.0764\n",
      "Epoch [23/30], Step [45/139], Loss: 0.0589\n",
      "Epoch [23/30], Step [46/139], Loss: 0.0639\n",
      "Epoch [23/30], Step [47/139], Loss: 0.0561\n",
      "Epoch [23/30], Step [48/139], Loss: 0.0678\n",
      "Epoch [23/30], Step [49/139], Loss: 0.0838\n",
      "Epoch [23/30], Step [50/139], Loss: 0.0503\n",
      "Epoch [23/30], Step [51/139], Loss: 0.1084\n",
      "Epoch [23/30], Step [52/139], Loss: 0.0818\n",
      "Epoch [23/30], Step [53/139], Loss: 0.0871\n",
      "Epoch [23/30], Step [54/139], Loss: 0.0721\n",
      "Epoch [23/30], Step [55/139], Loss: 0.0957\n",
      "Epoch [23/30], Step [56/139], Loss: 0.0715\n",
      "Epoch [23/30], Step [57/139], Loss: 0.0786\n",
      "Epoch [23/30], Step [58/139], Loss: 0.0792\n",
      "Epoch [23/30], Step [59/139], Loss: 0.0693\n",
      "Epoch [23/30], Step [60/139], Loss: 0.0814\n",
      "Epoch [23/30], Step [61/139], Loss: 0.0646\n",
      "Epoch [23/30], Step [62/139], Loss: 0.0825\n",
      "Epoch [23/30], Step [63/139], Loss: 0.0548\n",
      "Epoch [23/30], Step [64/139], Loss: 0.0556\n",
      "Epoch [23/30], Step [65/139], Loss: 0.0573\n",
      "Epoch [23/30], Step [66/139], Loss: 0.0750\n",
      "Epoch [23/30], Step [67/139], Loss: 0.0719\n",
      "Epoch [23/30], Step [68/139], Loss: 0.0808\n",
      "Epoch [23/30], Step [69/139], Loss: 0.0600\n",
      "Epoch [23/30], Step [70/139], Loss: 0.0548\n",
      "Epoch [23/30], Step [71/139], Loss: 0.1283\n",
      "Epoch [23/30], Step [72/139], Loss: 0.0668\n",
      "Epoch [23/30], Step [73/139], Loss: 0.0806\n",
      "Epoch [23/30], Step [74/139], Loss: 0.0633\n",
      "Epoch [23/30], Step [75/139], Loss: 0.0784\n",
      "Epoch [23/30], Step [76/139], Loss: 0.0780\n",
      "Epoch [23/30], Step [77/139], Loss: 0.1116\n",
      "Epoch [23/30], Step [78/139], Loss: 0.0917\n",
      "Epoch [23/30], Step [79/139], Loss: 0.0634\n",
      "Epoch [23/30], Step [80/139], Loss: 0.0824\n",
      "Epoch [23/30], Step [81/139], Loss: 0.0885\n",
      "Epoch [23/30], Step [82/139], Loss: 0.0781\n",
      "Epoch [23/30], Step [83/139], Loss: 0.0637\n",
      "Epoch [23/30], Step [84/139], Loss: 0.0839\n",
      "Epoch [23/30], Step [85/139], Loss: 0.0531\n",
      "Epoch [23/30], Step [86/139], Loss: 0.1359\n",
      "Epoch [23/30], Step [87/139], Loss: 0.0880\n",
      "Epoch [23/30], Step [88/139], Loss: 0.0782\n",
      "Epoch [23/30], Step [89/139], Loss: 0.1112\n",
      "Epoch [23/30], Step [90/139], Loss: 0.1088\n",
      "Epoch [23/30], Step [91/139], Loss: 0.0733\n",
      "Epoch [23/30], Step [92/139], Loss: 0.0981\n",
      "Epoch [23/30], Step [93/139], Loss: 0.0744\n",
      "Epoch [23/30], Step [94/139], Loss: 0.0591\n",
      "Epoch [23/30], Step [95/139], Loss: 0.0795\n",
      "Epoch [23/30], Step [96/139], Loss: 0.0623\n",
      "Epoch [23/30], Step [97/139], Loss: 0.0601\n",
      "Epoch [23/30], Step [98/139], Loss: 0.0921\n",
      "Epoch [23/30], Step [99/139], Loss: 0.0894\n",
      "Epoch [23/30], Step [100/139], Loss: 0.0814\n",
      "Epoch [23/30], Step [101/139], Loss: 0.0909\n",
      "Epoch [23/30], Step [102/139], Loss: 0.0907\n",
      "Epoch [23/30], Step [103/139], Loss: 0.1071\n",
      "Epoch [23/30], Step [104/139], Loss: 0.0548\n",
      "Epoch [23/30], Step [105/139], Loss: 0.0922\n",
      "Epoch [23/30], Step [106/139], Loss: 0.0872\n",
      "Epoch [23/30], Step [107/139], Loss: 0.0798\n",
      "Epoch [23/30], Step [108/139], Loss: 0.0621\n",
      "Epoch [23/30], Step [109/139], Loss: 0.0824\n",
      "Epoch [23/30], Step [110/139], Loss: 0.0701\n",
      "Epoch [23/30], Step [111/139], Loss: 0.0692\n",
      "Epoch [23/30], Step [112/139], Loss: 0.0799\n",
      "Epoch [23/30], Step [113/139], Loss: 0.0570\n",
      "Epoch [23/30], Step [114/139], Loss: 0.0832\n",
      "Epoch [23/30], Step [115/139], Loss: 0.0625\n",
      "Epoch [23/30], Step [116/139], Loss: 0.0952\n",
      "Epoch [23/30], Step [117/139], Loss: 0.0815\n",
      "Epoch [23/30], Step [118/139], Loss: 0.0560\n",
      "Epoch [23/30], Step [119/139], Loss: 0.0747\n",
      "Epoch [23/30], Step [120/139], Loss: 0.0681\n",
      "Epoch [23/30], Step [121/139], Loss: 0.0684\n",
      "Epoch [23/30], Step [122/139], Loss: 0.0713\n",
      "Epoch [23/30], Step [123/139], Loss: 0.0898\n",
      "Epoch [23/30], Step [124/139], Loss: 0.0653\n",
      "Epoch [23/30], Step [125/139], Loss: 0.0693\n",
      "Epoch [23/30], Step [126/139], Loss: 0.0738\n",
      "Epoch [23/30], Step [127/139], Loss: 0.0900\n",
      "Epoch [23/30], Step [128/139], Loss: 0.0960\n",
      "Epoch [23/30], Step [129/139], Loss: 0.0695\n",
      "Epoch [23/30], Step [130/139], Loss: 0.0751\n",
      "Epoch [23/30], Step [131/139], Loss: 0.0649\n",
      "Epoch [23/30], Step [132/139], Loss: 0.0835\n",
      "Epoch [23/30], Step [133/139], Loss: 0.1094\n",
      "Epoch [23/30], Step [134/139], Loss: 0.0724\n",
      "Epoch [23/30], Step [135/139], Loss: 0.0643\n",
      "Epoch [23/30], Step [136/139], Loss: 0.0780\n",
      "Epoch [23/30], Step [137/139], Loss: 0.0765\n",
      "Epoch [23/30], Step [138/139], Loss: 0.0741\n",
      "Epoch [23/30], Step [139/139], Loss: 0.0657\n",
      "Start validation #23\n",
      "Validation #23  Average Loss: 0.0748\n",
      "Best performance at epoch: 23\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [24/30], Step [1/139], Loss: 0.0756\n",
      "Epoch [24/30], Step [2/139], Loss: 0.0753\n",
      "Epoch [24/30], Step [3/139], Loss: 0.0760\n",
      "Epoch [24/30], Step [4/139], Loss: 0.0930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/30], Step [5/139], Loss: 0.0838\n",
      "Epoch [24/30], Step [6/139], Loss: 0.0928\n",
      "Epoch [24/30], Step [7/139], Loss: 0.0760\n",
      "Epoch [24/30], Step [8/139], Loss: 0.0677\n",
      "Epoch [24/30], Step [9/139], Loss: 0.0865\n",
      "Epoch [24/30], Step [10/139], Loss: 0.0814\n",
      "Epoch [24/30], Step [11/139], Loss: 0.0658\n",
      "Epoch [24/30], Step [12/139], Loss: 0.1001\n",
      "Epoch [24/30], Step [13/139], Loss: 0.0878\n",
      "Epoch [24/30], Step [14/139], Loss: 0.0752\n",
      "Epoch [24/30], Step [15/139], Loss: 0.0693\n",
      "Epoch [24/30], Step [16/139], Loss: 0.0820\n",
      "Epoch [24/30], Step [17/139], Loss: 0.0642\n",
      "Epoch [24/30], Step [18/139], Loss: 0.0653\n",
      "Epoch [24/30], Step [19/139], Loss: 0.0555\n",
      "Epoch [24/30], Step [20/139], Loss: 0.0724\n",
      "Epoch [24/30], Step [21/139], Loss: 0.0895\n",
      "Epoch [24/30], Step [22/139], Loss: 0.0712\n",
      "Epoch [24/30], Step [23/139], Loss: 0.0742\n",
      "Epoch [24/30], Step [24/139], Loss: 0.0545\n",
      "Epoch [24/30], Step [25/139], Loss: 0.0517\n",
      "Epoch [24/30], Step [26/139], Loss: 0.1100\n",
      "Epoch [24/30], Step [27/139], Loss: 0.1014\n",
      "Epoch [24/30], Step [28/139], Loss: 0.0841\n",
      "Epoch [24/30], Step [29/139], Loss: 0.0967\n",
      "Epoch [24/30], Step [30/139], Loss: 0.0877\n",
      "Epoch [24/30], Step [31/139], Loss: 0.0729\n",
      "Epoch [24/30], Step [32/139], Loss: 0.0651\n",
      "Epoch [24/30], Step [33/139], Loss: 0.0846\n",
      "Epoch [24/30], Step [34/139], Loss: 0.0791\n",
      "Epoch [24/30], Step [35/139], Loss: 0.0645\n",
      "Epoch [24/30], Step [36/139], Loss: 0.0801\n",
      "Epoch [24/30], Step [37/139], Loss: 0.0608\n",
      "Epoch [24/30], Step [38/139], Loss: 0.0851\n",
      "Epoch [24/30], Step [39/139], Loss: 0.0545\n",
      "Epoch [24/30], Step [40/139], Loss: 0.0711\n",
      "Epoch [24/30], Step [41/139], Loss: 0.0674\n",
      "Epoch [24/30], Step [42/139], Loss: 0.0589\n",
      "Epoch [24/30], Step [43/139], Loss: 0.0690\n",
      "Epoch [24/30], Step [44/139], Loss: 0.0738\n",
      "Epoch [24/30], Step [45/139], Loss: 0.0838\n",
      "Epoch [24/30], Step [46/139], Loss: 0.0769\n",
      "Epoch [24/30], Step [47/139], Loss: 0.0686\n",
      "Epoch [24/30], Step [48/139], Loss: 0.1222\n",
      "Epoch [24/30], Step [49/139], Loss: 0.0813\n",
      "Epoch [24/30], Step [50/139], Loss: 0.0755\n",
      "Epoch [24/30], Step [51/139], Loss: 0.0840\n",
      "Epoch [24/30], Step [52/139], Loss: 0.0628\n",
      "Epoch [24/30], Step [53/139], Loss: 0.0840\n",
      "Epoch [24/30], Step [54/139], Loss: 0.0890\n",
      "Epoch [24/30], Step [55/139], Loss: 0.0746\n",
      "Epoch [24/30], Step [56/139], Loss: 0.0758\n",
      "Epoch [24/30], Step [57/139], Loss: 0.1072\n",
      "Epoch [24/30], Step [58/139], Loss: 0.0683\n",
      "Epoch [24/30], Step [59/139], Loss: 0.0636\n",
      "Epoch [24/30], Step [60/139], Loss: 0.0768\n",
      "Epoch [24/30], Step [61/139], Loss: 0.0899\n",
      "Epoch [24/30], Step [62/139], Loss: 0.0722\n",
      "Epoch [24/30], Step [63/139], Loss: 0.0518\n",
      "Epoch [24/30], Step [64/139], Loss: 0.0823\n",
      "Epoch [24/30], Step [65/139], Loss: 0.0871\n",
      "Epoch [24/30], Step [66/139], Loss: 0.0784\n",
      "Epoch [24/30], Step [67/139], Loss: 0.0989\n",
      "Epoch [24/30], Step [68/139], Loss: 0.0881\n",
      "Epoch [24/30], Step [69/139], Loss: 0.1005\n",
      "Epoch [24/30], Step [70/139], Loss: 0.0629\n",
      "Epoch [24/30], Step [71/139], Loss: 0.0786\n",
      "Epoch [24/30], Step [72/139], Loss: 0.0685\n",
      "Epoch [24/30], Step [73/139], Loss: 0.0598\n",
      "Epoch [24/30], Step [74/139], Loss: 0.0857\n",
      "Epoch [24/30], Step [75/139], Loss: 0.0935\n",
      "Epoch [24/30], Step [76/139], Loss: 0.0628\n",
      "Epoch [24/30], Step [77/139], Loss: 0.0525\n",
      "Epoch [24/30], Step [78/139], Loss: 0.0674\n",
      "Epoch [24/30], Step [79/139], Loss: 0.0815\n",
      "Epoch [24/30], Step [80/139], Loss: 0.0688\n",
      "Epoch [24/30], Step [81/139], Loss: 0.0811\n",
      "Epoch [24/30], Step [82/139], Loss: 0.0826\n",
      "Epoch [24/30], Step [83/139], Loss: 0.0955\n",
      "Epoch [24/30], Step [84/139], Loss: 0.0994\n",
      "Epoch [24/30], Step [85/139], Loss: 0.0640\n",
      "Epoch [24/30], Step [86/139], Loss: 0.0761\n",
      "Epoch [24/30], Step [87/139], Loss: 0.0723\n",
      "Epoch [24/30], Step [88/139], Loss: 0.0668\n",
      "Epoch [24/30], Step [89/139], Loss: 0.0972\n",
      "Epoch [24/30], Step [90/139], Loss: 0.0990\n",
      "Epoch [24/30], Step [91/139], Loss: 0.0659\n",
      "Epoch [24/30], Step [92/139], Loss: 0.0608\n",
      "Epoch [24/30], Step [93/139], Loss: 0.0866\n",
      "Epoch [24/30], Step [94/139], Loss: 0.0786\n",
      "Epoch [24/30], Step [95/139], Loss: 0.0654\n",
      "Epoch [24/30], Step [96/139], Loss: 0.0701\n",
      "Epoch [24/30], Step [97/139], Loss: 0.0936\n",
      "Epoch [24/30], Step [98/139], Loss: 0.1020\n",
      "Epoch [24/30], Step [99/139], Loss: 0.0793\n",
      "Epoch [24/30], Step [100/139], Loss: 0.0684\n",
      "Epoch [24/30], Step [101/139], Loss: 0.0533\n",
      "Epoch [24/30], Step [102/139], Loss: 0.0817\n",
      "Epoch [24/30], Step [103/139], Loss: 0.0913\n",
      "Epoch [24/30], Step [104/139], Loss: 0.0753\n",
      "Epoch [24/30], Step [105/139], Loss: 0.0950\n",
      "Epoch [24/30], Step [106/139], Loss: 0.0780\n",
      "Epoch [24/30], Step [107/139], Loss: 0.0582\n",
      "Epoch [24/30], Step [108/139], Loss: 0.0703\n",
      "Epoch [24/30], Step [109/139], Loss: 0.0639\n",
      "Epoch [24/30], Step [110/139], Loss: 0.0695\n",
      "Epoch [24/30], Step [111/139], Loss: 0.0843\n",
      "Epoch [24/30], Step [112/139], Loss: 0.0858\n",
      "Epoch [24/30], Step [113/139], Loss: 0.0997\n",
      "Epoch [24/30], Step [114/139], Loss: 0.0838\n",
      "Epoch [24/30], Step [115/139], Loss: 0.0633\n",
      "Epoch [24/30], Step [116/139], Loss: 0.0624\n",
      "Epoch [24/30], Step [117/139], Loss: 0.0662\n",
      "Epoch [24/30], Step [118/139], Loss: 0.0788\n",
      "Epoch [24/30], Step [119/139], Loss: 0.0571\n",
      "Epoch [24/30], Step [120/139], Loss: 0.0686\n",
      "Epoch [24/30], Step [121/139], Loss: 0.0523\n",
      "Epoch [24/30], Step [122/139], Loss: 0.0853\n",
      "Epoch [24/30], Step [123/139], Loss: 0.0712\n",
      "Epoch [24/30], Step [124/139], Loss: 0.0831\n",
      "Epoch [24/30], Step [125/139], Loss: 0.0655\n",
      "Epoch [24/30], Step [126/139], Loss: 0.0708\n",
      "Epoch [24/30], Step [127/139], Loss: 0.0701\n",
      "Epoch [24/30], Step [128/139], Loss: 0.1032\n",
      "Epoch [24/30], Step [129/139], Loss: 0.0563\n",
      "Epoch [24/30], Step [130/139], Loss: 0.0781\n",
      "Epoch [24/30], Step [131/139], Loss: 0.1002\n",
      "Epoch [24/30], Step [132/139], Loss: 0.0606\n",
      "Epoch [24/30], Step [133/139], Loss: 0.0889\n",
      "Epoch [24/30], Step [134/139], Loss: 0.0830\n",
      "Epoch [24/30], Step [135/139], Loss: 0.0826\n",
      "Epoch [24/30], Step [136/139], Loss: 0.0671\n",
      "Epoch [24/30], Step [137/139], Loss: 0.0669\n",
      "Epoch [24/30], Step [138/139], Loss: 0.0948\n",
      "Epoch [24/30], Step [139/139], Loss: 0.0748\n",
      "Start validation #24\n",
      "Validation #24  Average Loss: 0.0738\n",
      "Best performance at epoch: 24\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [25/30], Step [1/139], Loss: 0.0621\n",
      "Epoch [25/30], Step [2/139], Loss: 0.0776\n",
      "Epoch [25/30], Step [3/139], Loss: 0.0847\n",
      "Epoch [25/30], Step [4/139], Loss: 0.0846\n",
      "Epoch [25/30], Step [5/139], Loss: 0.0781\n",
      "Epoch [25/30], Step [6/139], Loss: 0.0638\n",
      "Epoch [25/30], Step [7/139], Loss: 0.0805\n",
      "Epoch [25/30], Step [8/139], Loss: 0.0757\n",
      "Epoch [25/30], Step [9/139], Loss: 0.0927\n",
      "Epoch [25/30], Step [10/139], Loss: 0.0802\n",
      "Epoch [25/30], Step [11/139], Loss: 0.0928\n",
      "Epoch [25/30], Step [12/139], Loss: 0.0745\n",
      "Epoch [25/30], Step [13/139], Loss: 0.0661\n",
      "Epoch [25/30], Step [14/139], Loss: 0.0672\n",
      "Epoch [25/30], Step [15/139], Loss: 0.0763\n",
      "Epoch [25/30], Step [16/139], Loss: 0.0817\n",
      "Epoch [25/30], Step [17/139], Loss: 0.1003\n",
      "Epoch [25/30], Step [18/139], Loss: 0.0796\n",
      "Epoch [25/30], Step [19/139], Loss: 0.0842\n",
      "Epoch [25/30], Step [20/139], Loss: 0.0573\n",
      "Epoch [25/30], Step [21/139], Loss: 0.0811\n",
      "Epoch [25/30], Step [22/139], Loss: 0.0730\n",
      "Epoch [25/30], Step [23/139], Loss: 0.0558\n",
      "Epoch [25/30], Step [24/139], Loss: 0.0663\n",
      "Epoch [25/30], Step [25/139], Loss: 0.0611\n",
      "Epoch [25/30], Step [26/139], Loss: 0.0715\n",
      "Epoch [25/30], Step [27/139], Loss: 0.0619\n",
      "Epoch [25/30], Step [28/139], Loss: 0.1064\n",
      "Epoch [25/30], Step [29/139], Loss: 0.0751\n",
      "Epoch [25/30], Step [30/139], Loss: 0.0790\n",
      "Epoch [25/30], Step [31/139], Loss: 0.0736\n",
      "Epoch [25/30], Step [32/139], Loss: 0.0645\n",
      "Epoch [25/30], Step [33/139], Loss: 0.0725\n",
      "Epoch [25/30], Step [34/139], Loss: 0.0691\n",
      "Epoch [25/30], Step [35/139], Loss: 0.0669\n",
      "Epoch [25/30], Step [36/139], Loss: 0.0972\n",
      "Epoch [25/30], Step [37/139], Loss: 0.0767\n",
      "Epoch [25/30], Step [38/139], Loss: 0.0766\n",
      "Epoch [25/30], Step [39/139], Loss: 0.0575\n",
      "Epoch [25/30], Step [40/139], Loss: 0.0807\n",
      "Epoch [25/30], Step [41/139], Loss: 0.0800\n",
      "Epoch [25/30], Step [42/139], Loss: 0.0581\n",
      "Epoch [25/30], Step [43/139], Loss: 0.0892\n",
      "Epoch [25/30], Step [44/139], Loss: 0.0568\n",
      "Epoch [25/30], Step [45/139], Loss: 0.0886\n",
      "Epoch [25/30], Step [46/139], Loss: 0.0712\n",
      "Epoch [25/30], Step [47/139], Loss: 0.0865\n",
      "Epoch [25/30], Step [48/139], Loss: 0.0800\n",
      "Epoch [25/30], Step [49/139], Loss: 0.0907\n",
      "Epoch [25/30], Step [50/139], Loss: 0.0892\n",
      "Epoch [25/30], Step [51/139], Loss: 0.0854\n",
      "Epoch [25/30], Step [52/139], Loss: 0.0804\n",
      "Epoch [25/30], Step [53/139], Loss: 0.0707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/30], Step [54/139], Loss: 0.0924\n",
      "Epoch [25/30], Step [55/139], Loss: 0.0751\n",
      "Epoch [25/30], Step [56/139], Loss: 0.0553\n",
      "Epoch [25/30], Step [57/139], Loss: 0.0661\n",
      "Epoch [25/30], Step [58/139], Loss: 0.0774\n",
      "Epoch [25/30], Step [59/139], Loss: 0.0647\n",
      "Epoch [25/30], Step [60/139], Loss: 0.0761\n",
      "Epoch [25/30], Step [61/139], Loss: 0.0711\n",
      "Epoch [25/30], Step [62/139], Loss: 0.0739\n",
      "Epoch [25/30], Step [63/139], Loss: 0.0729\n",
      "Epoch [25/30], Step [64/139], Loss: 0.0828\n",
      "Epoch [25/30], Step [65/139], Loss: 0.0636\n",
      "Epoch [25/30], Step [66/139], Loss: 0.0590\n",
      "Epoch [25/30], Step [67/139], Loss: 0.0969\n",
      "Epoch [25/30], Step [68/139], Loss: 0.0585\n",
      "Epoch [25/30], Step [69/139], Loss: 0.0634\n",
      "Epoch [25/30], Step [70/139], Loss: 0.0775\n",
      "Epoch [25/30], Step [71/139], Loss: 0.0972\n",
      "Epoch [25/30], Step [72/139], Loss: 0.0813\n",
      "Epoch [25/30], Step [73/139], Loss: 0.0889\n",
      "Epoch [25/30], Step [74/139], Loss: 0.1522\n",
      "Epoch [25/30], Step [75/139], Loss: 0.0791\n",
      "Epoch [25/30], Step [76/139], Loss: 0.1034\n",
      "Epoch [25/30], Step [77/139], Loss: 0.0764\n",
      "Epoch [25/30], Step [78/139], Loss: 0.0703\n",
      "Epoch [25/30], Step [79/139], Loss: 0.0645\n",
      "Epoch [25/30], Step [80/139], Loss: 0.0594\n",
      "Epoch [25/30], Step [81/139], Loss: 0.0779\n",
      "Epoch [25/30], Step [82/139], Loss: 0.0729\n",
      "Epoch [25/30], Step [83/139], Loss: 0.0800\n",
      "Epoch [25/30], Step [84/139], Loss: 0.0751\n",
      "Epoch [25/30], Step [85/139], Loss: 0.0742\n",
      "Epoch [25/30], Step [86/139], Loss: 0.1070\n",
      "Epoch [25/30], Step [87/139], Loss: 0.0661\n",
      "Epoch [25/30], Step [88/139], Loss: 0.0826\n",
      "Epoch [25/30], Step [89/139], Loss: 0.0811\n",
      "Epoch [25/30], Step [90/139], Loss: 0.0734\n",
      "Epoch [25/30], Step [91/139], Loss: 0.0688\n",
      "Epoch [25/30], Step [92/139], Loss: 0.0739\n",
      "Epoch [25/30], Step [93/139], Loss: 0.0913\n",
      "Epoch [25/30], Step [94/139], Loss: 0.0617\n",
      "Epoch [25/30], Step [95/139], Loss: 0.0746\n",
      "Epoch [25/30], Step [96/139], Loss: 0.0782\n",
      "Epoch [25/30], Step [97/139], Loss: 0.0659\n",
      "Epoch [25/30], Step [98/139], Loss: 0.0712\n",
      "Epoch [25/30], Step [99/139], Loss: 0.0594\n",
      "Epoch [25/30], Step [100/139], Loss: 0.0623\n",
      "Epoch [25/30], Step [101/139], Loss: 0.0628\n",
      "Epoch [25/30], Step [102/139], Loss: 0.0529\n",
      "Epoch [25/30], Step [103/139], Loss: 0.0846\n",
      "Epoch [25/30], Step [104/139], Loss: 0.0742\n",
      "Epoch [25/30], Step [105/139], Loss: 0.1080\n",
      "Epoch [25/30], Step [106/139], Loss: 0.0813\n",
      "Epoch [25/30], Step [107/139], Loss: 0.0523\n",
      "Epoch [25/30], Step [108/139], Loss: 0.0861\n",
      "Epoch [25/30], Step [109/139], Loss: 0.0708\n",
      "Epoch [25/30], Step [110/139], Loss: 0.0653\n",
      "Epoch [25/30], Step [111/139], Loss: 0.0926\n",
      "Epoch [25/30], Step [112/139], Loss: 0.0899\n",
      "Epoch [25/30], Step [113/139], Loss: 0.0602\n",
      "Epoch [25/30], Step [114/139], Loss: 0.0850\n",
      "Epoch [25/30], Step [115/139], Loss: 0.0658\n",
      "Epoch [25/30], Step [116/139], Loss: 0.0701\n",
      "Epoch [25/30], Step [117/139], Loss: 0.0858\n",
      "Epoch [25/30], Step [118/139], Loss: 0.0868\n",
      "Epoch [25/30], Step [119/139], Loss: 0.1022\n",
      "Epoch [25/30], Step [120/139], Loss: 0.0810\n",
      "Epoch [25/30], Step [121/139], Loss: 0.0594\n",
      "Epoch [25/30], Step [122/139], Loss: 0.1057\n",
      "Epoch [25/30], Step [123/139], Loss: 0.0583\n",
      "Epoch [25/30], Step [124/139], Loss: 0.0826\n",
      "Epoch [25/30], Step [125/139], Loss: 0.0756\n",
      "Epoch [25/30], Step [126/139], Loss: 0.0682\n",
      "Epoch [25/30], Step [127/139], Loss: 0.0662\n",
      "Epoch [25/30], Step [128/139], Loss: 0.0710\n",
      "Epoch [25/30], Step [129/139], Loss: 0.0610\n",
      "Epoch [25/30], Step [130/139], Loss: 0.0611\n",
      "Epoch [25/30], Step [131/139], Loss: 0.1063\n",
      "Epoch [25/30], Step [132/139], Loss: 0.0840\n",
      "Epoch [25/30], Step [133/139], Loss: 0.0703\n",
      "Epoch [25/30], Step [134/139], Loss: 0.0818\n",
      "Epoch [25/30], Step [135/139], Loss: 0.0717\n",
      "Epoch [25/30], Step [136/139], Loss: 0.0990\n",
      "Epoch [25/30], Step [137/139], Loss: 0.0609\n",
      "Epoch [25/30], Step [138/139], Loss: 0.0859\n",
      "Epoch [25/30], Step [139/139], Loss: 0.0772\n",
      "Start validation #25\n",
      "Validation #25  Average Loss: 0.0739\n",
      "Epoch [26/30], Step [1/139], Loss: 0.0590\n",
      "Epoch [26/30], Step [2/139], Loss: 0.0675\n",
      "Epoch [26/30], Step [3/139], Loss: 0.0572\n",
      "Epoch [26/30], Step [4/139], Loss: 0.0698\n",
      "Epoch [26/30], Step [5/139], Loss: 0.0759\n",
      "Epoch [26/30], Step [6/139], Loss: 0.0884\n",
      "Epoch [26/30], Step [7/139], Loss: 0.0753\n",
      "Epoch [26/30], Step [8/139], Loss: 0.0885\n",
      "Epoch [26/30], Step [9/139], Loss: 0.0740\n",
      "Epoch [26/30], Step [10/139], Loss: 0.0810\n",
      "Epoch [26/30], Step [11/139], Loss: 0.0741\n",
      "Epoch [26/30], Step [12/139], Loss: 0.0785\n",
      "Epoch [26/30], Step [13/139], Loss: 0.0673\n",
      "Epoch [26/30], Step [14/139], Loss: 0.0690\n",
      "Epoch [26/30], Step [15/139], Loss: 0.0803\n",
      "Epoch [26/30], Step [16/139], Loss: 0.1172\n",
      "Epoch [26/30], Step [17/139], Loss: 0.0652\n",
      "Epoch [26/30], Step [18/139], Loss: 0.0596\n",
      "Epoch [26/30], Step [19/139], Loss: 0.0885\n",
      "Epoch [26/30], Step [20/139], Loss: 0.0627\n",
      "Epoch [26/30], Step [21/139], Loss: 0.0532\n",
      "Epoch [26/30], Step [22/139], Loss: 0.0825\n",
      "Epoch [26/30], Step [23/139], Loss: 0.0681\n",
      "Epoch [26/30], Step [24/139], Loss: 0.0806\n",
      "Epoch [26/30], Step [25/139], Loss: 0.0806\n",
      "Epoch [26/30], Step [26/139], Loss: 0.0909\n",
      "Epoch [26/30], Step [27/139], Loss: 0.0669\n",
      "Epoch [26/30], Step [28/139], Loss: 0.0807\n",
      "Epoch [26/30], Step [29/139], Loss: 0.0524\n",
      "Epoch [26/30], Step [30/139], Loss: 0.0838\n",
      "Epoch [26/30], Step [31/139], Loss: 0.0639\n",
      "Epoch [26/30], Step [32/139], Loss: 0.0781\n",
      "Epoch [26/30], Step [33/139], Loss: 0.0695\n",
      "Epoch [26/30], Step [34/139], Loss: 0.0893\n",
      "Epoch [26/30], Step [35/139], Loss: 0.0540\n",
      "Epoch [26/30], Step [36/139], Loss: 0.0658\n",
      "Epoch [26/30], Step [37/139], Loss: 0.0960\n",
      "Epoch [26/30], Step [38/139], Loss: 0.0714\n",
      "Epoch [26/30], Step [39/139], Loss: 0.0637\n",
      "Epoch [26/30], Step [40/139], Loss: 0.0823\n",
      "Epoch [26/30], Step [41/139], Loss: 0.0911\n",
      "Epoch [26/30], Step [42/139], Loss: 0.0876\n",
      "Epoch [26/30], Step [43/139], Loss: 0.0544\n",
      "Epoch [26/30], Step [44/139], Loss: 0.0744\n",
      "Epoch [26/30], Step [45/139], Loss: 0.0886\n",
      "Epoch [26/30], Step [46/139], Loss: 0.0755\n",
      "Epoch [26/30], Step [47/139], Loss: 0.0745\n",
      "Epoch [26/30], Step [48/139], Loss: 0.0800\n",
      "Epoch [26/30], Step [49/139], Loss: 0.0978\n",
      "Epoch [26/30], Step [50/139], Loss: 0.0829\n",
      "Epoch [26/30], Step [51/139], Loss: 0.1024\n",
      "Epoch [26/30], Step [52/139], Loss: 0.0759\n",
      "Epoch [26/30], Step [53/139], Loss: 0.0823\n",
      "Epoch [26/30], Step [54/139], Loss: 0.0673\n",
      "Epoch [26/30], Step [55/139], Loss: 0.0655\n",
      "Epoch [26/30], Step [56/139], Loss: 0.0688\n",
      "Epoch [26/30], Step [57/139], Loss: 0.0743\n",
      "Epoch [26/30], Step [58/139], Loss: 0.0933\n",
      "Epoch [26/30], Step [59/139], Loss: 0.0914\n",
      "Epoch [26/30], Step [60/139], Loss: 0.0654\n",
      "Epoch [26/30], Step [61/139], Loss: 0.0517\n",
      "Epoch [26/30], Step [62/139], Loss: 0.0812\n",
      "Epoch [26/30], Step [63/139], Loss: 0.0847\n",
      "Epoch [26/30], Step [64/139], Loss: 0.0835\n",
      "Epoch [26/30], Step [65/139], Loss: 0.0863\n",
      "Epoch [26/30], Step [66/139], Loss: 0.0669\n",
      "Epoch [26/30], Step [67/139], Loss: 0.0906\n",
      "Epoch [26/30], Step [68/139], Loss: 0.0963\n",
      "Epoch [26/30], Step [69/139], Loss: 0.0550\n",
      "Epoch [26/30], Step [70/139], Loss: 0.0689\n",
      "Epoch [26/30], Step [71/139], Loss: 0.0987\n",
      "Epoch [26/30], Step [72/139], Loss: 0.0926\n",
      "Epoch [26/30], Step [73/139], Loss: 0.0663\n",
      "Epoch [26/30], Step [74/139], Loss: 0.0619\n",
      "Epoch [26/30], Step [75/139], Loss: 0.0656\n",
      "Epoch [26/30], Step [76/139], Loss: 0.0579\n",
      "Epoch [26/30], Step [77/139], Loss: 0.0718\n",
      "Epoch [26/30], Step [78/139], Loss: 0.0645\n",
      "Epoch [26/30], Step [79/139], Loss: 0.0581\n",
      "Epoch [26/30], Step [80/139], Loss: 0.0914\n",
      "Epoch [26/30], Step [81/139], Loss: 0.0811\n",
      "Epoch [26/30], Step [82/139], Loss: 0.0736\n",
      "Epoch [26/30], Step [83/139], Loss: 0.0574\n",
      "Epoch [26/30], Step [84/139], Loss: 0.0941\n",
      "Epoch [26/30], Step [85/139], Loss: 0.1477\n",
      "Epoch [26/30], Step [86/139], Loss: 0.0538\n",
      "Epoch [26/30], Step [87/139], Loss: 0.0824\n",
      "Epoch [26/30], Step [88/139], Loss: 0.0757\n",
      "Epoch [26/30], Step [89/139], Loss: 0.0916\n",
      "Epoch [26/30], Step [90/139], Loss: 0.0711\n",
      "Epoch [26/30], Step [91/139], Loss: 0.0693\n",
      "Epoch [26/30], Step [92/139], Loss: 0.1107\n",
      "Epoch [26/30], Step [93/139], Loss: 0.0570\n",
      "Epoch [26/30], Step [94/139], Loss: 0.0911\n",
      "Epoch [26/30], Step [95/139], Loss: 0.0632\n",
      "Epoch [26/30], Step [96/139], Loss: 0.0779\n",
      "Epoch [26/30], Step [97/139], Loss: 0.0599\n",
      "Epoch [26/30], Step [98/139], Loss: 0.0979\n",
      "Epoch [26/30], Step [99/139], Loss: 0.0666\n",
      "Epoch [26/30], Step [100/139], Loss: 0.0821\n",
      "Epoch [26/30], Step [101/139], Loss: 0.0580\n",
      "Epoch [26/30], Step [102/139], Loss: 0.0576\n",
      "Epoch [26/30], Step [103/139], Loss: 0.0587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/30], Step [104/139], Loss: 0.0864\n",
      "Epoch [26/30], Step [105/139], Loss: 0.0928\n",
      "Epoch [26/30], Step [106/139], Loss: 0.0753\n",
      "Epoch [26/30], Step [107/139], Loss: 0.0614\n",
      "Epoch [26/30], Step [108/139], Loss: 0.0608\n",
      "Epoch [26/30], Step [109/139], Loss: 0.1105\n",
      "Epoch [26/30], Step [110/139], Loss: 0.0791\n",
      "Epoch [26/30], Step [111/139], Loss: 0.0845\n",
      "Epoch [26/30], Step [112/139], Loss: 0.0763\n",
      "Epoch [26/30], Step [113/139], Loss: 0.0860\n",
      "Epoch [26/30], Step [114/139], Loss: 0.0694\n",
      "Epoch [26/30], Step [115/139], Loss: 0.0810\n",
      "Epoch [26/30], Step [116/139], Loss: 0.1087\n",
      "Epoch [26/30], Step [117/139], Loss: 0.0751\n",
      "Epoch [26/30], Step [118/139], Loss: 0.0597\n",
      "Epoch [26/30], Step [119/139], Loss: 0.0642\n",
      "Epoch [26/30], Step [120/139], Loss: 0.0606\n",
      "Epoch [26/30], Step [121/139], Loss: 0.0793\n",
      "Epoch [26/30], Step [122/139], Loss: 0.0492\n",
      "Epoch [26/30], Step [123/139], Loss: 0.0602\n",
      "Epoch [26/30], Step [124/139], Loss: 0.0910\n",
      "Epoch [26/30], Step [125/139], Loss: 0.0799\n",
      "Epoch [26/30], Step [126/139], Loss: 0.0917\n",
      "Epoch [26/30], Step [127/139], Loss: 0.0851\n",
      "Epoch [26/30], Step [128/139], Loss: 0.1136\n",
      "Epoch [26/30], Step [129/139], Loss: 0.0573\n",
      "Epoch [26/30], Step [130/139], Loss: 0.0553\n",
      "Epoch [26/30], Step [131/139], Loss: 0.0701\n",
      "Epoch [26/30], Step [132/139], Loss: 0.0772\n",
      "Epoch [26/30], Step [133/139], Loss: 0.0641\n",
      "Epoch [26/30], Step [134/139], Loss: 0.0803\n",
      "Epoch [26/30], Step [135/139], Loss: 0.0827\n",
      "Epoch [26/30], Step [136/139], Loss: 0.0497\n",
      "Epoch [26/30], Step [137/139], Loss: 0.0723\n",
      "Epoch [26/30], Step [138/139], Loss: 0.0794\n",
      "Epoch [26/30], Step [139/139], Loss: 0.0743\n",
      "Start validation #26\n",
      "Validation #26  Average Loss: 0.0724\n",
      "Best performance at epoch: 26\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [27/30], Step [1/139], Loss: 0.0587\n",
      "Epoch [27/30], Step [2/139], Loss: 0.0604\n",
      "Epoch [27/30], Step [3/139], Loss: 0.0859\n",
      "Epoch [27/30], Step [4/139], Loss: 0.0726\n",
      "Epoch [27/30], Step [5/139], Loss: 0.1086\n",
      "Epoch [27/30], Step [6/139], Loss: 0.0852\n",
      "Epoch [27/30], Step [7/139], Loss: 0.0872\n",
      "Epoch [27/30], Step [8/139], Loss: 0.0719\n",
      "Epoch [27/30], Step [9/139], Loss: 0.0730\n",
      "Epoch [27/30], Step [10/139], Loss: 0.1032\n",
      "Epoch [27/30], Step [11/139], Loss: 0.0576\n",
      "Epoch [27/30], Step [12/139], Loss: 0.0771\n",
      "Epoch [27/30], Step [13/139], Loss: 0.0691\n",
      "Epoch [27/30], Step [14/139], Loss: 0.0740\n",
      "Epoch [27/30], Step [15/139], Loss: 0.0462\n",
      "Epoch [27/30], Step [16/139], Loss: 0.0894\n",
      "Epoch [27/30], Step [17/139], Loss: 0.0735\n",
      "Epoch [27/30], Step [18/139], Loss: 0.0883\n",
      "Epoch [27/30], Step [19/139], Loss: 0.0717\n",
      "Epoch [27/30], Step [20/139], Loss: 0.0860\n",
      "Epoch [27/30], Step [21/139], Loss: 0.0822\n",
      "Epoch [27/30], Step [22/139], Loss: 0.0726\n",
      "Epoch [27/30], Step [23/139], Loss: 0.1179\n",
      "Epoch [27/30], Step [24/139], Loss: 0.0480\n",
      "Epoch [27/30], Step [25/139], Loss: 0.0830\n",
      "Epoch [27/30], Step [26/139], Loss: 0.0695\n",
      "Epoch [27/30], Step [27/139], Loss: 0.0619\n",
      "Epoch [27/30], Step [28/139], Loss: 0.0505\n",
      "Epoch [27/30], Step [29/139], Loss: 0.0619\n",
      "Epoch [27/30], Step [30/139], Loss: 0.0693\n",
      "Epoch [27/30], Step [31/139], Loss: 0.0674\n",
      "Epoch [27/30], Step [32/139], Loss: 0.0850\n",
      "Epoch [27/30], Step [33/139], Loss: 0.0902\n",
      "Epoch [27/30], Step [34/139], Loss: 0.0735\n",
      "Epoch [27/30], Step [35/139], Loss: 0.0632\n",
      "Epoch [27/30], Step [36/139], Loss: 0.0745\n",
      "Epoch [27/30], Step [37/139], Loss: 0.0688\n",
      "Epoch [27/30], Step [38/139], Loss: 0.0894\n",
      "Epoch [27/30], Step [39/139], Loss: 0.0786\n",
      "Epoch [27/30], Step [40/139], Loss: 0.0615\n",
      "Epoch [27/30], Step [41/139], Loss: 0.0713\n",
      "Epoch [27/30], Step [42/139], Loss: 0.0473\n",
      "Epoch [27/30], Step [43/139], Loss: 0.0577\n",
      "Epoch [27/30], Step [44/139], Loss: 0.0708\n",
      "Epoch [27/30], Step [45/139], Loss: 0.1313\n",
      "Epoch [27/30], Step [46/139], Loss: 0.0735\n",
      "Epoch [27/30], Step [47/139], Loss: 0.0936\n",
      "Epoch [27/30], Step [48/139], Loss: 0.0775\n",
      "Epoch [27/30], Step [49/139], Loss: 0.0799\n",
      "Epoch [27/30], Step [50/139], Loss: 0.0904\n",
      "Epoch [27/30], Step [51/139], Loss: 0.0873\n",
      "Epoch [27/30], Step [52/139], Loss: 0.0749\n",
      "Epoch [27/30], Step [53/139], Loss: 0.0956\n",
      "Epoch [27/30], Step [54/139], Loss: 0.0578\n",
      "Epoch [27/30], Step [55/139], Loss: 0.0603\n",
      "Epoch [27/30], Step [56/139], Loss: 0.0706\n",
      "Epoch [27/30], Step [57/139], Loss: 0.0865\n",
      "Epoch [27/30], Step [58/139], Loss: 0.0515\n",
      "Epoch [27/30], Step [59/139], Loss: 0.0841\n",
      "Epoch [27/30], Step [60/139], Loss: 0.0532\n",
      "Epoch [27/30], Step [61/139], Loss: 0.0734\n",
      "Epoch [27/30], Step [62/139], Loss: 0.0793\n",
      "Epoch [27/30], Step [63/139], Loss: 0.0726\n",
      "Epoch [27/30], Step [64/139], Loss: 0.0502\n",
      "Epoch [27/30], Step [65/139], Loss: 0.0628\n",
      "Epoch [27/30], Step [66/139], Loss: 0.0682\n",
      "Epoch [27/30], Step [67/139], Loss: 0.0613\n",
      "Epoch [27/30], Step [68/139], Loss: 0.0790\n",
      "Epoch [27/30], Step [69/139], Loss: 0.0611\n",
      "Epoch [27/30], Step [70/139], Loss: 0.0837\n",
      "Epoch [27/30], Step [71/139], Loss: 0.0653\n",
      "Epoch [27/30], Step [72/139], Loss: 0.0695\n",
      "Epoch [27/30], Step [73/139], Loss: 0.0984\n",
      "Epoch [27/30], Step [74/139], Loss: 0.0783\n",
      "Epoch [27/30], Step [75/139], Loss: 0.0836\n",
      "Epoch [27/30], Step [76/139], Loss: 0.0841\n",
      "Epoch [27/30], Step [77/139], Loss: 0.0600\n",
      "Epoch [27/30], Step [78/139], Loss: 0.0537\n",
      "Epoch [27/30], Step [79/139], Loss: 0.0980\n",
      "Epoch [27/30], Step [80/139], Loss: 0.0881\n",
      "Epoch [27/30], Step [81/139], Loss: 0.0620\n",
      "Epoch [27/30], Step [82/139], Loss: 0.0743\n",
      "Epoch [27/30], Step [83/139], Loss: 0.0849\n",
      "Epoch [27/30], Step [84/139], Loss: 0.0639\n",
      "Epoch [27/30], Step [85/139], Loss: 0.0686\n",
      "Epoch [27/30], Step [86/139], Loss: 0.0784\n",
      "Epoch [27/30], Step [87/139], Loss: 0.0702\n",
      "Epoch [27/30], Step [88/139], Loss: 0.0682\n",
      "Epoch [27/30], Step [89/139], Loss: 0.0799\n",
      "Epoch [27/30], Step [90/139], Loss: 0.0723\n",
      "Epoch [27/30], Step [91/139], Loss: 0.1012\n",
      "Epoch [27/30], Step [92/139], Loss: 0.1171\n",
      "Epoch [27/30], Step [93/139], Loss: 0.0960\n",
      "Epoch [27/30], Step [94/139], Loss: 0.0553\n",
      "Epoch [27/30], Step [95/139], Loss: 0.0742\n",
      "Epoch [27/30], Step [96/139], Loss: 0.0875\n",
      "Epoch [27/30], Step [97/139], Loss: 0.0725\n",
      "Epoch [27/30], Step [98/139], Loss: 0.0820\n",
      "Epoch [27/30], Step [99/139], Loss: 0.0637\n",
      "Epoch [27/30], Step [100/139], Loss: 0.0730\n",
      "Epoch [27/30], Step [101/139], Loss: 0.0738\n",
      "Epoch [27/30], Step [102/139], Loss: 0.0655\n",
      "Epoch [27/30], Step [103/139], Loss: 0.0626\n",
      "Epoch [27/30], Step [104/139], Loss: 0.0703\n",
      "Epoch [27/30], Step [105/139], Loss: 0.0703\n",
      "Epoch [27/30], Step [106/139], Loss: 0.0860\n",
      "Epoch [27/30], Step [107/139], Loss: 0.0952\n",
      "Epoch [27/30], Step [108/139], Loss: 0.0911\n",
      "Epoch [27/30], Step [109/139], Loss: 0.0722\n",
      "Epoch [27/30], Step [110/139], Loss: 0.0481\n",
      "Epoch [27/30], Step [111/139], Loss: 0.0603\n",
      "Epoch [27/30], Step [112/139], Loss: 0.0761\n",
      "Epoch [27/30], Step [113/139], Loss: 0.0904\n",
      "Epoch [27/30], Step [114/139], Loss: 0.0576\n",
      "Epoch [27/30], Step [115/139], Loss: 0.0692\n",
      "Epoch [27/30], Step [116/139], Loss: 0.0964\n",
      "Epoch [27/30], Step [117/139], Loss: 0.0889\n",
      "Epoch [27/30], Step [118/139], Loss: 0.0833\n",
      "Epoch [27/30], Step [119/139], Loss: 0.0767\n",
      "Epoch [27/30], Step [120/139], Loss: 0.0820\n",
      "Epoch [27/30], Step [121/139], Loss: 0.0679\n",
      "Epoch [27/30], Step [122/139], Loss: 0.0723\n",
      "Epoch [27/30], Step [123/139], Loss: 0.1301\n",
      "Epoch [27/30], Step [124/139], Loss: 0.0675\n",
      "Epoch [27/30], Step [125/139], Loss: 0.0825\n",
      "Epoch [27/30], Step [126/139], Loss: 0.0591\n",
      "Epoch [27/30], Step [127/139], Loss: 0.0757\n",
      "Epoch [27/30], Step [128/139], Loss: 0.0861\n",
      "Epoch [27/30], Step [129/139], Loss: 0.0715\n",
      "Epoch [27/30], Step [130/139], Loss: 0.0526\n",
      "Epoch [27/30], Step [131/139], Loss: 0.0933\n",
      "Epoch [27/30], Step [132/139], Loss: 0.0637\n",
      "Epoch [27/30], Step [133/139], Loss: 0.0998\n",
      "Epoch [27/30], Step [134/139], Loss: 0.0765\n",
      "Epoch [27/30], Step [135/139], Loss: 0.0763\n",
      "Epoch [27/30], Step [136/139], Loss: 0.0894\n",
      "Epoch [27/30], Step [137/139], Loss: 0.0535\n",
      "Epoch [27/30], Step [138/139], Loss: 0.0842\n",
      "Epoch [27/30], Step [139/139], Loss: 0.0591\n",
      "Start validation #27\n",
      "Validation #27  Average Loss: 0.0726\n",
      "Epoch [28/30], Step [1/139], Loss: 0.0677\n",
      "Epoch [28/30], Step [2/139], Loss: 0.0739\n",
      "Epoch [28/30], Step [3/139], Loss: 0.0932\n",
      "Epoch [28/30], Step [4/139], Loss: 0.0736\n",
      "Epoch [28/30], Step [5/139], Loss: 0.0720\n",
      "Epoch [28/30], Step [6/139], Loss: 0.0808\n",
      "Epoch [28/30], Step [7/139], Loss: 0.0729\n",
      "Epoch [28/30], Step [8/139], Loss: 0.0589\n",
      "Epoch [28/30], Step [9/139], Loss: 0.0558\n",
      "Epoch [28/30], Step [10/139], Loss: 0.0878\n",
      "Epoch [28/30], Step [11/139], Loss: 0.0902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/30], Step [12/139], Loss: 0.0651\n",
      "Epoch [28/30], Step [13/139], Loss: 0.0865\n",
      "Epoch [28/30], Step [14/139], Loss: 0.0741\n",
      "Epoch [28/30], Step [15/139], Loss: 0.0602\n",
      "Epoch [28/30], Step [16/139], Loss: 0.0812\n",
      "Epoch [28/30], Step [17/139], Loss: 0.0751\n",
      "Epoch [28/30], Step [18/139], Loss: 0.0834\n",
      "Epoch [28/30], Step [19/139], Loss: 0.0779\n",
      "Epoch [28/30], Step [20/139], Loss: 0.0834\n",
      "Epoch [28/30], Step [21/139], Loss: 0.0765\n",
      "Epoch [28/30], Step [22/139], Loss: 0.0807\n",
      "Epoch [28/30], Step [23/139], Loss: 0.0466\n",
      "Epoch [28/30], Step [24/139], Loss: 0.0580\n",
      "Epoch [28/30], Step [25/139], Loss: 0.0825\n",
      "Epoch [28/30], Step [26/139], Loss: 0.0578\n",
      "Epoch [28/30], Step [27/139], Loss: 0.0651\n",
      "Epoch [28/30], Step [28/139], Loss: 0.0494\n",
      "Epoch [28/30], Step [29/139], Loss: 0.1063\n",
      "Epoch [28/30], Step [30/139], Loss: 0.0835\n",
      "Epoch [28/30], Step [31/139], Loss: 0.0703\n",
      "Epoch [28/30], Step [32/139], Loss: 0.0437\n",
      "Epoch [28/30], Step [33/139], Loss: 0.0951\n",
      "Epoch [28/30], Step [34/139], Loss: 0.0819\n",
      "Epoch [28/30], Step [35/139], Loss: 0.1010\n",
      "Epoch [28/30], Step [36/139], Loss: 0.0791\n",
      "Epoch [28/30], Step [37/139], Loss: 0.0733\n",
      "Epoch [28/30], Step [38/139], Loss: 0.1036\n",
      "Epoch [28/30], Step [39/139], Loss: 0.0617\n",
      "Epoch [28/30], Step [40/139], Loss: 0.0683\n",
      "Epoch [28/30], Step [41/139], Loss: 0.0846\n",
      "Epoch [28/30], Step [42/139], Loss: 0.0577\n",
      "Epoch [28/30], Step [43/139], Loss: 0.0827\n",
      "Epoch [28/30], Step [44/139], Loss: 0.0800\n",
      "Epoch [28/30], Step [45/139], Loss: 0.1101\n",
      "Epoch [28/30], Step [46/139], Loss: 0.0703\n",
      "Epoch [28/30], Step [47/139], Loss: 0.0999\n",
      "Epoch [28/30], Step [48/139], Loss: 0.0895\n",
      "Epoch [28/30], Step [49/139], Loss: 0.0795\n",
      "Epoch [28/30], Step [50/139], Loss: 0.0775\n",
      "Epoch [28/30], Step [51/139], Loss: 0.0586\n",
      "Epoch [28/30], Step [52/139], Loss: 0.0559\n",
      "Epoch [28/30], Step [53/139], Loss: 0.0759\n",
      "Epoch [28/30], Step [54/139], Loss: 0.0732\n",
      "Epoch [28/30], Step [55/139], Loss: 0.0675\n",
      "Epoch [28/30], Step [56/139], Loss: 0.0862\n",
      "Epoch [28/30], Step [57/139], Loss: 0.0844\n",
      "Epoch [28/30], Step [58/139], Loss: 0.0670\n",
      "Epoch [28/30], Step [59/139], Loss: 0.0660\n",
      "Epoch [28/30], Step [60/139], Loss: 0.0674\n",
      "Epoch [28/30], Step [61/139], Loss: 0.1044\n",
      "Epoch [28/30], Step [62/139], Loss: 0.0813\n",
      "Epoch [28/30], Step [63/139], Loss: 0.0782\n",
      "Epoch [28/30], Step [64/139], Loss: 0.0717\n",
      "Epoch [28/30], Step [65/139], Loss: 0.0567\n",
      "Epoch [28/30], Step [66/139], Loss: 0.0926\n",
      "Epoch [28/30], Step [67/139], Loss: 0.0861\n",
      "Epoch [28/30], Step [68/139], Loss: 0.0906\n",
      "Epoch [28/30], Step [69/139], Loss: 0.0520\n",
      "Epoch [28/30], Step [70/139], Loss: 0.0658\n",
      "Epoch [28/30], Step [71/139], Loss: 0.0854\n",
      "Epoch [28/30], Step [72/139], Loss: 0.0475\n",
      "Epoch [28/30], Step [73/139], Loss: 0.1114\n",
      "Epoch [28/30], Step [74/139], Loss: 0.0773\n",
      "Epoch [28/30], Step [75/139], Loss: 0.0798\n",
      "Epoch [28/30], Step [76/139], Loss: 0.0543\n",
      "Epoch [28/30], Step [77/139], Loss: 0.1472\n",
      "Epoch [28/30], Step [78/139], Loss: 0.0748\n",
      "Epoch [28/30], Step [79/139], Loss: 0.0624\n",
      "Epoch [28/30], Step [80/139], Loss: 0.0858\n",
      "Epoch [28/30], Step [81/139], Loss: 0.1002\n",
      "Epoch [28/30], Step [82/139], Loss: 0.0632\n",
      "Epoch [28/30], Step [83/139], Loss: 0.0781\n",
      "Epoch [28/30], Step [84/139], Loss: 0.0838\n",
      "Epoch [28/30], Step [85/139], Loss: 0.0854\n",
      "Epoch [28/30], Step [86/139], Loss: 0.0658\n",
      "Epoch [28/30], Step [87/139], Loss: 0.0689\n",
      "Epoch [28/30], Step [88/139], Loss: 0.0889\n",
      "Epoch [28/30], Step [89/139], Loss: 0.0565\n",
      "Epoch [28/30], Step [90/139], Loss: 0.0563\n",
      "Epoch [28/30], Step [91/139], Loss: 0.0658\n",
      "Epoch [28/30], Step [92/139], Loss: 0.0552\n",
      "Epoch [28/30], Step [93/139], Loss: 0.0682\n",
      "Epoch [28/30], Step [94/139], Loss: 0.0548\n",
      "Epoch [28/30], Step [95/139], Loss: 0.0756\n",
      "Epoch [28/30], Step [96/139], Loss: 0.0805\n",
      "Epoch [28/30], Step [97/139], Loss: 0.0593\n",
      "Epoch [28/30], Step [98/139], Loss: 0.0767\n",
      "Epoch [28/30], Step [99/139], Loss: 0.0620\n",
      "Epoch [28/30], Step [100/139], Loss: 0.0638\n",
      "Epoch [28/30], Step [101/139], Loss: 0.0899\n",
      "Epoch [28/30], Step [102/139], Loss: 0.0484\n",
      "Epoch [28/30], Step [103/139], Loss: 0.0543\n",
      "Epoch [28/30], Step [104/139], Loss: 0.0805\n",
      "Epoch [28/30], Step [105/139], Loss: 0.0910\n",
      "Epoch [28/30], Step [106/139], Loss: 0.0724\n",
      "Epoch [28/30], Step [107/139], Loss: 0.0867\n",
      "Epoch [28/30], Step [108/139], Loss: 0.0821\n",
      "Epoch [28/30], Step [109/139], Loss: 0.0869\n",
      "Epoch [28/30], Step [110/139], Loss: 0.0881\n",
      "Epoch [28/30], Step [111/139], Loss: 0.0867\n",
      "Epoch [28/30], Step [112/139], Loss: 0.0744\n",
      "Epoch [28/30], Step [113/139], Loss: 0.0547\n",
      "Epoch [28/30], Step [114/139], Loss: 0.0900\n",
      "Epoch [28/30], Step [115/139], Loss: 0.0650\n",
      "Epoch [28/30], Step [116/139], Loss: 0.0585\n",
      "Epoch [28/30], Step [117/139], Loss: 0.0864\n",
      "Epoch [28/30], Step [118/139], Loss: 0.0904\n",
      "Epoch [28/30], Step [119/139], Loss: 0.0697\n",
      "Epoch [28/30], Step [120/139], Loss: 0.0713\n",
      "Epoch [28/30], Step [121/139], Loss: 0.0709\n",
      "Epoch [28/30], Step [122/139], Loss: 0.0693\n",
      "Epoch [28/30], Step [123/139], Loss: 0.0934\n",
      "Epoch [28/30], Step [124/139], Loss: 0.0579\n",
      "Epoch [28/30], Step [125/139], Loss: 0.0811\n",
      "Epoch [28/30], Step [126/139], Loss: 0.0724\n",
      "Epoch [28/30], Step [127/139], Loss: 0.0498\n",
      "Epoch [28/30], Step [128/139], Loss: 0.0831\n",
      "Epoch [28/30], Step [129/139], Loss: 0.0556\n",
      "Epoch [28/30], Step [130/139], Loss: 0.0749\n",
      "Epoch [28/30], Step [131/139], Loss: 0.1147\n",
      "Epoch [28/30], Step [132/139], Loss: 0.0850\n",
      "Epoch [28/30], Step [133/139], Loss: 0.0687\n",
      "Epoch [28/30], Step [134/139], Loss: 0.0748\n",
      "Epoch [28/30], Step [135/139], Loss: 0.0802\n",
      "Epoch [28/30], Step [136/139], Loss: 0.0528\n",
      "Epoch [28/30], Step [137/139], Loss: 0.0707\n",
      "Epoch [28/30], Step [138/139], Loss: 0.0536\n",
      "Epoch [28/30], Step [139/139], Loss: 0.0720\n",
      "Start validation #28\n",
      "Validation #28  Average Loss: 0.0717\n",
      "Best performance at epoch: 28\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [29/30], Step [1/139], Loss: 0.0754\n",
      "Epoch [29/30], Step [2/139], Loss: 0.0687\n",
      "Epoch [29/30], Step [3/139], Loss: 0.0843\n",
      "Epoch [29/30], Step [4/139], Loss: 0.0724\n",
      "Epoch [29/30], Step [5/139], Loss: 0.0773\n",
      "Epoch [29/30], Step [6/139], Loss: 0.0972\n",
      "Epoch [29/30], Step [7/139], Loss: 0.0621\n",
      "Epoch [29/30], Step [8/139], Loss: 0.0915\n",
      "Epoch [29/30], Step [9/139], Loss: 0.0695\n",
      "Epoch [29/30], Step [10/139], Loss: 0.0937\n",
      "Epoch [29/30], Step [11/139], Loss: 0.0810\n",
      "Epoch [29/30], Step [12/139], Loss: 0.0677\n",
      "Epoch [29/30], Step [13/139], Loss: 0.0835\n",
      "Epoch [29/30], Step [14/139], Loss: 0.0509\n",
      "Epoch [29/30], Step [15/139], Loss: 0.1083\n",
      "Epoch [29/30], Step [16/139], Loss: 0.1015\n",
      "Epoch [29/30], Step [17/139], Loss: 0.0681\n",
      "Epoch [29/30], Step [18/139], Loss: 0.1029\n",
      "Epoch [29/30], Step [19/139], Loss: 0.0845\n",
      "Epoch [29/30], Step [20/139], Loss: 0.0509\n",
      "Epoch [29/30], Step [21/139], Loss: 0.0897\n",
      "Epoch [29/30], Step [22/139], Loss: 0.0692\n",
      "Epoch [29/30], Step [23/139], Loss: 0.0731\n",
      "Epoch [29/30], Step [24/139], Loss: 0.1326\n",
      "Epoch [29/30], Step [25/139], Loss: 0.0715\n",
      "Epoch [29/30], Step [26/139], Loss: 0.0543\n",
      "Epoch [29/30], Step [27/139], Loss: 0.0802\n",
      "Epoch [29/30], Step [28/139], Loss: 0.0665\n",
      "Epoch [29/30], Step [29/139], Loss: 0.0586\n",
      "Epoch [29/30], Step [30/139], Loss: 0.0753\n",
      "Epoch [29/30], Step [31/139], Loss: 0.0859\n",
      "Epoch [29/30], Step [32/139], Loss: 0.0716\n",
      "Epoch [29/30], Step [33/139], Loss: 0.0744\n",
      "Epoch [29/30], Step [34/139], Loss: 0.0979\n",
      "Epoch [29/30], Step [35/139], Loss: 0.0690\n",
      "Epoch [29/30], Step [36/139], Loss: 0.0747\n",
      "Epoch [29/30], Step [37/139], Loss: 0.0710\n",
      "Epoch [29/30], Step [38/139], Loss: 0.0748\n",
      "Epoch [29/30], Step [39/139], Loss: 0.0868\n",
      "Epoch [29/30], Step [40/139], Loss: 0.0666\n",
      "Epoch [29/30], Step [41/139], Loss: 0.0618\n",
      "Epoch [29/30], Step [42/139], Loss: 0.0805\n",
      "Epoch [29/30], Step [43/139], Loss: 0.0986\n",
      "Epoch [29/30], Step [44/139], Loss: 0.0593\n",
      "Epoch [29/30], Step [45/139], Loss: 0.0906\n",
      "Epoch [29/30], Step [46/139], Loss: 0.0842\n",
      "Epoch [29/30], Step [47/139], Loss: 0.0614\n",
      "Epoch [29/30], Step [48/139], Loss: 0.0876\n",
      "Epoch [29/30], Step [49/139], Loss: 0.0858\n",
      "Epoch [29/30], Step [50/139], Loss: 0.0741\n",
      "Epoch [29/30], Step [51/139], Loss: 0.0665\n",
      "Epoch [29/30], Step [52/139], Loss: 0.0738\n",
      "Epoch [29/30], Step [53/139], Loss: 0.0638\n",
      "Epoch [29/30], Step [54/139], Loss: 0.0657\n",
      "Epoch [29/30], Step [55/139], Loss: 0.0835\n",
      "Epoch [29/30], Step [56/139], Loss: 0.0778\n",
      "Epoch [29/30], Step [57/139], Loss: 0.0901\n",
      "Epoch [29/30], Step [58/139], Loss: 0.0796\n",
      "Epoch [29/30], Step [59/139], Loss: 0.0658\n",
      "Epoch [29/30], Step [60/139], Loss: 0.0689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/30], Step [61/139], Loss: 0.0781\n",
      "Epoch [29/30], Step [62/139], Loss: 0.0726\n",
      "Epoch [29/30], Step [63/139], Loss: 0.0763\n",
      "Epoch [29/30], Step [64/139], Loss: 0.0723\n",
      "Epoch [29/30], Step [65/139], Loss: 0.0717\n",
      "Epoch [29/30], Step [66/139], Loss: 0.0666\n",
      "Epoch [29/30], Step [67/139], Loss: 0.0564\n",
      "Epoch [29/30], Step [68/139], Loss: 0.0750\n",
      "Epoch [29/30], Step [69/139], Loss: 0.0644\n",
      "Epoch [29/30], Step [70/139], Loss: 0.0543\n",
      "Epoch [29/30], Step [71/139], Loss: 0.0575\n",
      "Epoch [29/30], Step [72/139], Loss: 0.0726\n",
      "Epoch [29/30], Step [73/139], Loss: 0.0580\n",
      "Epoch [29/30], Step [74/139], Loss: 0.0715\n",
      "Epoch [29/30], Step [75/139], Loss: 0.0644\n",
      "Epoch [29/30], Step [76/139], Loss: 0.0762\n",
      "Epoch [29/30], Step [77/139], Loss: 0.0764\n",
      "Epoch [29/30], Step [78/139], Loss: 0.0825\n",
      "Epoch [29/30], Step [79/139], Loss: 0.1033\n",
      "Epoch [29/30], Step [80/139], Loss: 0.0612\n",
      "Epoch [29/30], Step [81/139], Loss: 0.0837\n",
      "Epoch [29/30], Step [82/139], Loss: 0.1016\n",
      "Epoch [29/30], Step [83/139], Loss: 0.0680\n",
      "Epoch [29/30], Step [84/139], Loss: 0.1020\n",
      "Epoch [29/30], Step [85/139], Loss: 0.0773\n",
      "Epoch [29/30], Step [86/139], Loss: 0.0882\n",
      "Epoch [29/30], Step [87/139], Loss: 0.0662\n",
      "Epoch [29/30], Step [88/139], Loss: 0.0676\n",
      "Epoch [29/30], Step [89/139], Loss: 0.0912\n",
      "Epoch [29/30], Step [90/139], Loss: 0.0802\n",
      "Epoch [29/30], Step [91/139], Loss: 0.0690\n",
      "Epoch [29/30], Step [92/139], Loss: 0.0609\n",
      "Epoch [29/30], Step [93/139], Loss: 0.0741\n",
      "Epoch [29/30], Step [94/139], Loss: 0.0592\n",
      "Epoch [29/30], Step [95/139], Loss: 0.0966\n",
      "Epoch [29/30], Step [96/139], Loss: 0.0827\n",
      "Epoch [29/30], Step [97/139], Loss: 0.0748\n",
      "Epoch [29/30], Step [98/139], Loss: 0.0649\n",
      "Epoch [29/30], Step [99/139], Loss: 0.0799\n",
      "Epoch [29/30], Step [100/139], Loss: 0.0817\n",
      "Epoch [29/30], Step [101/139], Loss: 0.0841\n",
      "Epoch [29/30], Step [102/139], Loss: 0.0693\n",
      "Epoch [29/30], Step [103/139], Loss: 0.0817\n",
      "Epoch [29/30], Step [104/139], Loss: 0.0974\n",
      "Epoch [29/30], Step [105/139], Loss: 0.0873\n",
      "Epoch [29/30], Step [106/139], Loss: 0.0806\n",
      "Epoch [29/30], Step [107/139], Loss: 0.0541\n",
      "Epoch [29/30], Step [108/139], Loss: 0.0595\n",
      "Epoch [29/30], Step [109/139], Loss: 0.0675\n",
      "Epoch [29/30], Step [110/139], Loss: 0.0793\n",
      "Epoch [29/30], Step [111/139], Loss: 0.0995\n",
      "Epoch [29/30], Step [112/139], Loss: 0.0974\n",
      "Epoch [29/30], Step [113/139], Loss: 0.0652\n",
      "Epoch [29/30], Step [114/139], Loss: 0.0586\n",
      "Epoch [29/30], Step [115/139], Loss: 0.0500\n",
      "Epoch [29/30], Step [116/139], Loss: 0.0653\n",
      "Epoch [29/30], Step [117/139], Loss: 0.0751\n",
      "Epoch [29/30], Step [118/139], Loss: 0.0545\n",
      "Epoch [29/30], Step [119/139], Loss: 0.0794\n",
      "Epoch [29/30], Step [120/139], Loss: 0.0689\n",
      "Epoch [29/30], Step [121/139], Loss: 0.0744\n",
      "Epoch [29/30], Step [122/139], Loss: 0.0781\n",
      "Epoch [29/30], Step [123/139], Loss: 0.0610\n",
      "Epoch [29/30], Step [124/139], Loss: 0.0640\n",
      "Epoch [29/30], Step [125/139], Loss: 0.0694\n",
      "Epoch [29/30], Step [126/139], Loss: 0.0827\n",
      "Epoch [29/30], Step [127/139], Loss: 0.0751\n",
      "Epoch [29/30], Step [128/139], Loss: 0.0799\n",
      "Epoch [29/30], Step [129/139], Loss: 0.0605\n",
      "Epoch [29/30], Step [130/139], Loss: 0.0450\n",
      "Epoch [29/30], Step [131/139], Loss: 0.0583\n",
      "Epoch [29/30], Step [132/139], Loss: 0.0801\n",
      "Epoch [29/30], Step [133/139], Loss: 0.0679\n",
      "Epoch [29/30], Step [134/139], Loss: 0.0580\n",
      "Epoch [29/30], Step [135/139], Loss: 0.0529\n",
      "Epoch [29/30], Step [136/139], Loss: 0.0526\n",
      "Epoch [29/30], Step [137/139], Loss: 0.0750\n",
      "Epoch [29/30], Step [138/139], Loss: 0.0703\n",
      "Epoch [29/30], Step [139/139], Loss: 0.0617\n",
      "Start validation #29\n",
      "Validation #29  Average Loss: 0.0708\n",
      "Best performance at epoch: 29\n",
      "Save model in ./saved/LSTM\n",
      "Epoch [30/30], Step [1/139], Loss: 0.0616\n",
      "Epoch [30/30], Step [2/139], Loss: 0.0711\n",
      "Epoch [30/30], Step [3/139], Loss: 0.0716\n",
      "Epoch [30/30], Step [4/139], Loss: 0.0604\n",
      "Epoch [30/30], Step [5/139], Loss: 0.0473\n",
      "Epoch [30/30], Step [6/139], Loss: 0.0875\n",
      "Epoch [30/30], Step [7/139], Loss: 0.0689\n",
      "Epoch [30/30], Step [8/139], Loss: 0.0733\n",
      "Epoch [30/30], Step [9/139], Loss: 0.0949\n",
      "Epoch [30/30], Step [10/139], Loss: 0.0540\n",
      "Epoch [30/30], Step [11/139], Loss: 0.0835\n",
      "Epoch [30/30], Step [12/139], Loss: 0.0851\n",
      "Epoch [30/30], Step [13/139], Loss: 0.0849\n",
      "Epoch [30/30], Step [14/139], Loss: 0.0669\n",
      "Epoch [30/30], Step [15/139], Loss: 0.0578\n",
      "Epoch [30/30], Step [16/139], Loss: 0.0536\n",
      "Epoch [30/30], Step [17/139], Loss: 0.1087\n",
      "Epoch [30/30], Step [18/139], Loss: 0.0963\n",
      "Epoch [30/30], Step [19/139], Loss: 0.0871\n",
      "Epoch [30/30], Step [20/139], Loss: 0.0759\n",
      "Epoch [30/30], Step [21/139], Loss: 0.0575\n",
      "Epoch [30/30], Step [22/139], Loss: 0.0605\n",
      "Epoch [30/30], Step [23/139], Loss: 0.0760\n",
      "Epoch [30/30], Step [24/139], Loss: 0.0639\n",
      "Epoch [30/30], Step [25/139], Loss: 0.0760\n",
      "Epoch [30/30], Step [26/139], Loss: 0.1009\n",
      "Epoch [30/30], Step [27/139], Loss: 0.0593\n",
      "Epoch [30/30], Step [28/139], Loss: 0.0943\n",
      "Epoch [30/30], Step [29/139], Loss: 0.0791\n",
      "Epoch [30/30], Step [30/139], Loss: 0.0759\n",
      "Epoch [30/30], Step [31/139], Loss: 0.0535\n",
      "Epoch [30/30], Step [32/139], Loss: 0.0583\n",
      "Epoch [30/30], Step [33/139], Loss: 0.0644\n",
      "Epoch [30/30], Step [34/139], Loss: 0.0690\n",
      "Epoch [30/30], Step [35/139], Loss: 0.0745\n",
      "Epoch [30/30], Step [36/139], Loss: 0.0792\n",
      "Epoch [30/30], Step [37/139], Loss: 0.0913\n",
      "Epoch [30/30], Step [38/139], Loss: 0.0726\n",
      "Epoch [30/30], Step [39/139], Loss: 0.0830\n",
      "Epoch [30/30], Step [40/139], Loss: 0.0672\n",
      "Epoch [30/30], Step [41/139], Loss: 0.0932\n",
      "Epoch [30/30], Step [42/139], Loss: 0.0686\n",
      "Epoch [30/30], Step [43/139], Loss: 0.0609\n",
      "Epoch [30/30], Step [44/139], Loss: 0.0692\n",
      "Epoch [30/30], Step [45/139], Loss: 0.0613\n",
      "Epoch [30/30], Step [46/139], Loss: 0.0702\n",
      "Epoch [30/30], Step [47/139], Loss: 0.0689\n",
      "Epoch [30/30], Step [48/139], Loss: 0.0807\n",
      "Epoch [30/30], Step [49/139], Loss: 0.0906\n",
      "Epoch [30/30], Step [50/139], Loss: 0.0735\n",
      "Epoch [30/30], Step [51/139], Loss: 0.0883\n",
      "Epoch [30/30], Step [52/139], Loss: 0.0788\n",
      "Epoch [30/30], Step [53/139], Loss: 0.1458\n",
      "Epoch [30/30], Step [54/139], Loss: 0.0593\n",
      "Epoch [30/30], Step [55/139], Loss: 0.0779\n",
      "Epoch [30/30], Step [56/139], Loss: 0.0811\n",
      "Epoch [30/30], Step [57/139], Loss: 0.0688\n",
      "Epoch [30/30], Step [58/139], Loss: 0.0574\n",
      "Epoch [30/30], Step [59/139], Loss: 0.0771\n",
      "Epoch [30/30], Step [60/139], Loss: 0.0830\n",
      "Epoch [30/30], Step [61/139], Loss: 0.0583\n",
      "Epoch [30/30], Step [62/139], Loss: 0.0622\n",
      "Epoch [30/30], Step [63/139], Loss: 0.0969\n",
      "Epoch [30/30], Step [64/139], Loss: 0.0972\n",
      "Epoch [30/30], Step [65/139], Loss: 0.0802\n",
      "Epoch [30/30], Step [66/139], Loss: 0.0715\n",
      "Epoch [30/30], Step [67/139], Loss: 0.0717\n",
      "Epoch [30/30], Step [68/139], Loss: 0.0762\n",
      "Epoch [30/30], Step [69/139], Loss: 0.0858\n",
      "Epoch [30/30], Step [70/139], Loss: 0.0508\n",
      "Epoch [30/30], Step [71/139], Loss: 0.0691\n",
      "Epoch [30/30], Step [72/139], Loss: 0.0593\n",
      "Epoch [30/30], Step [73/139], Loss: 0.0524\n",
      "Epoch [30/30], Step [74/139], Loss: 0.0818\n",
      "Epoch [30/30], Step [75/139], Loss: 0.0691\n",
      "Epoch [30/30], Step [76/139], Loss: 0.0678\n",
      "Epoch [30/30], Step [77/139], Loss: 0.0512\n",
      "Epoch [30/30], Step [78/139], Loss: 0.0641\n",
      "Epoch [30/30], Step [79/139], Loss: 0.0726\n",
      "Epoch [30/30], Step [80/139], Loss: 0.0542\n",
      "Epoch [30/30], Step [81/139], Loss: 0.1005\n",
      "Epoch [30/30], Step [82/139], Loss: 0.0647\n",
      "Epoch [30/30], Step [83/139], Loss: 0.0727\n",
      "Epoch [30/30], Step [84/139], Loss: 0.0554\n",
      "Epoch [30/30], Step [85/139], Loss: 0.0794\n",
      "Epoch [30/30], Step [86/139], Loss: 0.0480\n",
      "Epoch [30/30], Step [87/139], Loss: 0.0871\n",
      "Epoch [30/30], Step [88/139], Loss: 0.0815\n",
      "Epoch [30/30], Step [89/139], Loss: 0.0797\n",
      "Epoch [30/30], Step [90/139], Loss: 0.0711\n",
      "Epoch [30/30], Step [91/139], Loss: 0.0835\n",
      "Epoch [30/30], Step [92/139], Loss: 0.0704\n",
      "Epoch [30/30], Step [93/139], Loss: 0.0575\n",
      "Epoch [30/30], Step [94/139], Loss: 0.0825\n",
      "Epoch [30/30], Step [95/139], Loss: 0.0747\n",
      "Epoch [30/30], Step [96/139], Loss: 0.0588\n",
      "Epoch [30/30], Step [97/139], Loss: 0.0906\n",
      "Epoch [30/30], Step [98/139], Loss: 0.0566\n",
      "Epoch [30/30], Step [99/139], Loss: 0.0756\n",
      "Epoch [30/30], Step [100/139], Loss: 0.0836\n",
      "Epoch [30/30], Step [101/139], Loss: 0.0842\n",
      "Epoch [30/30], Step [102/139], Loss: 0.0794\n",
      "Epoch [30/30], Step [103/139], Loss: 0.0723\n",
      "Epoch [30/30], Step [104/139], Loss: 0.0602\n",
      "Epoch [30/30], Step [105/139], Loss: 0.0683\n",
      "Epoch [30/30], Step [106/139], Loss: 0.0750\n",
      "Epoch [30/30], Step [107/139], Loss: 0.0531\n",
      "Epoch [30/30], Step [108/139], Loss: 0.0996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/30], Step [109/139], Loss: 0.0696\n",
      "Epoch [30/30], Step [110/139], Loss: 0.0752\n",
      "Epoch [30/30], Step [111/139], Loss: 0.0543\n",
      "Epoch [30/30], Step [112/139], Loss: 0.1041\n",
      "Epoch [30/30], Step [113/139], Loss: 0.0745\n",
      "Epoch [30/30], Step [114/139], Loss: 0.0946\n",
      "Epoch [30/30], Step [115/139], Loss: 0.0606\n",
      "Epoch [30/30], Step [116/139], Loss: 0.0746\n",
      "Epoch [30/30], Step [117/139], Loss: 0.1120\n",
      "Epoch [30/30], Step [118/139], Loss: 0.0553\n",
      "Epoch [30/30], Step [119/139], Loss: 0.0784\n",
      "Epoch [30/30], Step [120/139], Loss: 0.0738\n",
      "Epoch [30/30], Step [121/139], Loss: 0.0831\n",
      "Epoch [30/30], Step [122/139], Loss: 0.0785\n",
      "Epoch [30/30], Step [123/139], Loss: 0.0773\n",
      "Epoch [30/30], Step [124/139], Loss: 0.0876\n",
      "Epoch [30/30], Step [125/139], Loss: 0.0589\n",
      "Epoch [30/30], Step [126/139], Loss: 0.0712\n",
      "Epoch [30/30], Step [127/139], Loss: 0.0820\n",
      "Epoch [30/30], Step [128/139], Loss: 0.0950\n",
      "Epoch [30/30], Step [129/139], Loss: 0.0895\n",
      "Epoch [30/30], Step [130/139], Loss: 0.0749\n",
      "Epoch [30/30], Step [131/139], Loss: 0.0639\n",
      "Epoch [30/30], Step [132/139], Loss: 0.0662\n",
      "Epoch [30/30], Step [133/139], Loss: 0.0966\n",
      "Epoch [30/30], Step [134/139], Loss: 0.0838\n",
      "Epoch [30/30], Step [135/139], Loss: 0.0645\n",
      "Epoch [30/30], Step [136/139], Loss: 0.0685\n",
      "Epoch [30/30], Step [137/139], Loss: 0.0720\n",
      "Epoch [30/30], Step [138/139], Loss: 0.0633\n",
      "Epoch [30/30], Step [139/139], Loss: 0.0642\n",
      "Start validation #30\n",
      "Validation #30  Average Loss: 0.0707\n",
      "Best performance at epoch: 30\n",
      "Save model in ./saved/LSTM\n"
     ]
    }
   ],
   "source": [
    "train(num_epochs, model, train_loader, criterion, optimizer, saved_dir, val_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 저장된 모델 불러오기 및 test\n",
    "\n",
    "학습한 모델의 성능을 테스트합니다. 저장한 모델 파일을 [torch.load](https://pytorch.org/docs/stable/torch.html?highlight=load#torch.load)를 통해 불러옵니다. 위에서 학습을 끝까지 진행하지 않았다면, 아래의 주석 처리된 부분을 주석 해제하면, 제공해드린 미리 학습시킨 모델을 불러올 수 있습니다. \n",
    "\n",
    "이렇게 불러오면 우리가 얻게 되는 건 아까 저장한 **check_point** 딕셔너리입니다. 딕셔너리에 저장한 모델의 파라미터는 **'net'** key에 저장해두었습니다. 이를 불러와 **state_dict**에 저장합니다. 이렇게 불러온 모델의 파라미터를 모델에 실제로 로드하기 위해서는 [nn.Module.load_state_dict](https://pytorch.org/docs/stable/torch.html?highlight=load#torch.load)를 사용하면 됩니다. \n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "다음을 읽고 코드를 완성해보세요. 단, \"#코드 시작\"과 \"#코드 종료\" 사이에 주어진 변수 명으로 코드를 작성하세요!\n",
    "- **model_path**의 경로에 있는 모델 파일을 로드하여, 이를 **check_point** 변수에 저장합니다. \n",
    "- **check_point** 딕셔너리에 접근하여 모델의 파라미터를 **state_dict** 변수에 저장합니다.\n",
    "- **state_dict**의 파라미터들을 우리 모델에 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './saved/LSTM/best_model.pt'\n",
    "model = SimpleLSTM() # 아래의 모델 불러오기를 정확히 구현했는지 확인하기 위해 새로 모델을 선언하여 학습 이전 상태로 초기화\n",
    "# 코드 시작\n",
    "checkpoint = torch.load(model_path)\n",
    "state_dict = checkpoint['net']\n",
    "model.load_state_dict(state_dict)\n",
    "# 코드 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 모델의 성능을 테스트합니다. 베이스라인 성능을 뛰어 넘었다는 문구가 나오면 성공적으로 진행한 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start test..\n",
      "Test  Average Loss: 0.0918  Baseline Loss: 0.0963\n",
      "베이스라인 성능을 뛰어 넘었습니다!\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader, criterion, baseline_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 모델의 예측 기온과 실제 기온을 몇가지 살펴보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 기온: 7.0 / 실제 기온: 7.2\n",
      "예측 기온: 13.0 / 실제 기온: 10.2\n",
      "예측 기온: 10.7 / 실제 기온: 9.0\n",
      "예측 기온: 28.1 / 실제 기온: 25.7\n",
      "예측 기온: 21.3 / 실제 기온: 23.6\n",
      "예측 기온: 27.7 / 실제 기온: 28.4\n",
      "예측 기온: 15.8 / 실제 기온: 17.1\n",
      "예측 기온: 28.2 / 실제 기온: 28.3\n",
      "예측 기온: 20.1 / 실제 기온: 23.1\n",
      "예측 기온: 16.3 / 실제 기온: 17.7\n",
      "예측 기온: 1.1 / 실제 기온: 3.8\n",
      "예측 기온: 11.8 / 실제 기온: 5.6\n",
      "예측 기온: 10.8 / 실제 기온: 11.0\n",
      "예측 기온: -3.6 / 실제 기온: 0.0\n",
      "예측 기온: -0.7 / 실제 기온: -4.9\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    data_idx = np.random.randint(len(test_data))\n",
    "    sequence = test_data[data_idx][0]\n",
    "    sequence = torch.Tensor(sequence).unsqueeze(0)\n",
    "    \n",
    "    h0, c0 = model.init_hidden(batch_size=1)\n",
    "    pred = model(sequence, h0, c0)\n",
    "    pred = pred.item() * std[0] + mean[0]\n",
    "    target = test_data[data_idx][1][0] * std[0] + mean[0]\n",
    "    print('예측 기온: {:.1f} / 실제 기온: {:.1f}'.format(pred, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "우리는 이번 실습을 통해 다음과 같은 내용을 학습했습니다.\n",
    "- 날씨와 같은 시계열 정보를 다룰 수 있다.\n",
    "- RNN을 설계하고 시간 순서상 미래의 정보를 예측하는 모델을 학습시킬 수 있다.\n",
    "- 상식적인 수준의 베이스라인을 도입하여 학습한 모델의 성능을 검증할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 제출\n",
    "\n",
    "아래를 실행하면 최종 제출이 완료됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook rnn.ipynb to html\n",
      "[NbConvertApp] Writing 547746 bytes to check_util/rnn_submit.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert rnn.ipynb --to html --output rnn_submit --output-dir=check_util"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
